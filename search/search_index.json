{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction to consumerfinance.gov \u00b6 This is the documentation for the consumerfinance.gov project that powers the www.consumerfinance.gov website. It is organized thematically in order to create a central repository for all information pertaining to consumerfinance.gov. Disclaimer \u00b6 This project is a work in progress. Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on www.consumerfinance.gov . In-progress updates may appear on beta.consumerfinance.gov . Technology stack \u00b6 The standard technology stack for development of consumerfinance.gov within the CFPB consists of the following base: macOS Homebrew - package manager for installing system software on OSX Python 3.6 and pip (Python package manager) Jinja2 templates for front-end rendering. See requirements/libraries.txt for version. Wagtail CMS for content administration. See requirements/wagtail.txt for version. PostgreSQL 10.5 is the database we use in production and locally. Psycopg is the Python library that lets Python talk to Postgres. See requirements/libraries.txt for current version. Additional dependencies, listed below. Additional dependencies \u00b6 Elasticsearch : Used for full-text search capabilities and content indexing. Node and yarn : Used for downloading and managing front-end dependencies and assets. Front-end dependencies are listed in the project's package.json file. Gulp for running tasks, including compiling front-end assets and running front-end tests. virtualenv virtualenvwrapper Versions \u00b6 Versions for most front-end packages are kept updated in the project's package.json file. Versions for back-end software including Django, Wagtail, Jinja, etc. are kept in the project's requirements files: https://github.com/cfpb/consumerfinance.gov/tree/main/requirements base.txt : shortcut for django.txt + wagtail.txt + libraries.txt ci.txt : specific requirements for the continuous integration environment. Should/could be moved to CI configuration files? deployment.txt : requirements for deployment, includes base.txt and a New Relic library that we don't install anywhere else. django.txt : specifies the Django version. The file is used when running the site, but by having it separate we can test against other versions of Django by excluding this file. docs.txt : requirements to build the consumerfinance.gov docs. libraries.txt : Python libraries. local.txt : includes base.txt and some useful libraries when developing locally. scripts.txt : Requirements for running certain jobs on Jenkins, so scripts can run in Jenkins without having to install all the other requirements. test.txt : requirements for running Python tests. wagtail.txt : specifies Wagtail version. In its own file to make it easier to test multiple versions, same as with django.txt .","title":"Introduction"},{"location":"#introduction-to-consumerfinancegov","text":"This is the documentation for the consumerfinance.gov project that powers the www.consumerfinance.gov website. It is organized thematically in order to create a central repository for all information pertaining to consumerfinance.gov.","title":"Introduction to consumerfinance.gov"},{"location":"#disclaimer","text":"This project is a work in progress. Nothing presented in this repo\u2014whether in the source code, issue tracker, or wiki\u2014is a final product unless it is marked as such or appears on www.consumerfinance.gov . In-progress updates may appear on beta.consumerfinance.gov .","title":"Disclaimer"},{"location":"#technology-stack","text":"The standard technology stack for development of consumerfinance.gov within the CFPB consists of the following base: macOS Homebrew - package manager for installing system software on OSX Python 3.6 and pip (Python package manager) Jinja2 templates for front-end rendering. See requirements/libraries.txt for version. Wagtail CMS for content administration. See requirements/wagtail.txt for version. PostgreSQL 10.5 is the database we use in production and locally. Psycopg is the Python library that lets Python talk to Postgres. See requirements/libraries.txt for current version. Additional dependencies, listed below.","title":"Technology stack"},{"location":"#additional-dependencies","text":"Elasticsearch : Used for full-text search capabilities and content indexing. Node and yarn : Used for downloading and managing front-end dependencies and assets. Front-end dependencies are listed in the project's package.json file. Gulp for running tasks, including compiling front-end assets and running front-end tests. virtualenv virtualenvwrapper","title":"Additional dependencies"},{"location":"#versions","text":"Versions for most front-end packages are kept updated in the project's package.json file. Versions for back-end software including Django, Wagtail, Jinja, etc. are kept in the project's requirements files: https://github.com/cfpb/consumerfinance.gov/tree/main/requirements base.txt : shortcut for django.txt + wagtail.txt + libraries.txt ci.txt : specific requirements for the continuous integration environment. Should/could be moved to CI configuration files? deployment.txt : requirements for deployment, includes base.txt and a New Relic library that we don't install anywhere else. django.txt : specifies the Django version. The file is used when running the site, but by having it separate we can test against other versions of Django by excluding this file. docs.txt : requirements to build the consumerfinance.gov docs. libraries.txt : Python libraries. local.txt : includes base.txt and some useful libraries when developing locally. scripts.txt : Requirements for running certain jobs on Jenkins, so scripts can run in Jenkins without having to install all the other requirements. test.txt : requirements for running Python tests. wagtail.txt : specifies Wagtail version. In its own file to make it easier to test multiple versions, same as with django.txt .","title":"Versions"},{"location":"ask-cfpb/","text":"Ask CFPB API \u00b6 This API provides search access to the English and Spanish content behind Ask CFPB and Obtener respuestas . The financial topics covered include: Auto loans Bank accounts and services Credit cards Credit reports and scores Debt collection Families and money Money transfers Mortgages Payday loans Prepaid cards Student loans Usage \u00b6 The API is a read-only resource that delivers search results in json format. Requests follow this pattern: https://www.consumerfinance.gov/ask-cfpb/search/json/?q=[SEARCH TERMS] The json response will includes a list of results, each with a question, an answer, and a URL for the related CFPB page. If no results are found, the \"suggestion\" field will offer a more promising search term if one can be found. The payload for the search term \"tuition\" would look like this, but with more result entries: { query: \"tuition\", suggestion: null, result_query: \"tuition\", results: [ { url: \"https://www.consumerfinance.gov/ask-cfpb/what-is-a-tuition-payment-plan-en-563/\", text: \"Tuition payment plans, also called tuition installment plans, are short-term (12 months or less) payment plans that split your college bills into equal monthly payments. Tuition installment plans can be an alternative to student loans if you can afford to pay tuition, just not in a lump sum at the start of the semester or quarter. These payment plans do not generally charge interest, but they may have up-front fees. What is a tuition payment plan?\", question: \"What is a tuition payment plan?\" } ] } Spanish content \u00b6 For questions and answers in Spanish, requests should follow this pattern: https://www.consumerfinance.gov/es/obtener-respuestas/buscar/json/?q=[SPANISH SEARCH TERMS] The payload for the Spanish search term \"vehiculo\" would look like this, but with more result entries: { query: \"vehiculo\", suggestion: null, result_query: \"vehiculo\", results: [ { url: \"https://www.consumerfinance.gov/es/obtener-respuestas/como-puedo-averiguar-el-significado-de-los-terminos-de-mi-contrato-de-leasing-es-2047/\", text: \" Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingl\u00e9s), la persona o compa\u00f1\u00eda de quien usted hace el leasing de un veh\u00edculo, conocida como el \"arrendador\", deber\u00e1 informar por escrito ciertos costos y plazos si el leasing es de m\u00e1s de cuatro meses y si cumple con otros requisitos. La mayor\u00eda de los arrendamientos de veh\u00edculos est\u00e1 sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los t\u00e9rminos de su contrato de leasing. En el sitio web Comprenda c\u00f3mo funciona la financiaci\u00f3n de veh\u00edculos de la Comisi\u00f3n Federal de Comercio se ofrece la siguiente informaci\u00f3n en espa\u00f1ol: Antes de comprar un veh\u00edculo o hacer un leasing \u00bfDeber\u00eda hacer un leasing para un veh\u00edculo? Glosario de t\u00e9rminos espec\u00edficos M\u00e1s informaci\u00f3n en espa\u00f1ol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o \u201cleasing\u201d Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingles), la persona o compania de quien usted hace el leasing de un vehiculo, conocida como el \"arrendador\", debera informar por escrito ciertos costos y plazos si el leasing es de mas de cuatro meses y si cumple con otros requisitos. La mayoria de los arrendamientos de vehiculos esta sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los terminos de su contrato de leasing. En el sitio web Comprenda como funciona la financiacion de vehiculos de la Comision Federal de Comercio se ofrece la siguiente informacion en espanol: Antes de comprar un vehiculo o hacer un leasing Deberia hacer un leasing para un vehiculo? Glosario de terminos especificos Mas informacion en espanol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o leasing \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? Como puedo averiguar el significado de los terminos de mi contrato de leasing?\", question: \"\u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing?\" } ] }","title":"Ask CFPB"},{"location":"ask-cfpb/#ask-cfpb-api","text":"This API provides search access to the English and Spanish content behind Ask CFPB and Obtener respuestas . The financial topics covered include: Auto loans Bank accounts and services Credit cards Credit reports and scores Debt collection Families and money Money transfers Mortgages Payday loans Prepaid cards Student loans","title":"Ask CFPB API"},{"location":"ask-cfpb/#usage","text":"The API is a read-only resource that delivers search results in json format. Requests follow this pattern: https://www.consumerfinance.gov/ask-cfpb/search/json/?q=[SEARCH TERMS] The json response will includes a list of results, each with a question, an answer, and a URL for the related CFPB page. If no results are found, the \"suggestion\" field will offer a more promising search term if one can be found. The payload for the search term \"tuition\" would look like this, but with more result entries: { query: \"tuition\", suggestion: null, result_query: \"tuition\", results: [ { url: \"https://www.consumerfinance.gov/ask-cfpb/what-is-a-tuition-payment-plan-en-563/\", text: \"Tuition payment plans, also called tuition installment plans, are short-term (12 months or less) payment plans that split your college bills into equal monthly payments. Tuition installment plans can be an alternative to student loans if you can afford to pay tuition, just not in a lump sum at the start of the semester or quarter. These payment plans do not generally charge interest, but they may have up-front fees. What is a tuition payment plan?\", question: \"What is a tuition payment plan?\" } ] }","title":"Usage"},{"location":"ask-cfpb/#spanish-content","text":"For questions and answers in Spanish, requests should follow this pattern: https://www.consumerfinance.gov/es/obtener-respuestas/buscar/json/?q=[SPANISH SEARCH TERMS] The payload for the Spanish search term \"vehiculo\" would look like this, but with more result entries: { query: \"vehiculo\", suggestion: null, result_query: \"vehiculo\", results: [ { url: \"https://www.consumerfinance.gov/es/obtener-respuestas/como-puedo-averiguar-el-significado-de-los-terminos-de-mi-contrato-de-leasing-es-2047/\", text: \" Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingl\u00e9s), la persona o compa\u00f1\u00eda de quien usted hace el leasing de un veh\u00edculo, conocida como el \"arrendador\", deber\u00e1 informar por escrito ciertos costos y plazos si el leasing es de m\u00e1s de cuatro meses y si cumple con otros requisitos. La mayor\u00eda de los arrendamientos de veh\u00edculos est\u00e1 sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los t\u00e9rminos de su contrato de leasing. En el sitio web Comprenda c\u00f3mo funciona la financiaci\u00f3n de veh\u00edculos de la Comisi\u00f3n Federal de Comercio se ofrece la siguiente informaci\u00f3n en espa\u00f1ol: Antes de comprar un veh\u00edculo o hacer un leasing \u00bfDeber\u00eda hacer un leasing para un veh\u00edculo? Glosario de t\u00e9rminos espec\u00edficos M\u00e1s informaci\u00f3n en espa\u00f1ol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o \u201cleasing\u201d Bajo la Ley de Arrendamientos del Consumidor (CLA, por sus siglas en ingles), la persona o compania de quien usted hace el leasing de un vehiculo, conocida como el \"arrendador\", debera informar por escrito ciertos costos y plazos si el leasing es de mas de cuatro meses y si cumple con otros requisitos. La mayoria de los arrendamientos de vehiculos esta sujeta a la CLA. Los siguientes materiales le pueden ayudar a entender los terminos de su contrato de leasing. En el sitio web Comprenda como funciona la financiacion de vehiculos de la Comision Federal de Comercio se ofrece la siguiente informacion en espanol: Antes de comprar un vehiculo o hacer un leasing Deberia hacer un leasing para un vehiculo? Glosario de terminos especificos Mas informacion en espanol de GobiernoUSA.gov: Consejos para comprar un auto usado: Arrendamiento con derecho a compra o leasing \u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing? Como puedo averiguar el significado de los terminos de mi contrato de leasing?\", question: \"\u00bfC\u00f3mo puedo averiguar el significado de los t\u00e9rminos de mi contrato de leasing?\" } ] }","title":"Spanish content"},{"location":"atomic-structure/","text":"Notes on Atomic Design \u00b6 Our components employ the concept of atomic design, meaning that we break them down into atoms, molecules, and organisms, each successive level being more complex than the previous. (We do not currently use the template or page concepts as described in Brad Frost's seminal article introducing atomic design ). Our components are composed (on the front-end) of HTML, Less, and JavaScript. If a component doesn\u2019t have user interactions or require styling, then it won\u2019t have an associated JS and/or Less file. Components that are available for adding to a Wagtail page also require some Python programming\u2014see the creating and editing components page for details. We compose our atomic components as follows: Atoms \u00b6 The smallest kind of component. May not contain any other components. Prefixed with a- in class names. HTML \u00b6 <div class=\"a-tag\"> Tag label {{ svg_icon('error') }} </div> Less \u00b6 .a-tag { cursor: default; display: inline-block; padding: 5px 10px; \u2026 } JavaScript \u00b6 None of our atoms require any JavaScript at this time. Molecules \u00b6 The medium-sized component. May contain atoms. Prefixed with m- in class names. HTML \u00b6 <div class=\"m-notification m-notification__visible m-notification__error\" data-js-hook=\"state_atomic_init\"> {{ svg_icon('error') }} <div class=\"m-notification_content\" role=\"alert\"> <div class=\"h4 m-notification_message\">Page not found.</div> </div> </div> Less \u00b6 .m-notification { display: none; position: relative; padding: @notification-padding__px; \u2026 } JavaScript \u00b6 function Notification( element ) { const BASE_CLASS = 'm-notification'; // Constants for the state of this Notification. const SUCCESS = 'success'; const WARNING = 'warning'; const ERROR = 'error'; // Constants for the Notification modifiers. const MODIFIER_VISIBLE = BASE_CLASS + '__visible'; const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _contentDom = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Notification molecule can be instantiated by adding the following to your project's JavaScript code: const notification = new Notification( _dom ); notification.init(); Organisms \u00b6 The largest component. May contain atoms, molecules, or (if no other solution is viable) other organisms. Prefixed with o- in class names. HTML \u00b6 <div class=\"o-expandable o-expandable__borders o-expandable__midtone o-expandable__expanded\" data-js-hook=\"state_atomic_init\"> <button class=\"o-expandable_target\" aria-pressed=\"true\"> <div class=\"o-expandable_header\"> \u2026 Less \u00b6 .o-expandable { position: relative; &_target { padding: 0; border: 0; \u2026 } \u2026 } JavaScript \u00b6 function Expandable( element ) { const BASE_CLASS = 'o-expandable'; // Bitwise flags for the state of this Expandable. const COLLAPSED = 0; const COLLAPSING = 1; const EXPANDING = 2; const EXPANDED = 3; // The Expandable element will directly be the Expandable // when used in an ExpandableGroup, otherwise it can be the parent container. const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _target = _dom.querySelector( '.' + BASE_CLASS + '_target' ); const _content = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Expandable organism can be instantiated by adding the following to your project's JavaScript code: const expandable = new Expandable( _dom.querySelector( '.o-expandable' ) ); expandable.init( _expandable.EXPANDED ); or const atomicHelpers = require( '@cfpb/cfpb-atomic-component/src/utilities/atomic-helpers.js' ); const Expandable = require( '../../organisms/Expandable' ); atomicHelpers.instantiateAll( '.o-expandable', Expandable ); Folder structure \u00b6 Our atomic components are separated and named based on asset type. HTML, Less, and JavaScript for each component are in separate directories. HTML \u00b6 consumerfinance.gov/cfgov/jinja2/v1/_includes/atoms/ consumerfinance.gov/cfgov/jinja2/v1/_includes/molecules/ consumerfinance.gov/cfgov/jinja2/v1/_includes/organisms/ Note Some of our foundational components get their Less and JavaScript from the Design System , but the HTML for their Wagtail block templates is stored in the above folders. CSS \u00b6 consumerfinance.gov/cfgov/unprocessed/css/atoms/ consumerfinance.gov/cfgov/unprocessed/css/molecules/ consumerfinance.gov/cfgov/unprocessed/css/organisms/ JavaScript \u00b6 consumerfinance.gov/cfgov/unprocessed/js/molecules/ consumerfinance.gov/cfgov/unprocessed/js/organisms/ Tests \u00b6 consumerfinance.gov/test/unit_tests/js/molecules/ consumerfinance.gov/test/unit_tests/js/organisms/ JavaScript architecture \u00b6 JavaScript components are built to be rendered on the server and then enhanced via JavaScript on the client. The basic interface for the components is as follows: function AtomicComponent( domElement ) { // Ensure the passed in Element is in the DOM. // Query and store references to sub-elements. // Instantiate child atomic components. // Bind necessary events for referenced DOM elements. // Perform other initialization related tasks. this.init = function init(){} // General teardown function // We don't remove the element from the DOM so // we need to unbind the events. this.destroy = function destroy(){} } We generally favor composition over inheritance. You can get more information by reading the following: A Simple Challenge to Classical Inheritance Fans Composition over Inheritance (YouTube) Component build pipeline \u00b6 Gulp \u00b6 Gulp is used as a task automation tool. Tasks include compiling CSS, creating a standard Webpack workflow for bundling scripts, minifying code, linting, running unit tests, and more . Webpack \u00b6 Wepback is used as a module bundler, although it's capable of more. We create page, global, and component-specific bundles. The configuration for the bundles is contained in config/webpack-config.js . An explanation for the usage of each bundle is contained in gulp/tasks/scripts.js . Routes \u00b6 Routes are used to serve JavaScript bundles to the browser based on the requested URL or Wagtail page's Media definition. This happens via code contained in base.html . This file serves as the base HTML template for serving Wagtail pages. Wagtail page Media class \u00b6 Each atomic component has a Media class that lists the JavaScript files that should be loaded via base.html . When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is an example of the Media class on a component, the EmailSignUp organism : class Media: js = ['email-signup.js'] This will load the email-signup.js script on any page that includes the EmailSignUp organism in one of its StreamFields.","title":"Atomic Structure and Design"},{"location":"atomic-structure/#notes-on-atomic-design","text":"Our components employ the concept of atomic design, meaning that we break them down into atoms, molecules, and organisms, each successive level being more complex than the previous. (We do not currently use the template or page concepts as described in Brad Frost's seminal article introducing atomic design ). Our components are composed (on the front-end) of HTML, Less, and JavaScript. If a component doesn\u2019t have user interactions or require styling, then it won\u2019t have an associated JS and/or Less file. Components that are available for adding to a Wagtail page also require some Python programming\u2014see the creating and editing components page for details. We compose our atomic components as follows:","title":"Notes on Atomic Design"},{"location":"atomic-structure/#atoms","text":"The smallest kind of component. May not contain any other components. Prefixed with a- in class names.","title":"Atoms"},{"location":"atomic-structure/#html","text":"<div class=\"a-tag\"> Tag label {{ svg_icon('error') }} </div>","title":"HTML"},{"location":"atomic-structure/#less","text":".a-tag { cursor: default; display: inline-block; padding: 5px 10px; \u2026 }","title":"Less"},{"location":"atomic-structure/#javascript","text":"None of our atoms require any JavaScript at this time.","title":"JavaScript"},{"location":"atomic-structure/#molecules","text":"The medium-sized component. May contain atoms. Prefixed with m- in class names.","title":"Molecules"},{"location":"atomic-structure/#html_1","text":"<div class=\"m-notification m-notification__visible m-notification__error\" data-js-hook=\"state_atomic_init\"> {{ svg_icon('error') }} <div class=\"m-notification_content\" role=\"alert\"> <div class=\"h4 m-notification_message\">Page not found.</div> </div> </div>","title":"HTML"},{"location":"atomic-structure/#less_1","text":".m-notification { display: none; position: relative; padding: @notification-padding__px; \u2026 }","title":"Less"},{"location":"atomic-structure/#javascript_1","text":"function Notification( element ) { const BASE_CLASS = 'm-notification'; // Constants for the state of this Notification. const SUCCESS = 'success'; const WARNING = 'warning'; const ERROR = 'error'; // Constants for the Notification modifiers. const MODIFIER_VISIBLE = BASE_CLASS + '__visible'; const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _contentDom = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Notification molecule can be instantiated by adding the following to your project's JavaScript code: const notification = new Notification( _dom ); notification.init();","title":"JavaScript"},{"location":"atomic-structure/#organisms","text":"The largest component. May contain atoms, molecules, or (if no other solution is viable) other organisms. Prefixed with o- in class names.","title":"Organisms"},{"location":"atomic-structure/#html_2","text":"<div class=\"o-expandable o-expandable__borders o-expandable__midtone o-expandable__expanded\" data-js-hook=\"state_atomic_init\"> <button class=\"o-expandable_target\" aria-pressed=\"true\"> <div class=\"o-expandable_header\"> \u2026","title":"HTML"},{"location":"atomic-structure/#less_2","text":".o-expandable { position: relative; &_target { padding: 0; border: 0; \u2026 } \u2026 }","title":"Less"},{"location":"atomic-structure/#javascript_2","text":"function Expandable( element ) { const BASE_CLASS = 'o-expandable'; // Bitwise flags for the state of this Expandable. const COLLAPSED = 0; const COLLAPSING = 1; const EXPANDING = 2; const EXPANDED = 3; // The Expandable element will directly be the Expandable // when used in an ExpandableGroup, otherwise it can be the parent container. const _dom = atomicHelpers.checkDom( element, BASE_CLASS ); const _target = _dom.querySelector( '.' + BASE_CLASS + '_target' ); const _content = _dom.querySelector( '.' + BASE_CLASS + '_content' ); \u2026 } The Expandable organism can be instantiated by adding the following to your project's JavaScript code: const expandable = new Expandable( _dom.querySelector( '.o-expandable' ) ); expandable.init( _expandable.EXPANDED ); or const atomicHelpers = require( '@cfpb/cfpb-atomic-component/src/utilities/atomic-helpers.js' ); const Expandable = require( '../../organisms/Expandable' ); atomicHelpers.instantiateAll( '.o-expandable', Expandable );","title":"JavaScript"},{"location":"atomic-structure/#folder-structure","text":"Our atomic components are separated and named based on asset type. HTML, Less, and JavaScript for each component are in separate directories.","title":"Folder structure"},{"location":"atomic-structure/#html_3","text":"consumerfinance.gov/cfgov/jinja2/v1/_includes/atoms/ consumerfinance.gov/cfgov/jinja2/v1/_includes/molecules/ consumerfinance.gov/cfgov/jinja2/v1/_includes/organisms/ Note Some of our foundational components get their Less and JavaScript from the Design System , but the HTML for their Wagtail block templates is stored in the above folders.","title":"HTML"},{"location":"atomic-structure/#css","text":"consumerfinance.gov/cfgov/unprocessed/css/atoms/ consumerfinance.gov/cfgov/unprocessed/css/molecules/ consumerfinance.gov/cfgov/unprocessed/css/organisms/","title":"CSS"},{"location":"atomic-structure/#javascript_3","text":"consumerfinance.gov/cfgov/unprocessed/js/molecules/ consumerfinance.gov/cfgov/unprocessed/js/organisms/","title":"JavaScript"},{"location":"atomic-structure/#tests","text":"consumerfinance.gov/test/unit_tests/js/molecules/ consumerfinance.gov/test/unit_tests/js/organisms/","title":"Tests"},{"location":"atomic-structure/#javascript-architecture","text":"JavaScript components are built to be rendered on the server and then enhanced via JavaScript on the client. The basic interface for the components is as follows: function AtomicComponent( domElement ) { // Ensure the passed in Element is in the DOM. // Query and store references to sub-elements. // Instantiate child atomic components. // Bind necessary events for referenced DOM elements. // Perform other initialization related tasks. this.init = function init(){} // General teardown function // We don't remove the element from the DOM so // we need to unbind the events. this.destroy = function destroy(){} } We generally favor composition over inheritance. You can get more information by reading the following: A Simple Challenge to Classical Inheritance Fans Composition over Inheritance (YouTube)","title":"JavaScript architecture"},{"location":"atomic-structure/#component-build-pipeline","text":"","title":"Component build pipeline"},{"location":"atomic-structure/#gulp","text":"Gulp is used as a task automation tool. Tasks include compiling CSS, creating a standard Webpack workflow for bundling scripts, minifying code, linting, running unit tests, and more .","title":"Gulp"},{"location":"atomic-structure/#webpack","text":"Wepback is used as a module bundler, although it's capable of more. We create page, global, and component-specific bundles. The configuration for the bundles is contained in config/webpack-config.js . An explanation for the usage of each bundle is contained in gulp/tasks/scripts.js .","title":"Webpack"},{"location":"atomic-structure/#routes","text":"Routes are used to serve JavaScript bundles to the browser based on the requested URL or Wagtail page's Media definition. This happens via code contained in base.html . This file serves as the base HTML template for serving Wagtail pages.","title":"Routes"},{"location":"atomic-structure/#wagtail-page-media-class","text":"Each atomic component has a Media class that lists the JavaScript files that should be loaded via base.html . When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is an example of the Media class on a component, the EmailSignUp organism : class Media: js = ['email-signup.js'] This will load the email-signup.js script on any page that includes the EmailSignUp organism in one of its StreamFields.","title":"Wagtail page Media class"},{"location":"branching-merging/","text":"Branching and merging \u00b6 Branches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into main. All pull requests to merge into main must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to main with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate. When reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review. The consumerfinance.gov repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to main must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass. Pull requests that are not to main must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such. Generally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to main. Teams are responsible for governing their own branches and forks, these guidelines apply to main.","title":"Branching and Merging"},{"location":"branching-merging/#branching-and-merging","text":"Branches should be named descriptively, preferably in some way that indicates whether they are short-lived feature branches or longer-lived development branches. Short-lived feature branches should be deleted once they are merged into main. All pull requests to merge into main must be reviewed by at least one member of the cf.gov platform team. The cf.gov platform team will ensure that these reviews happen in a timely manner. To ensure timely code reviews, please tag all PRs to main with @cfpb/cfgov-backends and @cfpb/cfgov-frontends as appropriate. When reviewing pull requests, it is important to distinguish between explicit blockers and things that can be addressed in the future or would be nice to have. The latter two can be indicated with 'TODO'. This is best as a simple top-level post after review to summarize the review. The consumerfinance.gov repository makes use of automated testing and linting to ensure the quality, consistency, and readability of the codebase. All pull requests to main must pass all automated tests and must not reduce the code coverage of the codebase. It is the responsibility of the submitter to ensure that the tests pass. Pull requests that are not to main must use GitHub labels in such a way that individuals who are responsible for reviewing those pull requests can easily find them. Pull requests that are works-in-progress must be clearly labeled as such. Generally, teams working on cf.gov projects should create and collaborate on feature branches, with frequent merges back to main. Teams are responsible for governing their own branches and forks, these guidelines apply to main.","title":"Branching and merging"},{"location":"caching/","text":"Caching \u00b6 Akamai \u00b6 We use Akamai , a content delivery network, to cache the entirety of www.consumerfinance.gov (but not our development servers). We invalidate any given page in Wagtail when it is published or unpublished (by hooking up the custom class AkamaiBackend to Wagtail's frontend cache invalidator . By default, we clear the Akamai cache any time we deploy. There are certain pages that do not live in Wagtail or are impacted by changes on another page (imagine our newsroom page that lists titles of other pages) or another process (imagine data from Socrata gets updated) and thus will display outdated content until the page's time to live (TTL) has expired, a deploy has happened, or if someone manually invalidates that page. Our default TTL is 24 hours. Checking the cache state of a URL \u00b6 To get the current cache state of a URL (perhaps to see if that URL has been invalidated), you can use the following curl command to check the X-Cache header: curl -sI -H \"Pragma: akamai-x-cache-on\" https://beta.consumerfinance.gov | grep \"X-Cache\" It will return something like: X-Cache: TCP_HIT from a23-46-239-53.deploy.akamaitechnologies.com (AkamaiGHost/9.5.4-24580776) (-) The possible cache state values are: TCP_HIT : The object was fresh in cache and object from disk cache. TCP_MISS : The object was not in cache, server fetched object from origin. TCP_REFRESH_HIT : The object was stale in cache and we successfully refreshed with the origin on an If-modified-Since request. TCP_REFRESH_MISS : Object was stale in cache and refresh obtained a new object from origin in response to our IF-Modified-Since request. TCP_REFRESH_FAIL_HIT : Object was stale in cache and we failed on refresh (couldn't reach origin) so we served the stale object. TCP_IMS_HIT : IF-Modified-Since request from client and object was fresh in cache and served. TCP_NEGATIVE_HIT : Object previously returned a \"not found\" (or any other negatively cacheable response) and that cached response was a hit for this new request. TCP_MEM_HIT : Object was on disk and in the memory cache. Server served it without hitting the disk. TCP_DENIED : Denied access to the client for whatever reason. TCP_COOKIE_DENY : Denied access on cookie authentication (if centralized or decentralized authorization feature is being used in config). Django caching \u00b6 Starting in December 2017, we use template fragment caching to cache all or part of a template. It is enabled on our \"post previews\", snippets of a page that we display on results of filterable pages (e.g. our blog page & research & reports ). It can easily be enabled on other templates. See this PR as an example of the code that would need to be introduced to cache a new fragment. When a page gets published, it will update the post preview cache for that particular page. However, if there are code changes that impact the page's content, or the post preview template itself gets updated, the entire post preview cache will need to be manually cleared. Clearing this particular cache could be an option when deploying, as it is with Akamai, but should not be a default since most deploys wouldn't impact the code in question. Currently, the manual way to do this would be to run the following from a production server's django shell: from django.core.cache import caches caches['post_preview'].clear() To run the application locally with caching for post previews enabled, run ENABLE_POST_PREVIEW_CACHE=1 ./runserver.sh Alternatively, add this variable to your .env if you generally want it enabled locally. Due to the impossibility/difficulty/complexity of caching individual Wagtail blocks (they are not serializable) and invalidating content that does not have some type of post_save hook (e.g. Taggit models), we have started with caching segments that are tied to a Wagtail page (which can be easily invalidated using the page_published Wagtail signal), hence the post previews. With more research or improvements to these third-party libraries, it is possible we could expand Django-level caching to more content.","title":"Caching"},{"location":"caching/#caching","text":"","title":"Caching"},{"location":"caching/#akamai","text":"We use Akamai , a content delivery network, to cache the entirety of www.consumerfinance.gov (but not our development servers). We invalidate any given page in Wagtail when it is published or unpublished (by hooking up the custom class AkamaiBackend to Wagtail's frontend cache invalidator . By default, we clear the Akamai cache any time we deploy. There are certain pages that do not live in Wagtail or are impacted by changes on another page (imagine our newsroom page that lists titles of other pages) or another process (imagine data from Socrata gets updated) and thus will display outdated content until the page's time to live (TTL) has expired, a deploy has happened, or if someone manually invalidates that page. Our default TTL is 24 hours.","title":"Akamai"},{"location":"caching/#checking-the-cache-state-of-a-url","text":"To get the current cache state of a URL (perhaps to see if that URL has been invalidated), you can use the following curl command to check the X-Cache header: curl -sI -H \"Pragma: akamai-x-cache-on\" https://beta.consumerfinance.gov | grep \"X-Cache\" It will return something like: X-Cache: TCP_HIT from a23-46-239-53.deploy.akamaitechnologies.com (AkamaiGHost/9.5.4-24580776) (-) The possible cache state values are: TCP_HIT : The object was fresh in cache and object from disk cache. TCP_MISS : The object was not in cache, server fetched object from origin. TCP_REFRESH_HIT : The object was stale in cache and we successfully refreshed with the origin on an If-modified-Since request. TCP_REFRESH_MISS : Object was stale in cache and refresh obtained a new object from origin in response to our IF-Modified-Since request. TCP_REFRESH_FAIL_HIT : Object was stale in cache and we failed on refresh (couldn't reach origin) so we served the stale object. TCP_IMS_HIT : IF-Modified-Since request from client and object was fresh in cache and served. TCP_NEGATIVE_HIT : Object previously returned a \"not found\" (or any other negatively cacheable response) and that cached response was a hit for this new request. TCP_MEM_HIT : Object was on disk and in the memory cache. Server served it without hitting the disk. TCP_DENIED : Denied access to the client for whatever reason. TCP_COOKIE_DENY : Denied access on cookie authentication (if centralized or decentralized authorization feature is being used in config).","title":"Checking the cache state of a URL"},{"location":"caching/#django-caching","text":"Starting in December 2017, we use template fragment caching to cache all or part of a template. It is enabled on our \"post previews\", snippets of a page that we display on results of filterable pages (e.g. our blog page & research & reports ). It can easily be enabled on other templates. See this PR as an example of the code that would need to be introduced to cache a new fragment. When a page gets published, it will update the post preview cache for that particular page. However, if there are code changes that impact the page's content, or the post preview template itself gets updated, the entire post preview cache will need to be manually cleared. Clearing this particular cache could be an option when deploying, as it is with Akamai, but should not be a default since most deploys wouldn't impact the code in question. Currently, the manual way to do this would be to run the following from a production server's django shell: from django.core.cache import caches caches['post_preview'].clear() To run the application locally with caching for post previews enabled, run ENABLE_POST_PREVIEW_CACHE=1 ./runserver.sh Alternatively, add this variable to your .env if you generally want it enabled locally. Due to the impossibility/difficulty/complexity of caching individual Wagtail blocks (they are not serializable) and invalidating content that does not have some type of post_save hook (e.g. Taggit models), we have started with caching segments that are tied to a Wagtail page (which can be easily invalidated using the page_published Wagtail signal), hence the post previews. With more research or improvements to these third-party libraries, it is possible we could expand Django-level caching to more content.","title":"Django caching"},{"location":"consumer-complaint-database/","text":"Consumer Complaint Database API \u00b6 A resource for searching complaints submitted to the CFPB \u00b6 Each week the CFPB sends thousands of consumers\u2019 complaints about financial products and services to companies for response. Those complaints are published on our website after the company responds or after 15 days, whichever comes first. The API allows automation of the same filtering and searching functions offered to website visitors. Detailed documentation for the search API can be found here . Notes \u00b6 The database generally updates daily, and contains certain information for each complaint, including the source of the complaint, the date of submission, and the company the complaint was sent to for response. The database also includes information about the actions taken by the company in response to the complaint, such as, whether the company\u2019s response was timely and how the company responded. If the consumer opts to share it and after we take steps to remove personal information, we publish the consumer\u2019s description of what happened. Companies also have the option to select a public response. Company level information should be considered in context of company size and/or market share. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database.","title":"Consumer Complaints"},{"location":"consumer-complaint-database/#consumer-complaint-database-api","text":"","title":"Consumer Complaint Database API"},{"location":"consumer-complaint-database/#a-resource-for-searching-complaints-submitted-to-the-cfpb","text":"Each week the CFPB sends thousands of consumers\u2019 complaints about financial products and services to companies for response. Those complaints are published on our website after the company responds or after 15 days, whichever comes first. The API allows automation of the same filtering and searching functions offered to website visitors. Detailed documentation for the search API can be found here .","title":"A resource for searching complaints submitted to the CFPB"},{"location":"consumer-complaint-database/#notes","text":"The database generally updates daily, and contains certain information for each complaint, including the source of the complaint, the date of submission, and the company the complaint was sent to for response. The database also includes information about the actions taken by the company in response to the complaint, such as, whether the company\u2019s response was timely and how the company responded. If the consumer opts to share it and after we take steps to remove personal information, we publish the consumer\u2019s description of what happened. Companies also have the option to select a public response. Company level information should be considered in context of company size and/or market share. Complaints referred to other regulators, such as complaints about depository institutions with less than $10 billion in assets, are not published in the Consumer Complaint Database.","title":"Notes"},{"location":"contributing-docs/","text":"Contributing to the Docs \u00b6 Our documentation is written as Markdown files and served via GitHub Pages by MkDocs . Writing the docs \u00b6 As our documentation is written in Markdown, the base Markdown specification is a useful reference. MkDocs also includes some documentation to get you started writing in Markdown . In addition to standard Markdown, our documentation supports the following extensions: Admonitions adds specially called-out text anywhere within the document as notes, warnings, and other types. BetterEm improves the handling of bold and italics. MagicLink provides automatic linking for URLs in the Markdown text. SuperFences makes a number of improvements to standard Markdown code fences. Tilde adds support for creating <del></del> tags with ~~ . Tables adds support for tables to standard Markdown. When creating new documents, they should be added to the mkdocs.yml file in the appropriate place under nav: to get them to appear in the sidebar navigation. For example: nav: - Introduction: index.md Running the docs locally \u00b6 With Docker \u00b6 When running consumerfinance.gov using Docker-compose , this documentation is running by default at http://localhost:8888 . Manually \u00b6 When using the stand-alone installation of consumerfinance.gov, you can run these docs with: workon consumerfinance.gov pip install -r requirements/docs.txt mkdocs serve -a :8888 Once running, they are accessible at http://localhost:8888 . Deploying the docs to GitHub Pages \u00b6 Every time a PR is merged to main, GitHub Actions will build and deploy the documentation to https://cfpb.github.io/consumerfinance.gov/ . See How we use GitHub Actions for more info. If you would like to deploy to a fork of consumerfinance.gov owned by another user you can provide the -r argument: mkdocs gh-deploy -r USER Where USER is the GitHub user. The docs will then be available at https://USER.github.io/consumerfinance.gov/ after a short period of time. See the the MkDocs documentation for more information.","title":"Contributing to the Docs"},{"location":"contributing-docs/#contributing-to-the-docs","text":"Our documentation is written as Markdown files and served via GitHub Pages by MkDocs .","title":"Contributing to the Docs"},{"location":"contributing-docs/#writing-the-docs","text":"As our documentation is written in Markdown, the base Markdown specification is a useful reference. MkDocs also includes some documentation to get you started writing in Markdown . In addition to standard Markdown, our documentation supports the following extensions: Admonitions adds specially called-out text anywhere within the document as notes, warnings, and other types. BetterEm improves the handling of bold and italics. MagicLink provides automatic linking for URLs in the Markdown text. SuperFences makes a number of improvements to standard Markdown code fences. Tilde adds support for creating <del></del> tags with ~~ . Tables adds support for tables to standard Markdown. When creating new documents, they should be added to the mkdocs.yml file in the appropriate place under nav: to get them to appear in the sidebar navigation. For example: nav: - Introduction: index.md","title":"Writing the docs"},{"location":"contributing-docs/#running-the-docs-locally","text":"","title":"Running the docs locally"},{"location":"contributing-docs/#with-docker","text":"When running consumerfinance.gov using Docker-compose , this documentation is running by default at http://localhost:8888 .","title":"With Docker"},{"location":"contributing-docs/#manually","text":"When using the stand-alone installation of consumerfinance.gov, you can run these docs with: workon consumerfinance.gov pip install -r requirements/docs.txt mkdocs serve -a :8888 Once running, they are accessible at http://localhost:8888 .","title":"Manually"},{"location":"contributing-docs/#deploying-the-docs-to-github-pages","text":"Every time a PR is merged to main, GitHub Actions will build and deploy the documentation to https://cfpb.github.io/consumerfinance.gov/ . See How we use GitHub Actions for more info. If you would like to deploy to a fork of consumerfinance.gov owned by another user you can provide the -r argument: mkdocs gh-deploy -r USER Where USER is the GitHub user. The docs will then be available at https://USER.github.io/consumerfinance.gov/ after a short period of time. See the the MkDocs documentation for more information.","title":"Deploying the docs to GitHub Pages"},{"location":"debugging-monitoring/","text":"Debugging and Monitoring \u00b6 Using Django Debug Toolbar \u00b6 When running locally it is possible to enable the Django Debug Toolbar by defining the ENABLE_DEBUG_TOOLBAR environment variable: $ ENABLE_DEBUG_TOOLBAR=1 ./runserver.sh This tool exposes various useful pieces of information about things like HTTP headers, Django settings, SQL queries, and template variables. Note that running with the toolbar on may have an impact on local server performance. Logging database queries to console \u00b6 To log database queries to the console when running locally, define the ENABLE_SQL_LOGGING environment variable: $ ENABLE_SQL_LOGGING=1 cfgov/manage.py shell >>> from django.contrib.auth import get_user_model >>> User = get_user_model() >>> User.objects.count() (0.004) SELECT COUNT(*) AS \"__count\" FROM \"auth_user\"; args=() 97 This will log any database queries (and time taken to execute them) to the console, and works with any Django code invoked from the shell, including the server and management commands: $ ENABLE_SQL_LOGGING=1 ./runserver.sh $ ENABLE_SQL_LOGGING=1 cfgov/manage.py some_management_command Logging Elasticsearch queries to console \u00b6 Similarly, to log Elasticsearch queries to the console when running locally, define the ENABLE_ES_LOGGING environment variable. This will log any Elasticsearch queries (and time taken to execute them) to the console, and works with any Django code invoked from the shell, including the server and management commands: $ ENABLE_ES_LOGGING=1 ./runserver.sh $ ENABLE_ES_LOGGING=1 cfgov/manage.py some_management_command Monitoring using New Relic \u00b6 This project can be configured for real-time monitoring with New Relic . This requires an active New Relic account and a valid license key. First, you'll need to install the New Relic Python Agent from from PyPI . This package is not part of the standard development requirements but is included in the requirements/deployment.txt file. It can be installed into your Python environment using the following command: $ pip install newrelic New Relic monitoring is implemented by wrapping every request to the Django web server with a call to the Python agent. This agent records information about web requests and reports it back to New Relic's servers. Additionally, and depending on configuration, the agent also injects a bit of JavaScript into every page header allowing for monitoring of client-side performance in the web browser. The New Relic Python agent has many configuration settings that control the information that gets recorded about web requests. This project includes a default newrelic.ini file that enables New Relic's high security mode to limit the recording of potentially sensitive information. To activate New Relic monitoring of your instance of cf.gov, you'll need to set two environment variables: NEW_RELIC_LICENSE_KEY : a valid New Relic license key NEW_RELIC_APP_NAME : a unique identifier to describe your running application Make sure to choose a unique application name to help distinguish your web traffic from any other users who may be running against the same New Relic account. This project's .env_SAMPLE file contains placeholder entries for these two variables. Once you've set these two variables, start or restart your local web server. When you make your first web request, you'll see messages in the console indicating that the New Relic Python agent has been activated. You should also see a message containing a link to the New Relic console: Reporting to: https://rpm.newrelic.com/accounts/XXXXXXXX/applications/XXXXXXXX You should now be able to use that link to navigate to the New Relic console and, after a few seconds, see your web server traffic in New Relic APM and New Relic Browser.","title":"Debugging and Monitoring"},{"location":"debugging-monitoring/#debugging-and-monitoring","text":"","title":"Debugging and Monitoring"},{"location":"debugging-monitoring/#using-django-debug-toolbar","text":"When running locally it is possible to enable the Django Debug Toolbar by defining the ENABLE_DEBUG_TOOLBAR environment variable: $ ENABLE_DEBUG_TOOLBAR=1 ./runserver.sh This tool exposes various useful pieces of information about things like HTTP headers, Django settings, SQL queries, and template variables. Note that running with the toolbar on may have an impact on local server performance.","title":"Using Django Debug Toolbar"},{"location":"debugging-monitoring/#logging-database-queries-to-console","text":"To log database queries to the console when running locally, define the ENABLE_SQL_LOGGING environment variable: $ ENABLE_SQL_LOGGING=1 cfgov/manage.py shell >>> from django.contrib.auth import get_user_model >>> User = get_user_model() >>> User.objects.count() (0.004) SELECT COUNT(*) AS \"__count\" FROM \"auth_user\"; args=() 97 This will log any database queries (and time taken to execute them) to the console, and works with any Django code invoked from the shell, including the server and management commands: $ ENABLE_SQL_LOGGING=1 ./runserver.sh $ ENABLE_SQL_LOGGING=1 cfgov/manage.py some_management_command","title":"Logging database queries to console"},{"location":"debugging-monitoring/#logging-elasticsearch-queries-to-console","text":"Similarly, to log Elasticsearch queries to the console when running locally, define the ENABLE_ES_LOGGING environment variable. This will log any Elasticsearch queries (and time taken to execute them) to the console, and works with any Django code invoked from the shell, including the server and management commands: $ ENABLE_ES_LOGGING=1 ./runserver.sh $ ENABLE_ES_LOGGING=1 cfgov/manage.py some_management_command","title":"Logging Elasticsearch queries to console"},{"location":"debugging-monitoring/#monitoring-using-new-relic","text":"This project can be configured for real-time monitoring with New Relic . This requires an active New Relic account and a valid license key. First, you'll need to install the New Relic Python Agent from from PyPI . This package is not part of the standard development requirements but is included in the requirements/deployment.txt file. It can be installed into your Python environment using the following command: $ pip install newrelic New Relic monitoring is implemented by wrapping every request to the Django web server with a call to the Python agent. This agent records information about web requests and reports it back to New Relic's servers. Additionally, and depending on configuration, the agent also injects a bit of JavaScript into every page header allowing for monitoring of client-side performance in the web browser. The New Relic Python agent has many configuration settings that control the information that gets recorded about web requests. This project includes a default newrelic.ini file that enables New Relic's high security mode to limit the recording of potentially sensitive information. To activate New Relic monitoring of your instance of cf.gov, you'll need to set two environment variables: NEW_RELIC_LICENSE_KEY : a valid New Relic license key NEW_RELIC_APP_NAME : a unique identifier to describe your running application Make sure to choose a unique application name to help distinguish your web traffic from any other users who may be running against the same New Relic account. This project's .env_SAMPLE file contains placeholder entries for these two variables. Once you've set these two variables, start or restart your local web server. When you make your first web request, you'll see messages in the console indicating that the New Relic Python agent has been activated. You should also see a message containing a link to the New Relic console: Reporting to: https://rpm.newrelic.com/accounts/XXXXXXXX/applications/XXXXXXXX You should now be able to use that link to navigate to the New Relic console and, after a few seconds, see your web server traffic in New Relic APM and New Relic Browser.","title":"Monitoring using New Relic"},{"location":"debugging-templates/","text":"Debugging Templates \u00b6 Module templates can be debugged visually through use of a TemplateDebugView that renders a single module with a series of test cases. Consider an example module that renders a simple hyperlink. This module requires a target URL and also accepts optional link text. A simple Jinja template for this module might look like this: <a href=\"{{ value.url }}\"> {{ value.text | default( value.url, true ) }} </a> Defining template test cases \u00b6 Next, define test cases for the module that cover all supported input configurations. For this simple link module, there are only a few useful test cases, but more complicated modules might have many more. Test cases should be defined as a Python dict, where the key is a string name of the test case and the value is a dict that will be passed to the module template. # myapp/template_debug.py link_test_cases = { 'Link without text': { 'url': 'https://www.consumerfinance.gov', }, 'Link with empty text': { 'url': 'https://www.consumerfinance.gov', 'text': '', }, 'Link with text': { 'url': 'https://www.consumerfinance.gov', 'text': 'Visit our website', }, } Registering the template debug view \u00b6 The next step is to register the template debug view with Django so that it can be loaded in a browser. # in myapp/wagtail_hooks.py from myapp.template_debug import link_test_cases from v1.template_debug import register_template_debug register_template_debug('myapp', 'link', 'myapp/link.html', link_test_cases) Once logged into the Wagtail admin, the template debug view for this module will now be available at the /admin/template_debug/myapp/link/ URL. Including component JavaScript \u00b6 Associated JavaScript required by the module can be included in the template debug view by listing it in the register_template_debug call: register_template_debug( 'myapp', 'link', 'myapp/link.html', link_test_cases extra_js=['link.js'] )","title":"Debugging Templates"},{"location":"debugging-templates/#debugging-templates","text":"Module templates can be debugged visually through use of a TemplateDebugView that renders a single module with a series of test cases. Consider an example module that renders a simple hyperlink. This module requires a target URL and also accepts optional link text. A simple Jinja template for this module might look like this: <a href=\"{{ value.url }}\"> {{ value.text | default( value.url, true ) }} </a>","title":"Debugging Templates"},{"location":"debugging-templates/#defining-template-test-cases","text":"Next, define test cases for the module that cover all supported input configurations. For this simple link module, there are only a few useful test cases, but more complicated modules might have many more. Test cases should be defined as a Python dict, where the key is a string name of the test case and the value is a dict that will be passed to the module template. # myapp/template_debug.py link_test_cases = { 'Link without text': { 'url': 'https://www.consumerfinance.gov', }, 'Link with empty text': { 'url': 'https://www.consumerfinance.gov', 'text': '', }, 'Link with text': { 'url': 'https://www.consumerfinance.gov', 'text': 'Visit our website', }, }","title":"Defining template test cases"},{"location":"debugging-templates/#registering-the-template-debug-view","text":"The next step is to register the template debug view with Django so that it can be loaded in a browser. # in myapp/wagtail_hooks.py from myapp.template_debug import link_test_cases from v1.template_debug import register_template_debug register_template_debug('myapp', 'link', 'myapp/link.html', link_test_cases) Once logged into the Wagtail admin, the template debug view for this module will now be available at the /admin/template_debug/myapp/link/ URL.","title":"Registering the template debug view"},{"location":"debugging-templates/#including-component-javascript","text":"Associated JavaScript required by the module can be included in the template debug view by listing it in the register_template_debug call: register_template_debug( 'myapp', 'link', 'myapp/link.html', link_test_cases extra_js=['link.js'] )","title":"Including component JavaScript"},{"location":"deployment/","text":"This repository includes code for generating a self-extracting zip archive of the code and all of its Python dependencies. We use these archives to deploy the site to a Linux server. Generating a deployment artifact \u00b6 Running the script at ./docker/deployable-zipfile/build.sh will start a CentOS 6 container, generate the artifact (via this script ), and save it to ./cfgov_current_build.zip . We use CentOS 6 here, so that the Python modules that include compiled code, will be compiled for the same environment they will be run in. What's in an artifact? \u00b6 Here's a (very abbreviated) peek into what's in the zip file: __main__.py cfgov.pth install_wheels.py loadenv.py loadenv-init.pth wheels/botocore-1.10.84-py2.py3-none-any.whl wheels/tqdm-4.15.0-py2.py3-none-any.whl ... static.in/0/ static.in/0/fonts/ static.in/0/fonts/8344e877-560d-44d4-82eb-9822766676f9.woff ... bootstrap_wheels/setuptools-41.2.0-py2.py3-none-any.whl bootstrap_wheels/pip-19.2.3-py2.py3-none-any.whl cfgov/... The __main__.py file contains the code that runs when the zip file is invoked as a Python module . The wheels/ directory contains all of our python dependencies, while bootstrap_wheels/ contains modules needed at deployment time, to install everything. loadenv.py and loadenv-init.pth are used to load environment variables from a environment.json file, deployed seperately. Before the final version of the zip file is saved, we prepend a python \"shebang\" line, #!/usr/bin/env python . This, combined with __main__.py described above, is what makes the file executable and self-extracting. Deploying an artifact \u00b6 We currently use Ansible to prepare servers to run an artifact, and to do the actual deployment. If we ignore some specifics and quirks of our environment, the basic steps look something like this: copy the artifact to the system execute the artifact, with the -d (destination) argument, ./artifact.zip -d destination-dir . This will unpack the files, create a new virtualenv, and install all of the wheels in wheels/ into that virtualenv. Important Note: This should be done with the same Python interpreter that will run the application. For example, on our servers this means using scl enable to specify a particular Python version from Software Collections . put an environment.json file in place, in your destination-dir run Django utilities like 'collectstatic' and 'migrate' update a symlink to point to the latest release restart your WSGI server.","title":"Artifacts and Deployment"},{"location":"deployment/#generating-a-deployment-artifact","text":"Running the script at ./docker/deployable-zipfile/build.sh will start a CentOS 6 container, generate the artifact (via this script ), and save it to ./cfgov_current_build.zip . We use CentOS 6 here, so that the Python modules that include compiled code, will be compiled for the same environment they will be run in.","title":"Generating a deployment artifact"},{"location":"deployment/#whats-in-an-artifact","text":"Here's a (very abbreviated) peek into what's in the zip file: __main__.py cfgov.pth install_wheels.py loadenv.py loadenv-init.pth wheels/botocore-1.10.84-py2.py3-none-any.whl wheels/tqdm-4.15.0-py2.py3-none-any.whl ... static.in/0/ static.in/0/fonts/ static.in/0/fonts/8344e877-560d-44d4-82eb-9822766676f9.woff ... bootstrap_wheels/setuptools-41.2.0-py2.py3-none-any.whl bootstrap_wheels/pip-19.2.3-py2.py3-none-any.whl cfgov/... The __main__.py file contains the code that runs when the zip file is invoked as a Python module . The wheels/ directory contains all of our python dependencies, while bootstrap_wheels/ contains modules needed at deployment time, to install everything. loadenv.py and loadenv-init.pth are used to load environment variables from a environment.json file, deployed seperately. Before the final version of the zip file is saved, we prepend a python \"shebang\" line, #!/usr/bin/env python . This, combined with __main__.py described above, is what makes the file executable and self-extracting.","title":"What's in an artifact?"},{"location":"deployment/#deploying-an-artifact","text":"We currently use Ansible to prepare servers to run an artifact, and to do the actual deployment. If we ignore some specifics and quirks of our environment, the basic steps look something like this: copy the artifact to the system execute the artifact, with the -d (destination) argument, ./artifact.zip -d destination-dir . This will unpack the files, create a new virtualenv, and install all of the wheels in wheels/ into that virtualenv. Important Note: This should be done with the same Python interpreter that will run the application. For example, on our servers this means using scl enable to specify a particular Python version from Software Collections . put an environment.json file in place, in your destination-dir run Django utilities like 'collectstatic' and 'migrate' update a symlink to point to the latest release restart your WSGI server.","title":"Deploying an artifact"},{"location":"development-tips/","text":"Development tips \u00b6 Main front-end template/asset locations \u00b6 Templates that are served by the Django server: cfgov/jinja2/v1 Static assets prior to processing (combilation, minification, etc.): cfgov/unprocessed . Note After running gulp build (or ./setup.sh ) the site's assets are copied over to cfgov/static_built , ready to be served by Django. Installing new front-end dependencies \u00b6 Use yarn add new_dep@se.m.ver to install new dependencies or update existing dependencies. If you can't do this for some reason or are looking to freshen all dependencies, you will need to edit .yarnrc , temporarily commenting out the --install.pure-lockfile true and --install.offline true flags before proceeding with your installation or update. In the rare but observed case that yarn add new_dep@se.m.ver doesn't add every needed package to the offline cache, you likely need to first run yarn cache clean . Watching files for changes \u00b6 Some (but not all) JavaScript and CSS files can be rebuilt automatically when they are changed by using gulp watch or yarn run gulp watch . Note You must build the assets first, so you may want a command like: ./setup.sh docker && yarn run gulp watch Developing on nested satellite apps \u00b6 Some projects can sit inside consumerfinance.gov, but manage their own asset dependencies. These projects have their own package.json and base templates. The structure looks like this: npm modules \u00b6 List an app's own dependencies in cfgov/unprocessed/apps/[project namespace]/package.json . Webpack \u00b6 Apps may include their own webpack-config.js configuration that adjusts how their app-specific assets should be built. This configuration appears in cfgov/unprocessed/apps/[project namespace]/webpack-config.js . Browserslist \u00b6 Apps may include a browserslist config file, which is automatically picked up by @babel/preset-env inside the webpack config, if no browsers option is supplied. Adding Images \u00b6 Images should be compressed and optimized before being committed to the repo In order to keep builds fast and reduce dependencies, the front-end build does not contain an image optimization step A suggested workflow for those with Adobe Creative Suite is as follows: Export a full-quality PNG from Adobe Illustrator Reexport that PNG from Adobe Fireworks as an 8-bit PNG Run the 8-bit PNG through ImageOptim Templates \u00b6 Apps use a Jinja template that extends the base.html template used by the rest of the site. This template would reside in cfgov/jinja2/v1/[project namespace]/index.html or similar (for example, owning-a-home ). Note A template may support a non-standard browser, like an older IE version, by including the required dependencies, polyfills, etc. in its template's {% block css %} or {% block javascript scoped %} blocks.","title":"Development Tips"},{"location":"development-tips/#development-tips","text":"","title":"Development tips"},{"location":"development-tips/#main-front-end-templateasset-locations","text":"Templates that are served by the Django server: cfgov/jinja2/v1 Static assets prior to processing (combilation, minification, etc.): cfgov/unprocessed . Note After running gulp build (or ./setup.sh ) the site's assets are copied over to cfgov/static_built , ready to be served by Django.","title":"Main front-end template/asset locations"},{"location":"development-tips/#installing-new-front-end-dependencies","text":"Use yarn add new_dep@se.m.ver to install new dependencies or update existing dependencies. If you can't do this for some reason or are looking to freshen all dependencies, you will need to edit .yarnrc , temporarily commenting out the --install.pure-lockfile true and --install.offline true flags before proceeding with your installation or update. In the rare but observed case that yarn add new_dep@se.m.ver doesn't add every needed package to the offline cache, you likely need to first run yarn cache clean .","title":"Installing new front-end dependencies"},{"location":"development-tips/#watching-files-for-changes","text":"Some (but not all) JavaScript and CSS files can be rebuilt automatically when they are changed by using gulp watch or yarn run gulp watch . Note You must build the assets first, so you may want a command like: ./setup.sh docker && yarn run gulp watch","title":"Watching files for changes"},{"location":"development-tips/#developing-on-nested-satellite-apps","text":"Some projects can sit inside consumerfinance.gov, but manage their own asset dependencies. These projects have their own package.json and base templates. The structure looks like this:","title":"Developing on nested satellite apps"},{"location":"development-tips/#npm-modules","text":"List an app's own dependencies in cfgov/unprocessed/apps/[project namespace]/package.json .","title":"npm modules"},{"location":"development-tips/#webpack","text":"Apps may include their own webpack-config.js configuration that adjusts how their app-specific assets should be built. This configuration appears in cfgov/unprocessed/apps/[project namespace]/webpack-config.js .","title":"Webpack"},{"location":"development-tips/#browserslist","text":"Apps may include a browserslist config file, which is automatically picked up by @babel/preset-env inside the webpack config, if no browsers option is supplied.","title":"Browserslist"},{"location":"development-tips/#adding-images","text":"Images should be compressed and optimized before being committed to the repo In order to keep builds fast and reduce dependencies, the front-end build does not contain an image optimization step A suggested workflow for those with Adobe Creative Suite is as follows: Export a full-quality PNG from Adobe Illustrator Reexport that PNG from Adobe Fireworks as an 8-bit PNG Run the 8-bit PNG through ImageOptim","title":"Adding Images"},{"location":"development-tips/#templates","text":"Apps use a Jinja template that extends the base.html template used by the rest of the site. This template would reside in cfgov/jinja2/v1/[project namespace]/index.html or similar (for example, owning-a-home ). Note A template may support a non-standard browser, like an older IE version, by including the required dependencies, polyfills, etc. in its template's {% block css %} or {% block javascript scoped %} blocks.","title":"Templates"},{"location":"editing-components/","text":"Creating and Editing Wagtail Components \u00b6 consumerfinance.gov implements a number of components that editors can choose from when building a page, for example: Heroes, Expandable Groups, or Info Unit Groups. The CFPB Design System describes the design and intended usage of many of these components. In Wagtail parlance, these are called \"StreamField blocks\" * (or just \"blocks\"). We sometimes also refer to them as \"modules\", because we think that the terms \"component\" and \"module\" may be more obvious to non-developers. This page will use the terms somewhat interchangeably. One other important thing to note before we begin: blocks can be nested within other blocks. * If you're going to be doing anything more than making minor updates to existing components, this is highly recommended reading. Table of contents \u00b6 The parts of a Wagtail block The back end The Python class Adding it to a StreamField The front end The HTML template Adding CSS Adding JavaScript How-to guides Creating a new component Adding a field to an existing component Editing a field on an existing component Removing a field from an existing component Creating migrations for StreamField blocks The parts of a Wagtail block \u00b6 Blocks are implemented via several different bits of code: Defining a block's fields and other properties in a Python class Adding the class to a page's StreamField block options Creating an HTML template for rendering the block on a page (Optionally) adding some CSS for styling the block (Optionally) adding some JavaScript for adding advanced behavior Before you dive in further, check out the Notes on Atomic Design page and familiarize yourself with our basic concepts of atoms, molecules, and organisms. The back end \u00b6 The Python class \u00b6 A component's fields and other properties are defined in a Python class, typically a subclass of Wagtail's StructBlock . These classes are located in a number of different files across the repository, but there are two major categories they fall into: Files corresponding to a general-purpose, site-wide atomic component. These files\u2014 atoms.py , molecules.py , and organisms.py \u2014are located in cfgov/v1/atomic_elements . Files that are specific to a particular sub-app, such as regulations3k's blocks.py . There are other places where StreamField block classes are defined (particularly blocks that are only ever used as fields within another block), but these are the two most common locations where top-level Wagtail modules are stored. A simple component class looks like this: class RelatedContent(blocks.StructBlock): # 1 heading = blocks.CharBlock(required=False) # 2 paragraph = blocks.RichTextBlock(required=False) # 3 links = blocks.ListBlock(atoms.Hyperlink()) # 4 class Meta: # 5 icon = 'grip' # 6 label = 'Related content' # 7 template = '_includes/molecules/related-content.html' # 8 There are a few things happening here: The RelatedContent class is a subclass of Wagtail's StructBlock , which allows for the combination of a fixed number of other sub-blocks (see previous comment about blocks being nested within other blocks) into a single unit (what we'd think of as a \"module\" in the Wagtail editor). This one has three sub-blocks (lines 2, 3, and 4). The heading field uses the basic Wagtail CharBlock , which results in a field with a basic single-line text input. The paragraph field uses the basic Wagtail RichTextBlock , which results in a field with a multiline WYSIWYG text input. The links field uses another basic Wagtail block, ListBlock , which is a special type of block that can hold a variable number of some other block (the Hyperlink atom block, in this case). The Meta class defines some properties on the RelatedContent block that are used by the Wagtail admin or in rendering the block. The icon property tells Wagtail what icon to use in the editor for the button you use to add a RelatedContent block to a StreamField. Icon options can be found in the Wagtail style guide when running locally: http://localhost:8000/admin/styleguide/#icons The optional label property overrides the text of that same button; if label is not set, Wagtail will generate one from the name of the block. The template property is a pointer to the HTML template used to render this component. See below for more on templates. This results in a module that looks like this in the Wagtail editor: Note again that what we think of as fields are also blocks, and what we think of as components or modules are a special kind of block, StructBlock , that comprise the sub-blocks that are our fields. There are two common optional things that are also used in component classes: Overriding the default get_context method to pass additional data to the template Adding component-specific JavaScript via the Media class Adding it to a StreamField \u00b6 Components are made available in the page editing interface by adding them to one of a page types's StreamFields. These are usually the first things in a page's class definition. For example, see this snippet from blog_page.py : class BlogPage(AbstractFilterPage): content = StreamField([ ('full_width_text', organisms.FullWidthText()), ('info_unit_group', organisms.InfoUnitGroup()), ('expandable', organisms.Expandable()), ('well', organisms.Well()), ('email_signup', organisms.EmailSignUp()), ('feedback', v1_blocks.Feedback()), ]) \u2026 This sets up a StreamField named content that allows for the insertion of any of those seven listed blocks into it. To make the RelatedContent module (shown above) available to this StreamField, we'd add a new entry to this list following the same format: ('related_content', molecules.RelatedContent()), . Most page types have two StreamFields ( header and content ) in the general content area (the first tab on an editing screen), and most also share a common sidefoot StreamField (so named for the fact that it appears on the right side on some page types, but in the footer on others) on the sidebar tab. Don't forget the migrations! Adding or changing fields on either Python class will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration . See the guide on creating migrations for StreamField blocks for more details. The front end \u00b6 Before diving into the front-end code, a reminder to visit the Notes on Atomic Design page to learn about how we conceive of components in a hierarchy of atoms, molecules, and organisms. The HTML template \u00b6 Frontend rendering of Wagtail StreamField blocks to HTML can be controlled by writing a Django template and associating it with the block type. A custom block definition can specify the template path in its Meta class: from wagtail.core import blocks class PersonBlock(blocks.StructBlock): name = blocks.CharBlock() email = blocks.EmailBlock() class Meta: template = 'myapp/blocks/person_block.html' StreamField block templates are loaded by the Django template loader in the same way that Django page templates are. The specified template path must be loadable by one of the Django template engines configured in settings.TEMPLATES . (This project supports both the standard Django templates backend and the Jinja2 backend , but Jinja2 is more commonly used.) See the Django templates documentation for more details on the search algorithm used to locate a template. Returning to the RelatedContent example, this is what its Jinja2 template looks like (comments excluded): <div class=\"m-related-content\"> {% if value.heading %} <header class=\"m-slug-header\"> <h2 class=\"a-heading\"> {{ value.heading }} </h2> </header> {% endif %} {{ value.paragraph | safe }} {% if value.links %} <ul class=\"m-list m-list__links\"> {% for link in value.links %} <li class=\"m-list_item\"> <a href=\"{{ link.url }}\" class=\"m-list_link\">{{ link.text }}</a> </li> {% endfor %} </ul> {% endif %} </div> When Wagtail renders a block, it includes the values of its fields in an object named value . Above, you can see where the heading and paragraph fields are output with Jinja2 expression tags . And note how the links field (a ListBlock ) is iterated over, and the values of its Hyperlink child blocks are output. That's about as simple an example as it gets, but block templates can get much more complex when they have lots of child blocks and grandchild blocks. Also, if a block definition has overridden get_context to pass other data into the template (as described at the end of the Python class section above), those context variables can also be output with simple Jinja2 expression tags: {{ context_var }} . Adding CSS \u00b6 If a component needs any custom styling not already provided by the Design System or consumerfinance.gov, you can add it by creating a new Less file for the component. Note Please be sure that you actually need new Less before creating it. We have a wide array of styles already available in the Design System components and here in consumerfinance.gov , some of which could perhaps be combined to achieve your desired result. Also be sure that new component designs have gone through our internal approval process before adding them to the project. If you're working on a general-purpose atomic component for site-wide use, this file should live in cfgov/unprocessed/css/<atoms|molecules|organisms>/ . (Choose the deepest folder according to the atomic rank of the component.) Continuing the RelatedContent example, if it needed its own styles, it would live at cfgov/unprocessed/css/molecules/related-content.less . Newly-created Less files need to be imported into the project's main main.less file, located at cfgov/unprocessed/css/main.less . Please place them in the appropriate section for their atomic rank. Because consumerfinance.gov uses main.less to build a single CSS file for almost the entire project, it is not necessary to tell the Python model anything about a component-specific stylesheet (for general-purpose, site-wide components). That is not the case with JavaScript, as we will see in the next section. Note If you're working on a component that belongs to a particular sub-app, its Less file should live in cfgov/unprocessed/<app-name>/css/ . Adding JavaScript \u00b6 Each atomic component may optionally be given a Media class that can list one or more JavaScript files that should be loaded when using it. When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is how one would add the Media class to our RelatedContent example: class RelatedContent(blocks.StructBlock): \u2026 # see first example on this page class Media: js = ['related-content.js'] (The related-content.js file would need to be placed in cfgov/unprocessed/js/molecules/ ; see Notes on Atomic Design .) This will load the related-content.js script on any page that includes the RelatedContent molecule in one of its StreamFields. How-to guides \u00b6 Creating a new component \u00b6 Review the Notes on Atomic Design page. Add each of the parts mentioned above: Create the Python class Add the class to a StreamField Create an HTML template for the component (Optionally) add some CSS (Optionally) add some JavaScript Note Before creating a new component, please consider whether one of our existing components can meet your needs. Talk to the consumerfinance.gov product owners if your content has specific display requirements that aren't served by an existing component, or if a specific maintenance efficiency will be gained from a new component. Adding a field to an existing component \u00b6 Locate the Python class of the component you want to add a field to. Add the field by inserting a snippet like this in the list of fields, in the order in which you want it to appear in the editor: field_name = blocks.BlockName() . Replace field_name with a succinct name for what data the field contains Replace BlockName with one of the basic Wagtail block types . Sometimes we create our own custom blocks that can be used, as well. See, for example, the HeadingBlock , used in InfoUnitGroup , among other places. Add any desired parameters: required=False if you do not want the field to be required (it usually is, by default) label='Some label' if you would like the editor to show a label more meaningful than the sentence-case transformation of the field name help_text='Some text' if the field needs a more verbose explanation to be shown in the editor to make it clear to users how it should work default=<some appropriate value> if you want the field to have a specific default value, e.g., True to have a BooleanBlock checkbox default to checked. Certain blocks may take other arguments, as described in the basic Wagtail blocks documentation . Edit the component template to do something with the field's data \u2013 output it, use it to trigger a CSS class, etc. Create a schema migration. Editing a field on an existing component \u00b6 Determine if the change you want to make will need a data migration. If the answer is no : make your changes, create a schema migration , and be on your merry way. If the answer is yes : continue on. Add the new version of the field. Create a schema migration for adding the new field. Create a data migration to copy data from the old field into the new field. Edit the component template to use the new field's data instead of the old field's data. Remove the old field. Create a schema migration for removing the old field. Removing a field from an existing component \u00b6 These instructions presume that you do not care about any data stored in the field you are deleting. If that is not the case, please go up to the instructions for editing a field and come back here when instructed. Locate the field you want to remove in the block's Python class. Delete the field definition. Create a schema migration. Creating migrations for StreamField blocks \u00b6 To automatically generate a schema migration , run ./cfgov/manage.py makemigrations -n <description_of_changes> from the root of the repository. You may also need a data migration \u00b6 Some field edits (like changing the default , label , help_text , and required properties, or changing the order of fields on a block) do not require a data migration. A schema migration is sufficient. Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it. Not sure if there's actually any data that could potentially be lost? \u00b6 You may not know off the top of your head if a component you are modifying actually has any data stored that could be lost. One way that you can manually check to see if this is the case is to use the Block Inventory feature that we added to Wagtail . This feature lets you search all Wagtail pages on the site for the presence of a component. Here's how: With a current database dump, visit http://localhost:8000/admin/inventory/ . (This is also available from the admin menu by going to Settings > Block Inventory .) Using the three pairs of dropdown menus at the top of the page, choose to look for pages that either include or exclude particular components. If multiple components are selected, resulting pages must match all of the conditions. Click Find matching pages to execute the search. If no results are found, lucky you! You're in the clear to make whatever changes you desire to that component without worrying about data loss. If, more likely, there are results, you should open each result, look through the page for the instance(s) of the component in question, and see if the changes you want to make would cause the loss of important data. If there is only a small amount of potential data loss, it may be more practical to forego the data migration and manually replace that data once the code changes have been deployed. If you think this is the preferable route, consult with the appropriate stakeholders to confirm that they are OK with the window of time in which users may experience a gap in the data that is being manually replaced. For more details on both kinds of migrations, see the Wagtail Migrations page .","title":"Creating and Editing Components"},{"location":"editing-components/#creating-and-editing-wagtail-components","text":"consumerfinance.gov implements a number of components that editors can choose from when building a page, for example: Heroes, Expandable Groups, or Info Unit Groups. The CFPB Design System describes the design and intended usage of many of these components. In Wagtail parlance, these are called \"StreamField blocks\" * (or just \"blocks\"). We sometimes also refer to them as \"modules\", because we think that the terms \"component\" and \"module\" may be more obvious to non-developers. This page will use the terms somewhat interchangeably. One other important thing to note before we begin: blocks can be nested within other blocks. * If you're going to be doing anything more than making minor updates to existing components, this is highly recommended reading.","title":"Creating and Editing Wagtail Components"},{"location":"editing-components/#table-of-contents","text":"The parts of a Wagtail block The back end The Python class Adding it to a StreamField The front end The HTML template Adding CSS Adding JavaScript How-to guides Creating a new component Adding a field to an existing component Editing a field on an existing component Removing a field from an existing component Creating migrations for StreamField blocks","title":"Table of contents"},{"location":"editing-components/#the-parts-of-a-wagtail-block","text":"Blocks are implemented via several different bits of code: Defining a block's fields and other properties in a Python class Adding the class to a page's StreamField block options Creating an HTML template for rendering the block on a page (Optionally) adding some CSS for styling the block (Optionally) adding some JavaScript for adding advanced behavior Before you dive in further, check out the Notes on Atomic Design page and familiarize yourself with our basic concepts of atoms, molecules, and organisms.","title":"The parts of a Wagtail block"},{"location":"editing-components/#the-back-end","text":"","title":"The back end"},{"location":"editing-components/#the-python-class","text":"A component's fields and other properties are defined in a Python class, typically a subclass of Wagtail's StructBlock . These classes are located in a number of different files across the repository, but there are two major categories they fall into: Files corresponding to a general-purpose, site-wide atomic component. These files\u2014 atoms.py , molecules.py , and organisms.py \u2014are located in cfgov/v1/atomic_elements . Files that are specific to a particular sub-app, such as regulations3k's blocks.py . There are other places where StreamField block classes are defined (particularly blocks that are only ever used as fields within another block), but these are the two most common locations where top-level Wagtail modules are stored. A simple component class looks like this: class RelatedContent(blocks.StructBlock): # 1 heading = blocks.CharBlock(required=False) # 2 paragraph = blocks.RichTextBlock(required=False) # 3 links = blocks.ListBlock(atoms.Hyperlink()) # 4 class Meta: # 5 icon = 'grip' # 6 label = 'Related content' # 7 template = '_includes/molecules/related-content.html' # 8 There are a few things happening here: The RelatedContent class is a subclass of Wagtail's StructBlock , which allows for the combination of a fixed number of other sub-blocks (see previous comment about blocks being nested within other blocks) into a single unit (what we'd think of as a \"module\" in the Wagtail editor). This one has three sub-blocks (lines 2, 3, and 4). The heading field uses the basic Wagtail CharBlock , which results in a field with a basic single-line text input. The paragraph field uses the basic Wagtail RichTextBlock , which results in a field with a multiline WYSIWYG text input. The links field uses another basic Wagtail block, ListBlock , which is a special type of block that can hold a variable number of some other block (the Hyperlink atom block, in this case). The Meta class defines some properties on the RelatedContent block that are used by the Wagtail admin or in rendering the block. The icon property tells Wagtail what icon to use in the editor for the button you use to add a RelatedContent block to a StreamField. Icon options can be found in the Wagtail style guide when running locally: http://localhost:8000/admin/styleguide/#icons The optional label property overrides the text of that same button; if label is not set, Wagtail will generate one from the name of the block. The template property is a pointer to the HTML template used to render this component. See below for more on templates. This results in a module that looks like this in the Wagtail editor: Note again that what we think of as fields are also blocks, and what we think of as components or modules are a special kind of block, StructBlock , that comprise the sub-blocks that are our fields. There are two common optional things that are also used in component classes: Overriding the default get_context method to pass additional data to the template Adding component-specific JavaScript via the Media class","title":"The Python class"},{"location":"editing-components/#adding-it-to-a-streamfield","text":"Components are made available in the page editing interface by adding them to one of a page types's StreamFields. These are usually the first things in a page's class definition. For example, see this snippet from blog_page.py : class BlogPage(AbstractFilterPage): content = StreamField([ ('full_width_text', organisms.FullWidthText()), ('info_unit_group', organisms.InfoUnitGroup()), ('expandable', organisms.Expandable()), ('well', organisms.Well()), ('email_signup', organisms.EmailSignUp()), ('feedback', v1_blocks.Feedback()), ]) \u2026 This sets up a StreamField named content that allows for the insertion of any of those seven listed blocks into it. To make the RelatedContent module (shown above) available to this StreamField, we'd add a new entry to this list following the same format: ('related_content', molecules.RelatedContent()), . Most page types have two StreamFields ( header and content ) in the general content area (the first tab on an editing screen), and most also share a common sidefoot StreamField (so named for the fact that it appears on the right side on some page types, but in the footer on others) on the sidebar tab. Don't forget the migrations! Adding or changing fields on either Python class will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration . See the guide on creating migrations for StreamField blocks for more details.","title":"Adding it to a StreamField"},{"location":"editing-components/#the-front-end","text":"Before diving into the front-end code, a reminder to visit the Notes on Atomic Design page to learn about how we conceive of components in a hierarchy of atoms, molecules, and organisms.","title":"The front end"},{"location":"editing-components/#the-html-template","text":"Frontend rendering of Wagtail StreamField blocks to HTML can be controlled by writing a Django template and associating it with the block type. A custom block definition can specify the template path in its Meta class: from wagtail.core import blocks class PersonBlock(blocks.StructBlock): name = blocks.CharBlock() email = blocks.EmailBlock() class Meta: template = 'myapp/blocks/person_block.html' StreamField block templates are loaded by the Django template loader in the same way that Django page templates are. The specified template path must be loadable by one of the Django template engines configured in settings.TEMPLATES . (This project supports both the standard Django templates backend and the Jinja2 backend , but Jinja2 is more commonly used.) See the Django templates documentation for more details on the search algorithm used to locate a template. Returning to the RelatedContent example, this is what its Jinja2 template looks like (comments excluded): <div class=\"m-related-content\"> {% if value.heading %} <header class=\"m-slug-header\"> <h2 class=\"a-heading\"> {{ value.heading }} </h2> </header> {% endif %} {{ value.paragraph | safe }} {% if value.links %} <ul class=\"m-list m-list__links\"> {% for link in value.links %} <li class=\"m-list_item\"> <a href=\"{{ link.url }}\" class=\"m-list_link\">{{ link.text }}</a> </li> {% endfor %} </ul> {% endif %} </div> When Wagtail renders a block, it includes the values of its fields in an object named value . Above, you can see where the heading and paragraph fields are output with Jinja2 expression tags . And note how the links field (a ListBlock ) is iterated over, and the values of its Hyperlink child blocks are output. That's about as simple an example as it gets, but block templates can get much more complex when they have lots of child blocks and grandchild blocks. Also, if a block definition has overridden get_context to pass other data into the template (as described at the end of the Python class section above), those context variables can also be output with simple Jinja2 expression tags: {{ context_var }} .","title":"The HTML template"},{"location":"editing-components/#adding-css","text":"If a component needs any custom styling not already provided by the Design System or consumerfinance.gov, you can add it by creating a new Less file for the component. Note Please be sure that you actually need new Less before creating it. We have a wide array of styles already available in the Design System components and here in consumerfinance.gov , some of which could perhaps be combined to achieve your desired result. Also be sure that new component designs have gone through our internal approval process before adding them to the project. If you're working on a general-purpose atomic component for site-wide use, this file should live in cfgov/unprocessed/css/<atoms|molecules|organisms>/ . (Choose the deepest folder according to the atomic rank of the component.) Continuing the RelatedContent example, if it needed its own styles, it would live at cfgov/unprocessed/css/molecules/related-content.less . Newly-created Less files need to be imported into the project's main main.less file, located at cfgov/unprocessed/css/main.less . Please place them in the appropriate section for their atomic rank. Because consumerfinance.gov uses main.less to build a single CSS file for almost the entire project, it is not necessary to tell the Python model anything about a component-specific stylesheet (for general-purpose, site-wide components). That is not the case with JavaScript, as we will see in the next section. Note If you're working on a component that belongs to a particular sub-app, its Less file should live in cfgov/unprocessed/<app-name>/css/ .","title":"Adding CSS"},{"location":"editing-components/#adding-javascript","text":"Each atomic component may optionally be given a Media class that can list one or more JavaScript files that should be loaded when using it. When a page is requested via the browser, code contained in base.html will loop all atomic components for the requested page and load the appropriate atomic JavaScript bundles. Here is how one would add the Media class to our RelatedContent example: class RelatedContent(blocks.StructBlock): \u2026 # see first example on this page class Media: js = ['related-content.js'] (The related-content.js file would need to be placed in cfgov/unprocessed/js/molecules/ ; see Notes on Atomic Design .) This will load the related-content.js script on any page that includes the RelatedContent molecule in one of its StreamFields.","title":"Adding JavaScript"},{"location":"editing-components/#how-to-guides","text":"","title":"How-to guides"},{"location":"editing-components/#creating-a-new-component","text":"Review the Notes on Atomic Design page. Add each of the parts mentioned above: Create the Python class Add the class to a StreamField Create an HTML template for the component (Optionally) add some CSS (Optionally) add some JavaScript Note Before creating a new component, please consider whether one of our existing components can meet your needs. Talk to the consumerfinance.gov product owners if your content has specific display requirements that aren't served by an existing component, or if a specific maintenance efficiency will be gained from a new component.","title":"Creating a new component"},{"location":"editing-components/#adding-a-field-to-an-existing-component","text":"Locate the Python class of the component you want to add a field to. Add the field by inserting a snippet like this in the list of fields, in the order in which you want it to appear in the editor: field_name = blocks.BlockName() . Replace field_name with a succinct name for what data the field contains Replace BlockName with one of the basic Wagtail block types . Sometimes we create our own custom blocks that can be used, as well. See, for example, the HeadingBlock , used in InfoUnitGroup , among other places. Add any desired parameters: required=False if you do not want the field to be required (it usually is, by default) label='Some label' if you would like the editor to show a label more meaningful than the sentence-case transformation of the field name help_text='Some text' if the field needs a more verbose explanation to be shown in the editor to make it clear to users how it should work default=<some appropriate value> if you want the field to have a specific default value, e.g., True to have a BooleanBlock checkbox default to checked. Certain blocks may take other arguments, as described in the basic Wagtail blocks documentation . Edit the component template to do something with the field's data \u2013 output it, use it to trigger a CSS class, etc. Create a schema migration.","title":"Adding a field to an existing component"},{"location":"editing-components/#editing-a-field-on-an-existing-component","text":"Determine if the change you want to make will need a data migration. If the answer is no : make your changes, create a schema migration , and be on your merry way. If the answer is yes : continue on. Add the new version of the field. Create a schema migration for adding the new field. Create a data migration to copy data from the old field into the new field. Edit the component template to use the new field's data instead of the old field's data. Remove the old field. Create a schema migration for removing the old field.","title":"Editing a field on an existing component"},{"location":"editing-components/#removing-a-field-from-an-existing-component","text":"These instructions presume that you do not care about any data stored in the field you are deleting. If that is not the case, please go up to the instructions for editing a field and come back here when instructed. Locate the field you want to remove in the block's Python class. Delete the field definition. Create a schema migration.","title":"Removing a field from an existing component"},{"location":"editing-components/#creating-migrations-for-streamfield-blocks","text":"To automatically generate a schema migration , run ./cfgov/manage.py makemigrations -n <description_of_changes> from the root of the repository.","title":"Creating migrations for StreamField blocks"},{"location":"editing-components/#you-may-also-need-a-data-migration","text":"Some field edits (like changing the default , label , help_text , and required properties, or changing the order of fields on a block) do not require a data migration. A schema migration is sufficient. Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it.","title":"You may also need a data migration"},{"location":"editing-components/#not-sure-if-theres-actually-any-data-that-could-potentially-be-lost","text":"You may not know off the top of your head if a component you are modifying actually has any data stored that could be lost. One way that you can manually check to see if this is the case is to use the Block Inventory feature that we added to Wagtail . This feature lets you search all Wagtail pages on the site for the presence of a component. Here's how: With a current database dump, visit http://localhost:8000/admin/inventory/ . (This is also available from the admin menu by going to Settings > Block Inventory .) Using the three pairs of dropdown menus at the top of the page, choose to look for pages that either include or exclude particular components. If multiple components are selected, resulting pages must match all of the conditions. Click Find matching pages to execute the search. If no results are found, lucky you! You're in the clear to make whatever changes you desire to that component without worrying about data loss. If, more likely, there are results, you should open each result, look through the page for the instance(s) of the component in question, and see if the changes you want to make would cause the loss of important data. If there is only a small amount of potential data loss, it may be more practical to forego the data migration and manually replace that data once the code changes have been deployed. If you think this is the preferable route, consult with the appropriate stakeholders to confirm that they are OK with the window of time in which users may experience a gap in the data that is being manually replaced. For more details on both kinds of migrations, see the Wagtail Migrations page .","title":"Not sure if there's actually any data that could potentially be lost?"},{"location":"feature-flags/","text":"Feature flags \u00b6 Feature flags are implemented using our Django-Flags and Wagtail-Flags apps. The Django-Flags documentation contains an overview of feature flags and how to use them and the Wagtail-Flags README describes how to add feature flag conditions in the Wagtail admin. This document covers how to add and use feature flags with consumerfinance.gov and the conventions we have around their use. Adding a flag Checking a flag In templates Jinja2 Django In code In URLs Enabling a flag Hard-coded conditions Database conditions Satellite apps Hygiene Adding a flag \u00b6 Feature flags are defined in code in the cfgov/settings/base.py file as part of the FLAGS setting. Each flag consists of a single string and a Python list of its hard-coded conditions (see Enabling a flag below). FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # \"This beta site is a work in progress.\" 'BETA_NOTICE': [], } By convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled. Checking a flag \u00b6 Flags can be checked either in Python code or in Django or Jinja2 template files. See the full Wagtail Flags API is documented for more information. In templates \u00b6 Jinja2 \u00b6 Most of consumerfinance.gov's templates are Jinja2. In these templates, two template functions are provided, flag_enabled and flag_disabled . Each takes a flag name as its first argument and request` object as the second. flag_enabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. An example is the BETA_NOTICE flag as implemented in header.html : {% if flag_enabled('BETA_NOTICE') %} <div class=\"m-global-banner\"> <div class=\"wrapper wrapper__match-content o-expandable o-expandable__expanded\"> <div class=\"m-global-banner_head\"> <span class=\"cf-icon cf-icon-error-round m-global-banner_icon\"></span> This beta site is a work in progress. </div> \u2026 </div> </div> {% endif %} Django \u00b6 In Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided flag_enabled and flag_disabled once the feature_flags template tag library is loaded. flag_enabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. The BETA_NOTICE Jinja2 example above when implemented with Django templates would look like this: {% load feature_flags %} {% flag_enabled 'BETA_NOTICE' as beta_flag %} {% if beta_flag %} <div class=\"m-global-banner\"> <div class=\"wrapper wrapper__match-content o-expandable o-expandable__expanded\"> <div class=\"m-global-banner_head\"> <span class=\"cf-icon cf-icon-error-round m-global-banner_icon\"></span> This beta site is a work in progress. </div> \u2026 </div> </div> {% endif %} In code \u00b6 In Python code three functions are available for checking feature flags, flag_state , flag_enabled , and flag_disabled . The Python API is slightly different from the Jinja2 or Django template API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle). See the Django-Flags flag state API documentation for more . Additionally two decorators, flag_check and flag_required , are provided for wrapping views (and another functions) in a feature flag check. See the Django-Flags flag decorators API documentation for more . In URLs \u00b6 There are two ways to flag Django URL patterns in urls.py : with flagged_url() in place of url() for a single pattern, or with the flagged_urls() context manager for multiple URLs. flagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None) works exactly like url() except it takes a flag name as its first argument. If the flag's state matches the given state , the URL pattern will be served from the given view ; if not, and fallback is given, the fallback will be used. An example is our WAGTAIL_ABOUT_US flag : flagged_url('WAGTAIL_ABOUT_US', r'^about-us/$', lambda req: ServeView.as_view()(req, req.path), fallback=SheerTemplateView.as_view( template_name='about-us/index.html'), name='about-us'), Ignoring the view being a lambda for now (see Flagging Wagtail URLs below ), this URL will be served via Wagtail if WAGTAIL_ABOUT_US 's conditions are True , and from a TemplateView if its conditions are False . If you need to flag multiple URLs with the same flag, you can use the flagged_urls() context manager. with flagged_urls(flag_name, state=True, fallback=None) as url provides a context in which the returned url() function can be used in place of the Django url() function in patterns and those patterns will share the same feature flag, state, and fallback. An example is our WAGTAIL_ASK_CFPB flag : with flagged_urls('WAGTAIL_ASK_CFPB') as url: ask_patterns = [ url(r'^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$', view_answer, name='ask-english-answer'), url(r'^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$', view_answer, name='ask-spanish-answer'), \u2026 ] urlpatterns += ask_patterns Warning Do not attempt to use flag_check or any flag state-checking functions in urls.py . Because they will be evaluated on import of urls.py they will attempt to access the Django FlagState model before it is ready and will error. Flagging Wagtail URLs \u00b6 Wagtail views in flagged_url with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with request as the first argument, and Wagtail's serve view takes both the request and the path. To get around this, in flagged_url we typically use a lambda for the view: lambda req: ServeView.as_view()(req, req.path) This lambda takes the request and calls the Wagtail-Sharing ServeView (which we're using in place of wagtail.core.views.serve ). Enabling a flag \u00b6 Feature flags are enabled based on a set of conditions that are given either in the Django settings files (in cfgov/cfgov/settings/ ) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled. A list of available conditions and how to use them is available in the Django-Flags documentation . Hard-coded conditions \u00b6 Conditions that are defined in the Django settings are hard-coded, and require a change to files in consumerfinance.gov, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change. When adding a flag to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type. See the Django-Flags conditions documentation for more on individual conditions. There is a simple boolean condition that is either True or False , and if it is True the flag is enabled and if it is False the flag is disabled. If we want to always turn the BETA_NOTICE flag on in settings with a boolean condition, that would look like this: FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # \"This beta site is a work in progress.\" 'BETA_NOTICE': [ { 'condition': 'boolean', 'value': True, }, ], } Database conditions \u00b6 Conditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place). To view, delete, and add database conditions, navigate to \"Settings > Flags\" in the Wagtail admin. Once in the flag settings, you'll have a list of all flags and their statuses. Select a flag to see its conditions. Existing database conditions can be edited or deleted here. To create a new database condition, select \"Add a condition\". As with hard-coded conditions , to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled. Database conditions can only be set for flags that exist in the Django settings. Satellite apps \u00b6 Feature flags can be used in satellite apps in exactly the same way they are used in consumerfinance.gov. An example is the use of a feature flagged template choice in the complaintdatabase app . Hygiene \u00b6 Most feature flags are no longer needed once a feature is launched. Follow these steps to remove a flag when its job is done: Create a pull request that deletes the flag from settings, if it was declared there, and removes any related code and tests that referenced the flag. After the changes are merged and deployed to production, check Settings ==> Flags in Wagtail ( /admin/flags/ ) to see if the removed flag is still listed. If so, the flag has been saved in our database. Select the flag, click the DELETE FLAG button at top right, and then choose YES, DELETE IT.","title":"Feature Flags"},{"location":"feature-flags/#feature-flags","text":"Feature flags are implemented using our Django-Flags and Wagtail-Flags apps. The Django-Flags documentation contains an overview of feature flags and how to use them and the Wagtail-Flags README describes how to add feature flag conditions in the Wagtail admin. This document covers how to add and use feature flags with consumerfinance.gov and the conventions we have around their use. Adding a flag Checking a flag In templates Jinja2 Django In code In URLs Enabling a flag Hard-coded conditions Database conditions Satellite apps Hygiene","title":"Feature flags"},{"location":"feature-flags/#adding-a-flag","text":"Feature flags are defined in code in the cfgov/settings/base.py file as part of the FLAGS setting. Each flag consists of a single string and a Python list of its hard-coded conditions (see Enabling a flag below). FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # \"This beta site is a work in progress.\" 'BETA_NOTICE': [], } By convention our flag names are all uppercase, with underscores instead of whitespace. A comment is expected above each flag with a short description fo what happens when it is enabled.","title":"Adding a flag"},{"location":"feature-flags/#checking-a-flag","text":"Flags can be checked either in Python code or in Django or Jinja2 template files. See the full Wagtail Flags API is documented for more information.","title":"Checking a flag"},{"location":"feature-flags/#in-templates","text":"","title":"In templates"},{"location":"feature-flags/#jinja2","text":"Most of consumerfinance.gov's templates are Jinja2. In these templates, two template functions are provided, flag_enabled and flag_disabled . Each takes a flag name as its first argument and request` object as the second. flag_enabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled('MY_FLAG') will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. An example is the BETA_NOTICE flag as implemented in header.html : {% if flag_enabled('BETA_NOTICE') %} <div class=\"m-global-banner\"> <div class=\"wrapper wrapper__match-content o-expandable o-expandable__expanded\"> <div class=\"m-global-banner_head\"> <span class=\"cf-icon cf-icon-error-round m-global-banner_icon\"></span> This beta site is a work in progress. </div> \u2026 </div> </div> {% endif %}","title":"Jinja2"},{"location":"feature-flags/#django","text":"In Django templates (used in Satellite apps and the Wagtail admin), two template functions are provided flag_enabled and flag_disabled once the feature_flags template tag library is loaded. flag_enabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are met. flag_disabled 'MY_FLAG' will return True if the conditions under which MY_FLAG is enabled are not met. See Enabling a flag below for more on flag conditions. The BETA_NOTICE Jinja2 example above when implemented with Django templates would look like this: {% load feature_flags %} {% flag_enabled 'BETA_NOTICE' as beta_flag %} {% if beta_flag %} <div class=\"m-global-banner\"> <div class=\"wrapper wrapper__match-content o-expandable o-expandable__expanded\"> <div class=\"m-global-banner_head\"> <span class=\"cf-icon cf-icon-error-round m-global-banner_icon\"></span> This beta site is a work in progress. </div> \u2026 </div> </div> {% endif %}","title":"Django"},{"location":"feature-flags/#in-code","text":"In Python code three functions are available for checking feature flags, flag_state , flag_enabled , and flag_disabled . The Python API is slightly different from the Jinja2 or Django template API, in that flag conditions can take more potential arguments than requests, and thus flags are more flexible when checked in Python (in and outside a request cycle). See the Django-Flags flag state API documentation for more . Additionally two decorators, flag_check and flag_required , are provided for wrapping views (and another functions) in a feature flag check. See the Django-Flags flag decorators API documentation for more .","title":"In code"},{"location":"feature-flags/#in-urls","text":"There are two ways to flag Django URL patterns in urls.py : with flagged_url() in place of url() for a single pattern, or with the flagged_urls() context manager for multiple URLs. flagged_url(flag_name, regex, view, kwargs=None, name=None, state=True, fallback=None) works exactly like url() except it takes a flag name as its first argument. If the flag's state matches the given state , the URL pattern will be served from the given view ; if not, and fallback is given, the fallback will be used. An example is our WAGTAIL_ABOUT_US flag : flagged_url('WAGTAIL_ABOUT_US', r'^about-us/$', lambda req: ServeView.as_view()(req, req.path), fallback=SheerTemplateView.as_view( template_name='about-us/index.html'), name='about-us'), Ignoring the view being a lambda for now (see Flagging Wagtail URLs below ), this URL will be served via Wagtail if WAGTAIL_ABOUT_US 's conditions are True , and from a TemplateView if its conditions are False . If you need to flag multiple URLs with the same flag, you can use the flagged_urls() context manager. with flagged_urls(flag_name, state=True, fallback=None) as url provides a context in which the returned url() function can be used in place of the Django url() function in patterns and those patterns will share the same feature flag, state, and fallback. An example is our WAGTAIL_ASK_CFPB flag : with flagged_urls('WAGTAIL_ASK_CFPB') as url: ask_patterns = [ url(r'^(?i)ask-cfpb/([-\\w]{1,244})-(en)-(\\d{1,6})/?$', view_answer, name='ask-english-answer'), url(r'^(?i)obtener-respuestas/([-\\w]{1,244})-(es)-(\\d{1,6})/?$', view_answer, name='ask-spanish-answer'), \u2026 ] urlpatterns += ask_patterns Warning Do not attempt to use flag_check or any flag state-checking functions in urls.py . Because they will be evaluated on import of urls.py they will attempt to access the Django FlagState model before it is ready and will error.","title":"In URLs"},{"location":"feature-flags/#flagging-wagtail-urls","text":"Wagtail views in flagged_url with a Django view as fallback (or vice-versa) can be a bit awkward. Django views are typically called with request as the first argument, and Wagtail's serve view takes both the request and the path. To get around this, in flagged_url we typically use a lambda for the view: lambda req: ServeView.as_view()(req, req.path) This lambda takes the request and calls the Wagtail-Sharing ServeView (which we're using in place of wagtail.core.views.serve ).","title":"Flagging Wagtail URLs"},{"location":"feature-flags/#enabling-a-flag","text":"Feature flags are enabled based on a set of conditions that are given either in the Django settings files (in cfgov/cfgov/settings/ ) or in the Django or Wagtail admin. Multiple conditions can be given, both in settings and in the admin, and if any condition is satisfied a flag is enabled. A list of available conditions and how to use them is available in the Django-Flags documentation .","title":"Enabling a flag"},{"location":"feature-flags/#hard-coded-conditions","text":"Conditions that are defined in the Django settings are hard-coded, and require a change to files in consumerfinance.gov, a new tagged release, and new deployment to change. These conditions should be used for flags that are relatively long-lasting and that can require a round-trip through the release and deployment process to change. When adding a flag to the Django settings the flag's dictionary of conditions can contain a condition name and value that must be satisfied for the flag to be enabled. The nature of that value changes depending on the condition type. See the Django-Flags conditions documentation for more on individual conditions. There is a simple boolean condition that is either True or False , and if it is True the flag is enabled and if it is False the flag is disabled. If we want to always turn the BETA_NOTICE flag on in settings with a boolean condition, that would look like this: FLAGS = { # Beta banner, seen on beta.consumerfinance.gov # When enabled, a banner appears across the top of the site proclaiming # \"This beta site is a work in progress.\" 'BETA_NOTICE': [ { 'condition': 'boolean', 'value': True, }, ], }","title":"Hard-coded conditions"},{"location":"feature-flags/#database-conditions","text":"Conditions that are managed via the Wagtail or Django admin are stored in the database. These conditions can be changed in real-time and do not require any code changes or release and deployment to change (presuming the code that uses the feature flag is in place). To view, delete, and add database conditions, navigate to \"Settings > Flags\" in the Wagtail admin. Once in the flag settings, you'll have a list of all flags and their statuses. Select a flag to see its conditions. Existing database conditions can be edited or deleted here. To create a new database condition, select \"Add a condition\". As with hard-coded conditions , to create a database condition you must select which condition type you would like to use and give it a value that must be satisfied for the flag to be enabled. Database conditions can only be set for flags that exist in the Django settings.","title":"Database conditions"},{"location":"feature-flags/#satellite-apps","text":"Feature flags can be used in satellite apps in exactly the same way they are used in consumerfinance.gov. An example is the use of a feature flagged template choice in the complaintdatabase app .","title":"Satellite apps"},{"location":"feature-flags/#hygiene","text":"Most feature flags are no longer needed once a feature is launched. Follow these steps to remove a flag when its job is done: Create a pull request that deletes the flag from settings, if it was declared there, and removes any related code and tests that referenced the flag. After the changes are merged and deployed to production, check Settings ==> Flags in Wagtail ( /admin/flags/ ) to see if the removed flag is still listed. If so, the flag has been saved in our database. Select the flag, click the DELETE FLAG button at top right, and then choose YES, DELETE IT.","title":"Hygiene"},{"location":"filterable-lists/","text":"Filterable Lists \u00b6 In order to provide a broad, configurable search and filtering interface across areas of our site, we have implemented a custom StreamField block, FilterableList , that allows a user to specify what filters are available, how to order results, and which pages should be included in the search. How It Works FilterableListMixin CategoryFilterableListMixin Forms FilterableListForm EnforcementActionsFilterForm EventArchiveFilterForm Documents Search FilterablePagesDocumentSearch EventFilterablePagesDocumentSearch EnforcementActionFilterablePagesDocumentSearch How It Works \u00b6 The journey on how a page gets a filterable form is not necessarily a straight or simple path, but it is something that is important to know. To start, the page must support the FilterableList block within a StreamField as we mentioned earlier, but from there we start to see some divergence. In order to utilize the FilterableList the page must support one of the following two classes: FilterableListMixin or CategoryFilterableListMixin . FilterableListMixin \u00b6 The more common mixin that pages will extend is the FilterableListMixin . This class defines several important methods, such as get_form_class , which defines the form to use. We also have some methods that retrieve relevant information for the form to use, such as get_filterable_list_wagtail_block , get_filterable_root , and get_filterable_queryset . The bulk of the work is done in the get_context method, which is responsible for getting and populating the form, processing the form, and returning the results to the user. CategoryFilterableListMixin \u00b6 The CategoryFilterableMixin is an extension of the base FilterableListMixin that exposes some new functionality. It modifies how get_filterable_queryset operates in that it gets an initial list of pages but limits them to only ones that are assigned a category within a set of initial categories, which is defined as the variable filterable_categories on a given page model. We can see this in action with both Newsroom ( NewsroomLandingPage ) and Recent Updates ( ActivityLogPage ) pages. Forms \u00b6 As of our initial release of Elasticsearch-backed filterable lists in March 2021, our filterable forms can be broken into three specific forms: FilterableListForm , EnforcementActionsFilterableListForm , and EventArchiveFilterForm . The majority of our filterable lists rely on FilterableListForm and the other two are each leveraged by a single page. FilterableListForm \u00b6 This is the base form that the vast majority of cf.gov uses for filterable lists. It defines the core fields that are visible on the form as well as functions to assist in setting initial data and sanitizing form input. The important information regarding FilterableListForm is that it defines the function get_page_set , which is responsible for invoking a search query. The logic regarding how to pass categories into the search object is due to the previously mentioned CategoryFilterableMixin , which modifies the initial search parameters to enforce a category search if and only if the filterable_categories list is passed into the form when initialized. EnforcementActionsFilterForm \u00b6 The EnforcementActionsFilterForm is an extension of FilterableListForm , adding on two fields specific to Enforcement Actions, and using a refined search class to provide search functionality against the new fields and a proper ordering by initial filing date. EventArchiveFilterForm \u00b6 The EventArchiveFilterForm is another extension of FilterableListForm , the only real modification being the invocation of an event specific search class that allows us to provide filtering based on event dates rather than page publication dates. Documents \u00b6 There is currently only one type of document defined, FilterablePagesDocument , which is based off the AbstractFilterPage class. This document is responsible for housing data related to any of the filterable page types that extend AbstractFilterPage , including EnforcementActionPage , BlogPage , EventPage , and NewsroomPage , to name a few. In order to get fields that are specific to a page type, such as the status list for an Enforcement Action, you use the prepare_field function syntax. The use of get_instances_from_related is to enforce the auto-updating of our index when changes occur to a specific page we have indexed, rather than just the relation to AbstractFilterPage that is reflected in the database. Search \u00b6 Search is the final piece of the puzzle, where we actually leverage Elasticsearch to filter and match documents and return them in an ordered QuerySet . Before breaking down the search classes, it's important to discuss the current implementation from an Elasticsearch perspective to understand how we're gathering results. The expanded search for filterable lists is using a multi-match query across the title, topic name, preview description, and content fields of all FilterablePagesDocument s. We are leveraging a phrase_prefix matching style with a currently configured slop of 2, to allow for some looser matching restrictions. We also provide a boost score for matching to the title and topic name fields, indicated by ^10 within the code base. This boost score is to enable better ordering by relevance when desired. Search currently supports two different methods of ordering results: relevance and date published. Relevance is calculated by the Elasticsearch engine when returning results, and the date published is calculated based on page publication date. Enforcement Actions define their own ordering logic based on initial filing date. FilterablePagesDocumentSearch \u00b6 FilterablePagesDocumentSearch is the core search class that is used across the majority of our searching. It is invoked from FilterableListForm . This search class defines the common structure for our search function, as well as the base logic for filtering against all common fields and logic behind our multi-match and ordering steps. The core function called from outside the class is the search function, which properly chains all of our filter/match/sorting logic and returns the resulting list as a Django QuerySet . EventFilterablePagesDocumentSearch \u00b6 EventFilterablePagesDocumentSearch is an extension of FilterablePagesDocumentSearch that defines behavior specific to our future and past Events listings. The class overwrites one method from its parent, the filter_date function, to change the behavior to filter based on fields specific to events, the start and end date of an event. EnforcementActionsFilterableListForm \u00b6 EnforcementActionsFilterForm is an extension of FilterablePagesDocumentSearch that exposes some additional filter logic through the apply_specific_filters function. We also see that filter_date and order_results have been overwritten to leverage an Enforcement Action-specific field, initial filing date.","title":"Filterable Lists"},{"location":"filterable-lists/#filterable-lists","text":"In order to provide a broad, configurable search and filtering interface across areas of our site, we have implemented a custom StreamField block, FilterableList , that allows a user to specify what filters are available, how to order results, and which pages should be included in the search. How It Works FilterableListMixin CategoryFilterableListMixin Forms FilterableListForm EnforcementActionsFilterForm EventArchiveFilterForm Documents Search FilterablePagesDocumentSearch EventFilterablePagesDocumentSearch EnforcementActionFilterablePagesDocumentSearch","title":"Filterable Lists"},{"location":"filterable-lists/#how-it-works","text":"The journey on how a page gets a filterable form is not necessarily a straight or simple path, but it is something that is important to know. To start, the page must support the FilterableList block within a StreamField as we mentioned earlier, but from there we start to see some divergence. In order to utilize the FilterableList the page must support one of the following two classes: FilterableListMixin or CategoryFilterableListMixin .","title":"How It Works"},{"location":"filterable-lists/#filterablelistmixin","text":"The more common mixin that pages will extend is the FilterableListMixin . This class defines several important methods, such as get_form_class , which defines the form to use. We also have some methods that retrieve relevant information for the form to use, such as get_filterable_list_wagtail_block , get_filterable_root , and get_filterable_queryset . The bulk of the work is done in the get_context method, which is responsible for getting and populating the form, processing the form, and returning the results to the user.","title":"FilterableListMixin"},{"location":"filterable-lists/#categoryfilterablelistmixin","text":"The CategoryFilterableMixin is an extension of the base FilterableListMixin that exposes some new functionality. It modifies how get_filterable_queryset operates in that it gets an initial list of pages but limits them to only ones that are assigned a category within a set of initial categories, which is defined as the variable filterable_categories on a given page model. We can see this in action with both Newsroom ( NewsroomLandingPage ) and Recent Updates ( ActivityLogPage ) pages.","title":"CategoryFilterableListMixin"},{"location":"filterable-lists/#forms","text":"As of our initial release of Elasticsearch-backed filterable lists in March 2021, our filterable forms can be broken into three specific forms: FilterableListForm , EnforcementActionsFilterableListForm , and EventArchiveFilterForm . The majority of our filterable lists rely on FilterableListForm and the other two are each leveraged by a single page.","title":"Forms"},{"location":"filterable-lists/#filterablelistform","text":"This is the base form that the vast majority of cf.gov uses for filterable lists. It defines the core fields that are visible on the form as well as functions to assist in setting initial data and sanitizing form input. The important information regarding FilterableListForm is that it defines the function get_page_set , which is responsible for invoking a search query. The logic regarding how to pass categories into the search object is due to the previously mentioned CategoryFilterableMixin , which modifies the initial search parameters to enforce a category search if and only if the filterable_categories list is passed into the form when initialized.","title":"FilterableListForm"},{"location":"filterable-lists/#enforcementactionsfilterform","text":"The EnforcementActionsFilterForm is an extension of FilterableListForm , adding on two fields specific to Enforcement Actions, and using a refined search class to provide search functionality against the new fields and a proper ordering by initial filing date.","title":"EnforcementActionsFilterForm"},{"location":"filterable-lists/#eventarchivefilterform","text":"The EventArchiveFilterForm is another extension of FilterableListForm , the only real modification being the invocation of an event specific search class that allows us to provide filtering based on event dates rather than page publication dates.","title":"EventArchiveFilterForm"},{"location":"filterable-lists/#documents","text":"There is currently only one type of document defined, FilterablePagesDocument , which is based off the AbstractFilterPage class. This document is responsible for housing data related to any of the filterable page types that extend AbstractFilterPage , including EnforcementActionPage , BlogPage , EventPage , and NewsroomPage , to name a few. In order to get fields that are specific to a page type, such as the status list for an Enforcement Action, you use the prepare_field function syntax. The use of get_instances_from_related is to enforce the auto-updating of our index when changes occur to a specific page we have indexed, rather than just the relation to AbstractFilterPage that is reflected in the database.","title":"Documents"},{"location":"filterable-lists/#search","text":"Search is the final piece of the puzzle, where we actually leverage Elasticsearch to filter and match documents and return them in an ordered QuerySet . Before breaking down the search classes, it's important to discuss the current implementation from an Elasticsearch perspective to understand how we're gathering results. The expanded search for filterable lists is using a multi-match query across the title, topic name, preview description, and content fields of all FilterablePagesDocument s. We are leveraging a phrase_prefix matching style with a currently configured slop of 2, to allow for some looser matching restrictions. We also provide a boost score for matching to the title and topic name fields, indicated by ^10 within the code base. This boost score is to enable better ordering by relevance when desired. Search currently supports two different methods of ordering results: relevance and date published. Relevance is calculated by the Elasticsearch engine when returning results, and the date published is calculated based on page publication date. Enforcement Actions define their own ordering logic based on initial filing date.","title":"Search"},{"location":"filterable-lists/#filterablepagesdocumentsearch","text":"FilterablePagesDocumentSearch is the core search class that is used across the majority of our searching. It is invoked from FilterableListForm . This search class defines the common structure for our search function, as well as the base logic for filtering against all common fields and logic behind our multi-match and ordering steps. The core function called from outside the class is the search function, which properly chains all of our filter/match/sorting logic and returns the resulting list as a Django QuerySet .","title":"FilterablePagesDocumentSearch"},{"location":"filterable-lists/#eventfilterablepagesdocumentsearch","text":"EventFilterablePagesDocumentSearch is an extension of FilterablePagesDocumentSearch that defines behavior specific to our future and past Events listings. The class overwrites one method from its parent, the filter_date function, to change the behavior to filter based on fields specific to events, the start and end date of an event.","title":"EventFilterablePagesDocumentSearch"},{"location":"filterable-lists/#enforcementactionsfilterablelistform","text":"EnforcementActionsFilterForm is an extension of FilterablePagesDocumentSearch that exposes some additional filter logic through the apply_specific_filters function. We also see that filter_date and order_results have been overwritten to leverage an Enforcement Action-specific field, initial filing date.","title":"EnforcementActionsFilterableListForm"},{"location":"forms-in-wagtail/","text":"Including forms into Wagtail page context \u00b6 Wagtail provides a FormBuilder module, but it cannot be used with subclasses of Page, like our CFGOVPage. For our purposes there is AbstractFormBlock , a subclass of StructBlock that implements methods to process a request. If a developer wishes to add a module that includes a form, they only need to follow a few steps in order to get it handled properly: Create the form. Create handler class that implements a method named process . The process method should take in a boolean parameter named is_submitted that flags whether or not that particular module has been the source of the request. The process method should return a dictionary that will be included in the context of the page and a JSONResponse for AJAX requests. If a context is returned, this is where the form would go. Create a subclass of AbstractFormBlock with any other blocks that are required. Add the path to the handler class to the block class' Meta handler attribute. Create a template in which to render the form. Here's an example of a form's block class: ... class FormBlock(AbstractFormBlock): heading = blocks.CharBlock() class Meta: handler = 'app_name.handlers.handler_class' # defaults method = 'POST' icon = 'form' ... And an example of a handler class: ... class ConferenceRegistrationHandler(Handler): def process(self, is_submitted): if is_submitted: form = Form(self.request.POST) if form.is_valid(): return success else: return fail return {'form': Form()} ...","title":"Forms in Wagtail Page Context"},{"location":"forms-in-wagtail/#including-forms-into-wagtail-page-context","text":"Wagtail provides a FormBuilder module, but it cannot be used with subclasses of Page, like our CFGOVPage. For our purposes there is AbstractFormBlock , a subclass of StructBlock that implements methods to process a request. If a developer wishes to add a module that includes a form, they only need to follow a few steps in order to get it handled properly: Create the form. Create handler class that implements a method named process . The process method should take in a boolean parameter named is_submitted that flags whether or not that particular module has been the source of the request. The process method should return a dictionary that will be included in the context of the page and a JSONResponse for AJAX requests. If a context is returned, this is where the form would go. Create a subclass of AbstractFormBlock with any other blocks that are required. Add the path to the handler class to the block class' Meta handler attribute. Create a template in which to render the form. Here's an example of a form's block class: ... class FormBlock(AbstractFormBlock): heading = blocks.CharBlock() class Meta: handler = 'app_name.handlers.handler_class' # defaults method = 'POST' icon = 'form' ... And an example of a handler class: ... class ConferenceRegistrationHandler(Handler): def process(self, is_submitted): if is_submitted: form = Form(self.request.POST) if form.is_valid(): return success else: return fail return {'form': Form()} ...","title":"Including forms into Wagtail page context"},{"location":"functional-testing/","text":"Functional Testing with Cypress \u00b6 We use Cypress for functional testing. Our functional tests make sure that common elements and critical pages are working correctly on the front end by simulating interactions with consumerfinance.gov in a browser. They're particularly useful in giving us confidence that our code is working as intended through dependency upgrades. Installing Cypress \u00b6 We have included Cypress as a dependency of this project. The only installation step is doing a fresh yarn if you haven't already. Running Cypress tests \u00b6 Docker \u00b6 We support a headless Docker container to execute our Cypress tests. The test files are located in the test/cypress/integration/ directory. If you have not previously set up a local Docker network, you will need to stop any running consumerfinance.gov Docker containers, run docker network create cfgov , and start the containers again before you run these commands. docker-compose -f docker-compose.e2e.yml run admin-tests runs a single Docker container (the Wagtail admin test suite, in this case) docker-compose -f docker-compose.e2e.yml up runs all Docker containers Cypress app \u00b6 To run the desktop Cypress app execute yarn run cypress open from the command line. From the app, you can select the tests you want to run and the browser you want to run them in. Command line \u00b6 You can run functional tests from the command line with yarn run cypress run . That will run all tests in the test/cypress/integration/ directory with the default test configuration: headless, in Cypress's default Electron browser, and against localhost:8000 . You might want to modify the test run with some common arguments: --spec test/cypress/integration/{path/to/test.js} runs a single test suite --browser chrome runs the tests in Chrome, which is what we use to run tests in our continuous integration pipeline --headed shows the browser and Cypress output as the tests run, handy for watching what's happening during the tests --no-exit will keep the browser and Cypress output open after the tests complete, handy to inspect any errors --config baseUrl={url} will run the tests against a server other than localhost:8000 Cypress's command line documentation has the list of all the options you can set. Writing Cypress tests \u00b6 When developing new tests for Cypress, it is important to consider what the test is trying to accomplish. We want to ensure that we are not polluting our Cypress tests with things that can be tested at another level, like in unit tests. When adding a test it is often helpful to separate the arrange/act code from the actual assertions in order to improve the readability of our testing code. To do this we have adopted the page model of testing, where we define a page within the application and the methods of interacting with the page separate from the test file itself where we define the assertions. We call these files \"helpers\" and label them as such (example: megamenu-helpers.js ), and we include them alongside the test files themselves in the test/cypress/integration/ directory. (They are ignored when running tests thanks to the configuration of cypress.json .) For example consider the ConsumerTools page \"helper\": export default class ConsumerTools { constructor() {} open() { cy.visit('/consumer-tools/'); } signUp(email) { cy.get('.o-form__email-signup').within(() => { cy.get('input:first').type(email); cy.get('button:first').click(); }); } successNotification() { return cy.get('.m-notification_message'); } } Notice how this class defines functions to retrieve and modify elements on the page but in a more human readable manner. This allows our test file for consumer tools to look like: import ConsumerTools from './consumer-tools-helpers'; let page = new ConsumerTools(); describe('Consumer Tools', () => { it('Should have an email sign up', () => { // Arrange page.open(); // Act page.signUp('testing@cfpb.gov'); // Assert page.successNotification().should('exist'); page.successNotification().contains('Your submission was successfully received.') }); }); Overall it lets our tests show what is intended to be happening on a page without showing the more technical side of how we reference and interact with elements.","title":"Functional Testing"},{"location":"functional-testing/#functional-testing-with-cypress","text":"We use Cypress for functional testing. Our functional tests make sure that common elements and critical pages are working correctly on the front end by simulating interactions with consumerfinance.gov in a browser. They're particularly useful in giving us confidence that our code is working as intended through dependency upgrades.","title":"Functional Testing with Cypress"},{"location":"functional-testing/#installing-cypress","text":"We have included Cypress as a dependency of this project. The only installation step is doing a fresh yarn if you haven't already.","title":"Installing Cypress"},{"location":"functional-testing/#running-cypress-tests","text":"","title":"Running Cypress tests"},{"location":"functional-testing/#docker","text":"We support a headless Docker container to execute our Cypress tests. The test files are located in the test/cypress/integration/ directory. If you have not previously set up a local Docker network, you will need to stop any running consumerfinance.gov Docker containers, run docker network create cfgov , and start the containers again before you run these commands. docker-compose -f docker-compose.e2e.yml run admin-tests runs a single Docker container (the Wagtail admin test suite, in this case) docker-compose -f docker-compose.e2e.yml up runs all Docker containers","title":"Docker"},{"location":"functional-testing/#cypress-app","text":"To run the desktop Cypress app execute yarn run cypress open from the command line. From the app, you can select the tests you want to run and the browser you want to run them in.","title":"Cypress app"},{"location":"functional-testing/#command-line","text":"You can run functional tests from the command line with yarn run cypress run . That will run all tests in the test/cypress/integration/ directory with the default test configuration: headless, in Cypress's default Electron browser, and against localhost:8000 . You might want to modify the test run with some common arguments: --spec test/cypress/integration/{path/to/test.js} runs a single test suite --browser chrome runs the tests in Chrome, which is what we use to run tests in our continuous integration pipeline --headed shows the browser and Cypress output as the tests run, handy for watching what's happening during the tests --no-exit will keep the browser and Cypress output open after the tests complete, handy to inspect any errors --config baseUrl={url} will run the tests against a server other than localhost:8000 Cypress's command line documentation has the list of all the options you can set.","title":"Command line"},{"location":"functional-testing/#writing-cypress-tests","text":"When developing new tests for Cypress, it is important to consider what the test is trying to accomplish. We want to ensure that we are not polluting our Cypress tests with things that can be tested at another level, like in unit tests. When adding a test it is often helpful to separate the arrange/act code from the actual assertions in order to improve the readability of our testing code. To do this we have adopted the page model of testing, where we define a page within the application and the methods of interacting with the page separate from the test file itself where we define the assertions. We call these files \"helpers\" and label them as such (example: megamenu-helpers.js ), and we include them alongside the test files themselves in the test/cypress/integration/ directory. (They are ignored when running tests thanks to the configuration of cypress.json .) For example consider the ConsumerTools page \"helper\": export default class ConsumerTools { constructor() {} open() { cy.visit('/consumer-tools/'); } signUp(email) { cy.get('.o-form__email-signup').within(() => { cy.get('input:first').type(email); cy.get('button:first').click(); }); } successNotification() { return cy.get('.m-notification_message'); } } Notice how this class defines functions to retrieve and modify elements on the page but in a more human readable manner. This allows our test file for consumer tools to look like: import ConsumerTools from './consumer-tools-helpers'; let page = new ConsumerTools(); describe('Consumer Tools', () => { it('Should have an email sign up', () => { // Arrange page.open(); // Act page.signUp('testing@cfpb.gov'); // Assert page.successNotification().should('exist'); page.successNotification().contains('Your submission was successfully received.') }); }); Overall it lets our tests show what is intended to be happening on a page without showing the more technical side of how we reference and interact with elements.","title":"Writing Cypress tests"},{"location":"github-actions/","text":"How we use GitHub Actions for continuous integration and automation \u00b6 What GitHub Actions do \u00b6 We use GitHub Actions on consumerfinance.gov to perform the following tasks: Run automated lint checkers Run automated unit tests Measure unit test coverage Build and deploy this documentation to GitHub on the gh-pages branch . Clean up stored artifacts How GitHub Actions are configured \u00b6 We use the following constraints to optimize our CI builds for speed and utility: Our linting and unit tests run on pull requests only, including subsequent pushes to a pull request's branch. Tests are not run on the main branch. Our documentation is deployed on merges and pushes to the main branch only, and not on pull requests. We do not run builds of any kind on any other branches that are not main , and that are not pull request branches. We store coverage artifacts in between the unit test jobs and the coverage jobs. This requires us to run an action every hour using the purge-artifacts action to clean up these stored artifacts. We use a combination of: Build matrices to run the same tests on different versions of our dependnecies Additional services to provide, for example, PostgreSQL for our tests Our workflows are defined in our .github/workflows directory. An extra task for satellite repositories \u00b6 For our satellite apps , we use GitHub Actions (or Travis, if a repo hasn't been migrated to Actions yet) to build and attach a deployment wheel file to every release. An example is the .whl file on this release of the retirement app .","title":"How We Use GitHub Actions"},{"location":"github-actions/#how-we-use-github-actions-for-continuous-integration-and-automation","text":"","title":"How we use GitHub Actions for continuous integration and automation"},{"location":"github-actions/#what-github-actions-do","text":"We use GitHub Actions on consumerfinance.gov to perform the following tasks: Run automated lint checkers Run automated unit tests Measure unit test coverage Build and deploy this documentation to GitHub on the gh-pages branch . Clean up stored artifacts","title":"What GitHub Actions do"},{"location":"github-actions/#how-github-actions-are-configured","text":"We use the following constraints to optimize our CI builds for speed and utility: Our linting and unit tests run on pull requests only, including subsequent pushes to a pull request's branch. Tests are not run on the main branch. Our documentation is deployed on merges and pushes to the main branch only, and not on pull requests. We do not run builds of any kind on any other branches that are not main , and that are not pull request branches. We store coverage artifacts in between the unit test jobs and the coverage jobs. This requires us to run an action every hour using the purge-artifacts action to clean up these stored artifacts. We use a combination of: Build matrices to run the same tests on different versions of our dependnecies Additional services to provide, for example, PostgreSQL for our tests Our workflows are defined in our .github/workflows directory.","title":"How GitHub Actions are configured"},{"location":"github-actions/#an-extra-task-for-satellite-repositories","text":"For our satellite apps , we use GitHub Actions (or Travis, if a repo hasn't been migrated to Actions yet) to build and attach a deployment wheel file to every release. An example is the .whl file on this release of the retirement app .","title":"An extra task for satellite repositories"},{"location":"housing-counselor-tool/","text":"Find a Housing Counselor \u00b6 The Find a Housing Counselor tool allows users to search for HUD-approved housing counselors. Users enter a U.S. ZIP code, and the tool returns a list of the ten housing counselors nearest to that ZIP code location. The page also displays a map of the results using Mapbox. When the user enters a ZIP code in the page's search box, the Django view fetches a JSON file of the results from our Amazon S3 bucket. The results are inserted into the page template and the Mapbox map . The page also contains a link to a PDF version of the results, which is also stored in S3. The files are publicly accessible, so the tool can run on localhost or in a container without any change in behavior. This page documents the process we use to generate the JSON and PDF files the Find a Housing Counselor tool relies on. Housing Counselor Data Processing \u00b6 The tool gets its data from the U.S. Department of Housing and Urban Development (HUD). HUD provides an API to their list of approved counseling agencies. A daily job, cf.gov-housing-counselor-data on our external Jenkins server, queries HUD data and produces the JSON and PDF files we use for the Find a Housing Counselor tool. It performs the following steps, each of which is optional and configured using parameters before starting the job. Geocode ZIP codes ( GEOCODE_ZIPCODES ) Generate JSON files ( MAKE_JSON ) Generate HTML files ( MAKE_HTML ) Generate PDFs ( MAKE_PDF ) Upload to S3 ( UPLOAD_TO_S3 ) Geocode ZIP codes \u00b6 The GEOCODE_ZIPCODES step generates a file of all ZIP codes in the United States and their location latitude and longitude and saves it in the Jenkins workspace. By default, this step is not enabled. Since ZIP code geographical information rarely changes, we run this step rarely by manually enabling the option in the Jenkins job. If enabled, this step calls the hud_geocode_zipcodes management command. The management command in turn calls the BulkZipCodeGeocoder in geocoder.py . BulkZipCodeGeocoder uses the Mapbox geocoding API to determine which 5-digit number sequences are ZIP codes and fetch their latitude and longitude values. The management command inserts this data into a CSV and saves it to ./zipcodes.csv on the Jenkins job workspace. The generated file looks like this: 12305,42.81,-73.94 12306,42.77,-73.96 12307,42.81,-73.93 12308,42.82,-73.93 12309,42.81,-73.91 12325,42.88779,-73.99597 12345,42.80856,-74.02737 Generate JSON files \u00b6 When enabled, the MAKE_JSON step generates a JSON file of housing counselor data for each ZIP code in the U.S. Each file contains the ten results geographically nearest to the ZIP code's latitude and longitude. This step is enabled by default. This step calls the hud_generate_json management command, which performs the following steps: fetch agency listings from HUD save a copy of the full results, for our records clean the results fill in any missing latitude and longitude values create files of the 10 nearest results for each U.S. ZIP code Fetch agency listings \u00b6 ( in fetcher.py ) Request every housing counselor in HUD's database with a request to https://data.hud.gov/Housing_Counselor/searchByLocation?Lat=38.8951&Long=-77.0367&Distance=5000 . It returns thousands of results like this: { \"services\": \"DFC,FBC,PPC,RHC\", \"languages\": \"ENG,SPA\", \"agc_STATUS\": \"A\", \"agc_SRC_CD\": \"HUD\", \"counslg_METHOD\": \"Face to Face Counseling,Group Counseling,Phone Counseling\", \"agcid\": \"80790\", \"adr1\": \"1234 N. Example St.\", \"adr2\": \" \", \"city\": \"SPRINGFIELD\", \"email\": \"counselor@example.org\", \"fax\": \"999-888-7777\", \"nme\": \"EXAMPLE COMMUNITY HOUSING SERVICES\", \"phone1\": \"111-222-3333\", \"statecd\": \"MI\", \"weburl\": \"www.example.org\", \"zipcd\": \"48219-8888\", \"agc_ADDR_LATITUDE\": \"42.442658\", \"agc_ADDR_LONGITUDE\": \"-83.28329\", \"parentid\": \"81228\", \"county_NME\": \"\", \"phone2\": \" \", \"mailingadr1\": \"1234 N. Example St.\", \"mailingadr2\": \" \", \"mailingcity\": \"SPRINGFIELD\", \"mailingzipcd\": \"48219-8888\", \"mailingstatecd\": \"MI\", \"state_NME\": \"Michigan\", \"state_FIPS_CODE\": null, \"faithbased\": \"N\", \"colonias_IND\": \"Y\", \"migrantwkrs_IND\": \"N\" }, Next, we replace the languages value with the full language names. We replace the services value with fully spelled out service descriptions. Both of these mappings come from the HUD API. Save the results \u00b6 ( in results_archiver.py ) Save a copy of the full set of results as a zip file in the home directory. The Jenkins job then transfers the zip file to S3 as part of the MAKE JSON step. The file is saved in S3 at /archive/{date}.zip . This copy can be used as the canonical set of HUD data that day in case of an enforcement action. Clean the results \u00b6 ( in cleaner.py ) Clean the data: Convert latitude and longitude values to float values. Convert the the city and organization name to title case. Ensure email appears to be an email (otherwise leave it blank). If a URL value is present, ensure it begins with http:// . Backfill missing latitude and longitude values \u00b6 ( in geocoder.py ) If any counselor records are missing their latitude or longitude data, we fill in those values with the lat/long location of the agency's ZIP code. This uses the data file created in the Geocode ZIP Codes stage of the Jenkins job ( above ). Create collections of results by ZIP code \u00b6 ( in generator.py ) Create an in-memory SQLite database and define a distance_in_miles function in it. Fill the database with a three-column table: For each counselor in the list, include its latitude and longitude (both in radians) and all of its information from the HUD API as a JSON text string. For each ZIP code in the U.S., query the database to find the 10 closest housing counselors to the lat/long of that ZIP. Put the information in a JSON structure like this (but with ten results instead of one): { \"zip\": { \"lat\": 42.80856, \"lng\": -74.02737, \"zipcode\": \"12345\" }, \"counseling_agencies\": [{ \"adr1\": \"1234 N. Example St.\", \"state_FIPS_CODE\": null, \"adr2\": \" \", \"zipcd\": \"48219-8888\", \"mailingcity\": \"Springfield\", \"weburl\": \"http://www.example.org\", \"agc_STATUS\": \"A\", \"city\": \"Springfield\", \"languages\": [\"English\", \"Spanish\"], \"faithbased\": \"N\", \"mailingstatecd\": \"MI\", \"email\": \"counselor@example.org\", \"fax\": \"999-888-7777\", \"phone1\": \"111-222-3333\", \"distance\": 5.430720023569205, \"phone2\": \" \", \"agc_ADDR_LATITUDE\": 42.442658, \"agcid\": \"80790\", \"agc_SRC_CD\": \"HUD\", \"nme\": \"Example Community Housing Services\", \"migrantwkrs_IND\": \"N\", \"parentid\": \"82772\", \"services\": [\"Mortgage Delinquency and Default Resolution Counse\", \"Home Improvement and Rehabilitation Counseling\", \"Pre-purchase Counseling\", \"Pre-purchase Homebuyer Education Workshops\", \"Rental Housing Counseling\"], \"counslg_METHOD\": \"Face to Face Counseling,Group Counseling,Phone Counseling\", \"county_NME\": \"\", \"mailingadr1\": \"1234 N. Example St.\", \"statecd\": \"MI\", \"mailingadr2\": \" \", \"mailingzipcd\": \"48219-8888\", \"state_NME\": \"Michigan\", \"agc_ADDR_LONGITUDE\": -83.28329, \"colonias_IND\": \"Y\" }] } Save the resulting JSON files on the Jenkins job workspace, in a jsons directory, e.g. jsons/12345.json . Generate HTML files \u00b6 When enabled, the MAKE_HTML step generates an HTML page of housing counselor results, including styles, for each file created in the previous step. It saves them in an htmls directory on the Jenkins job workspace. This step is enabled by default. This step calls the hud_generate_html management command, which calls HTML generation code in generator.py . Django renders the housing_counselor/pdf_selfcontained.html template with the housing counselor data from each JSON file. We save the resulting HTML files on the Jenkins job workspace, in a htmls directory, e.g. htmls/12345.html . Generate PDFs \u00b6 When enabled, the MAKE_PDF step generates a PDF of each file created in the previous step. It saves them in a pdfs directory on the Jenkins job workspace. This step is enabled by default. This step uses a HTML to PDF conversion command line tool. We run the conversion on each file in the htmls directory, generating a PDF version of each, e.g. pdfs/12345.pdf . Upload to S3 \u00b6 When enabled, the UPLOAD_TO_S3 step uploads the contents of the jsons and pdfs directories to an Amazon S3 bucket where it can be accessed by consumerfinance.gov. This step is enabled by default. The files are publicly accessible, e.g.: https://files.consumerfinance.gov/a/assets/hud/jsons/12345.json https://files.consumerfinance.gov/a/assets/hud/pdfs/12345.pdf","title":"Find a Housing Counselor Tool"},{"location":"housing-counselor-tool/#find-a-housing-counselor","text":"The Find a Housing Counselor tool allows users to search for HUD-approved housing counselors. Users enter a U.S. ZIP code, and the tool returns a list of the ten housing counselors nearest to that ZIP code location. The page also displays a map of the results using Mapbox. When the user enters a ZIP code in the page's search box, the Django view fetches a JSON file of the results from our Amazon S3 bucket. The results are inserted into the page template and the Mapbox map . The page also contains a link to a PDF version of the results, which is also stored in S3. The files are publicly accessible, so the tool can run on localhost or in a container without any change in behavior. This page documents the process we use to generate the JSON and PDF files the Find a Housing Counselor tool relies on.","title":"Find a Housing Counselor"},{"location":"housing-counselor-tool/#housing-counselor-data-processing","text":"The tool gets its data from the U.S. Department of Housing and Urban Development (HUD). HUD provides an API to their list of approved counseling agencies. A daily job, cf.gov-housing-counselor-data on our external Jenkins server, queries HUD data and produces the JSON and PDF files we use for the Find a Housing Counselor tool. It performs the following steps, each of which is optional and configured using parameters before starting the job. Geocode ZIP codes ( GEOCODE_ZIPCODES ) Generate JSON files ( MAKE_JSON ) Generate HTML files ( MAKE_HTML ) Generate PDFs ( MAKE_PDF ) Upload to S3 ( UPLOAD_TO_S3 )","title":"Housing Counselor Data Processing"},{"location":"housing-counselor-tool/#geocode-zip-codes","text":"The GEOCODE_ZIPCODES step generates a file of all ZIP codes in the United States and their location latitude and longitude and saves it in the Jenkins workspace. By default, this step is not enabled. Since ZIP code geographical information rarely changes, we run this step rarely by manually enabling the option in the Jenkins job. If enabled, this step calls the hud_geocode_zipcodes management command. The management command in turn calls the BulkZipCodeGeocoder in geocoder.py . BulkZipCodeGeocoder uses the Mapbox geocoding API to determine which 5-digit number sequences are ZIP codes and fetch their latitude and longitude values. The management command inserts this data into a CSV and saves it to ./zipcodes.csv on the Jenkins job workspace. The generated file looks like this: 12305,42.81,-73.94 12306,42.77,-73.96 12307,42.81,-73.93 12308,42.82,-73.93 12309,42.81,-73.91 12325,42.88779,-73.99597 12345,42.80856,-74.02737","title":"Geocode ZIP codes"},{"location":"housing-counselor-tool/#generate-json-files","text":"When enabled, the MAKE_JSON step generates a JSON file of housing counselor data for each ZIP code in the U.S. Each file contains the ten results geographically nearest to the ZIP code's latitude and longitude. This step is enabled by default. This step calls the hud_generate_json management command, which performs the following steps: fetch agency listings from HUD save a copy of the full results, for our records clean the results fill in any missing latitude and longitude values create files of the 10 nearest results for each U.S. ZIP code","title":"Generate JSON files"},{"location":"housing-counselor-tool/#fetch-agency-listings","text":"( in fetcher.py ) Request every housing counselor in HUD's database with a request to https://data.hud.gov/Housing_Counselor/searchByLocation?Lat=38.8951&Long=-77.0367&Distance=5000 . It returns thousands of results like this: { \"services\": \"DFC,FBC,PPC,RHC\", \"languages\": \"ENG,SPA\", \"agc_STATUS\": \"A\", \"agc_SRC_CD\": \"HUD\", \"counslg_METHOD\": \"Face to Face Counseling,Group Counseling,Phone Counseling\", \"agcid\": \"80790\", \"adr1\": \"1234 N. Example St.\", \"adr2\": \" \", \"city\": \"SPRINGFIELD\", \"email\": \"counselor@example.org\", \"fax\": \"999-888-7777\", \"nme\": \"EXAMPLE COMMUNITY HOUSING SERVICES\", \"phone1\": \"111-222-3333\", \"statecd\": \"MI\", \"weburl\": \"www.example.org\", \"zipcd\": \"48219-8888\", \"agc_ADDR_LATITUDE\": \"42.442658\", \"agc_ADDR_LONGITUDE\": \"-83.28329\", \"parentid\": \"81228\", \"county_NME\": \"\", \"phone2\": \" \", \"mailingadr1\": \"1234 N. Example St.\", \"mailingadr2\": \" \", \"mailingcity\": \"SPRINGFIELD\", \"mailingzipcd\": \"48219-8888\", \"mailingstatecd\": \"MI\", \"state_NME\": \"Michigan\", \"state_FIPS_CODE\": null, \"faithbased\": \"N\", \"colonias_IND\": \"Y\", \"migrantwkrs_IND\": \"N\" }, Next, we replace the languages value with the full language names. We replace the services value with fully spelled out service descriptions. Both of these mappings come from the HUD API.","title":"Fetch agency listings"},{"location":"housing-counselor-tool/#save-the-results","text":"( in results_archiver.py ) Save a copy of the full set of results as a zip file in the home directory. The Jenkins job then transfers the zip file to S3 as part of the MAKE JSON step. The file is saved in S3 at /archive/{date}.zip . This copy can be used as the canonical set of HUD data that day in case of an enforcement action.","title":"Save the results"},{"location":"housing-counselor-tool/#clean-the-results","text":"( in cleaner.py ) Clean the data: Convert latitude and longitude values to float values. Convert the the city and organization name to title case. Ensure email appears to be an email (otherwise leave it blank). If a URL value is present, ensure it begins with http:// .","title":"Clean the results"},{"location":"housing-counselor-tool/#backfill-missing-latitude-and-longitude-values","text":"( in geocoder.py ) If any counselor records are missing their latitude or longitude data, we fill in those values with the lat/long location of the agency's ZIP code. This uses the data file created in the Geocode ZIP Codes stage of the Jenkins job ( above ).","title":"Backfill missing latitude and longitude values"},{"location":"housing-counselor-tool/#create-collections-of-results-by-zip-code","text":"( in generator.py ) Create an in-memory SQLite database and define a distance_in_miles function in it. Fill the database with a three-column table: For each counselor in the list, include its latitude and longitude (both in radians) and all of its information from the HUD API as a JSON text string. For each ZIP code in the U.S., query the database to find the 10 closest housing counselors to the lat/long of that ZIP. Put the information in a JSON structure like this (but with ten results instead of one): { \"zip\": { \"lat\": 42.80856, \"lng\": -74.02737, \"zipcode\": \"12345\" }, \"counseling_agencies\": [{ \"adr1\": \"1234 N. Example St.\", \"state_FIPS_CODE\": null, \"adr2\": \" \", \"zipcd\": \"48219-8888\", \"mailingcity\": \"Springfield\", \"weburl\": \"http://www.example.org\", \"agc_STATUS\": \"A\", \"city\": \"Springfield\", \"languages\": [\"English\", \"Spanish\"], \"faithbased\": \"N\", \"mailingstatecd\": \"MI\", \"email\": \"counselor@example.org\", \"fax\": \"999-888-7777\", \"phone1\": \"111-222-3333\", \"distance\": 5.430720023569205, \"phone2\": \" \", \"agc_ADDR_LATITUDE\": 42.442658, \"agcid\": \"80790\", \"agc_SRC_CD\": \"HUD\", \"nme\": \"Example Community Housing Services\", \"migrantwkrs_IND\": \"N\", \"parentid\": \"82772\", \"services\": [\"Mortgage Delinquency and Default Resolution Counse\", \"Home Improvement and Rehabilitation Counseling\", \"Pre-purchase Counseling\", \"Pre-purchase Homebuyer Education Workshops\", \"Rental Housing Counseling\"], \"counslg_METHOD\": \"Face to Face Counseling,Group Counseling,Phone Counseling\", \"county_NME\": \"\", \"mailingadr1\": \"1234 N. Example St.\", \"statecd\": \"MI\", \"mailingadr2\": \" \", \"mailingzipcd\": \"48219-8888\", \"state_NME\": \"Michigan\", \"agc_ADDR_LONGITUDE\": -83.28329, \"colonias_IND\": \"Y\" }] } Save the resulting JSON files on the Jenkins job workspace, in a jsons directory, e.g. jsons/12345.json .","title":"Create collections of results by ZIP code"},{"location":"housing-counselor-tool/#generate-html-files","text":"When enabled, the MAKE_HTML step generates an HTML page of housing counselor results, including styles, for each file created in the previous step. It saves them in an htmls directory on the Jenkins job workspace. This step is enabled by default. This step calls the hud_generate_html management command, which calls HTML generation code in generator.py . Django renders the housing_counselor/pdf_selfcontained.html template with the housing counselor data from each JSON file. We save the resulting HTML files on the Jenkins job workspace, in a htmls directory, e.g. htmls/12345.html .","title":"Generate HTML files"},{"location":"housing-counselor-tool/#generate-pdfs","text":"When enabled, the MAKE_PDF step generates a PDF of each file created in the previous step. It saves them in a pdfs directory on the Jenkins job workspace. This step is enabled by default. This step uses a HTML to PDF conversion command line tool. We run the conversion on each file in the htmls directory, generating a PDF version of each, e.g. pdfs/12345.pdf .","title":"Generate PDFs"},{"location":"housing-counselor-tool/#upload-to-s3","text":"When enabled, the UPLOAD_TO_S3 step uploads the contents of the jsons and pdfs directories to an Amazon S3 bucket where it can be accessed by consumerfinance.gov. This step is enabled by default. The files are publicly accessible, e.g.: https://files.consumerfinance.gov/a/assets/hud/jsons/12345.json https://files.consumerfinance.gov/a/assets/hud/pdfs/12345.pdf","title":"Upload to S3"},{"location":"installation/","text":"Setting up consumerfinance.gov \u00b6 Quickstart \u00b6 This quickstart requires a working Docker Desktop installation and git: Clone the repository : git clone https://github.com/cfpb/consumerfinance.gov.git cd consumerfinance.gov Set up and run the Docker containers : docker-compose up This may take some time, as it will also load initial data and build the frontend . consumerfinance.gov should now be available at http://localhost:8000 . This documentation will be available at http://localhost:8888 . The Wagtail admin area will be available at http://localhost:8000/admin/ , which you can log into with the credentials admin / admin . Please see our running consumerfinance.gov documentation for next steps. There are also optional steps described below, as well as alternative setup options . Detailed installation \u00b6 The quickstart above should get you started. Each step has some additional detail below. Clone the repository \u00b6 Using the console, navigate to the root directory in which your projects live and clone this project's repository: git clone git@github.com:cfpb/consumerfinance.gov.git cd consumerfinance.gov You may also wish to fork the repository on GitHub and clone the resultant personal fork. This is advised if you are going to be doing development on consumerfinance.gov and contributing to the project. Set up the environment (optional) \u00b6 The consumerfinance.gov Django site relies on environment variables defined in a .env file. If this is your first time setting up the project, copy .env_SAMPLE to .env : cp -a .env_SAMPLE .env Set up a local Python environment (optional) \u00b6 For running our Python unit tests, linting, etc outside of the Docker container, we rely on a local Python environment. Note Our local Python environment requires pyenv with pyenv-virtualenv . They can be installed from Homebrew on macOS: brew install pyenv pyenv-virtualenv Python 3.8 must then be installed once pyenv is installed: pyenv install 3.8.12 First we need to create a Python virtualenv for consumerfinance.gov: pyenv virtualenv 3.8.12 consumerfinance.gov Then we'll need to activate it. Activating the virtualenv is necessary before using it in the future as well: pyenv activate consumerfinance.gov Once activated, our Python CI requirements can be installed in the virtualenv: pip install -r requirements/ci.txt Install our private fonts (optional) \u00b6 consumerfinance.gov uses a proprietary licensed font, Avenir. If you want to pull this from a content delivery network (CDN), you can set the @use-font-cdn to true and rebuild the assets with yarn run gulp build . If you want to install self-hosted fonts locally, you can place the font files in static.in/cfgov-fonts/fonts/ . If you are a CFPB employee, you can perform this step with: git clone https://[GHE]/CFGOV/cfgov-fonts/ static.in/cfgov-fonts Where [GHE] is our GitHub Enterprise URL. Build the frontend \u00b6 Note Our frontend requires Node.js 16 with Yarn . We prefer nvm for Node.js version management. nvm can be installed using: curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | sh Node 16 must then be installed once nvm is installed: nvm install 16 Node.js 16 can then be used in any sh using: nvm use 16 Yarn must then be installed: curl -o- -L https://yarnpkg.com/install.sh | sh We have a single script that will install our frontend dependencies for both building and unit testing/linting/etc : ./frontend.sh Gulp can be used to rebuild our assets after the initial setup: yarn run gulp build Note If you are having trouble loading JavaScript edits locally, you may need to turn off service workers for localhost:8000. Learn how to manage service workers in Firefox and Chrome . Set up and run the Docker containers \u00b6 consumerfinance.gov depends on PostgreSQL database and Elasticsearch. We use docker-compose to run these services along side the consumerfinance.gov Django site. To build and run our Docker containers for the first time, run: docker-compose up This will build and start our PostgreSQL, Elasticsearch, Python, and documentation services. The first time this is fun, it will load initial data and build the frontend for you. Load initial data \u00b6 Our initial-data.sh script can be used to initialize a new database to make it easy to get started working on consumerfinance.gov. This script ensures that all migrations are applied to the database and then does the following: Creates an admin superuser with password admin . If it doesn't already exist, creates a new Wagtail home page named CFGOV , with a slug of cfgov . Updates the default Wagtail site to use the port defined by the DJANGO_HTTP_PORT environment variable, if defined; otherwise this port is set to 80. If it doesn't already exist, creates a new wagtail-sharing SharingSite with a hostname and port defined by the WAGTAIL_SHARING_HOSTNAME and DJANGO_HTTP_PORT environment variables. This script must be run inside the Docker python container: docker-compose exec python bash ./initial-data.sh Load a database dump \u00b6 Alternatively, one of our database dumps can be installed using our refresh-data.sh script. You can get a database dump by defining CFGOV_PROD_DB_LOCATION in your .env file as described in GitHub Enterprise at [GHE]/CFGOV/platform/wiki/Database-downloads#resources-available-via-s3, or inside a Docker python container sh immediately before running refresh-data.sh : docker-compose exec python bash CFGOV_PROD_DB_LOCATION=http://(rest of the URL) ./refresh-data.sh refresh-data.sh can also be given a path to a gziped database dump: ./refresh-data.sh /path/to/dump.sql.gz Finally, build the Elasticsearch index : ./cfgov/manage.py search_index --create Alternative setups \u00b6 consumerfinance.gov requires a Python environment, PostgreSQL, and Elasticsearch to run. None of this requires Docker, Docker is simply a convenient way to ensure consistent versioning and running of these services along with the consumerfinance.gov Django site. The consumerfinance.gov Django site can be run locally in a virtualenv and can use PostgreSQL and Elasticsearch from either our docker-compose file or from Homebrew. PostgreSQL and Elasticsearch from Docker \u00b6 To build and start only the PostgreSQL ( postgres ) and Elasticsearch ( elasticsearch ) containers from our docker-compose file, explicitly specify them as arguments to docker-compose : docker-compose up postgres elasticsearch This will expose PostgreSQL on port 5432 on localhost and Elasticsearch on port 9200 on localhost . PostgreSQL and Elasticsearch from Homebrew \u00b6 You can install PostgreSQL and Elasticsearch from Homebrew if you're on a Mac: brew install postgresql brew install elasticsearch Once it's installed, you can configure it to run as a service: brew services start postgresql brew services start elasticsearch Our recommended Postgres configuration is a database named cfgov and a user named cfpb , with data stored in schema cfpb . This can be created with the following commands: dropdb --if-exists cfgov && dropuser --if-exists cfpb psql postgres -c \"CREATE USER cfpb WITH LOGIN PASSWORD 'cfpb' CREATEDB\" psql postgres -c \"CREATE DATABASE cfgov OWNER cfpb\" psql postgres://cfpb:cfpb@localhost/cfgov -c \"CREATE SCHEMA cfpb\" We don't support using an SQLite database because we use database fields that are specific to Postgres. The CREATEDB keyword above allows the cfpb user to create a temporary Django database when running unit tests. Set up the consumerfinance.gov virtualenv \u00b6 After you have chosen a means to run PostgreSQL and Elasticsearch, set up the environment , set up a local Python environment , optionally installed our private fonts , and built the frontend , all the Python dependencies for running locally can be installed: pyenv activate consumerfinance.gov pip install -r requirements/local.txt Once complete, our runserver.sh script will bring up the site at http://localhost:8000 . ./runserver.sh Additional setup \u00b6 Sync local image storage (optional) \u00b6 If using a database dump, pages will contain links to images that exist in the database but don't exist on your local disk. This will cause broken or missing images when browsing the site locally. For example, in production images are stored on S3, but when running locally they are stored on disk. This project includes a Django management command that can be used to download any remote images referenced in the database so that they can be served when running locally. cfgov/manage.py sync_image_storage https://files.consumerfinance.gov/f/ ./cfgov/f/ This downloads all remote images (and image renditions) referenced in the database, retrieving them from the specified URL and storing them in the specified local directory. Install GNU gettext for Django translation support (optional) \u00b6 In order to generate Django translations as documented here , you'll need to install the GNU gettext library. On macOS, GNU gettext is available via Homebrew: brew install gettext but it gets installed as \"keg-only\" due to conflicts with the default installation of BSD gettext. This means that GNU gettext won't be loaded in your PATH by default. To fix this, you can run brew link --force gettext to force its installation, or see brew info gettext for an alternate solution. If installed locally, you should be able to run this command successfully: $ gettext --version GNU gettext is also required to run our translation-related unit tests locally.","title":"Setup"},{"location":"installation/#setting-up-consumerfinancegov","text":"","title":"Setting up consumerfinance.gov"},{"location":"installation/#quickstart","text":"This quickstart requires a working Docker Desktop installation and git: Clone the repository : git clone https://github.com/cfpb/consumerfinance.gov.git cd consumerfinance.gov Set up and run the Docker containers : docker-compose up This may take some time, as it will also load initial data and build the frontend . consumerfinance.gov should now be available at http://localhost:8000 . This documentation will be available at http://localhost:8888 . The Wagtail admin area will be available at http://localhost:8000/admin/ , which you can log into with the credentials admin / admin . Please see our running consumerfinance.gov documentation for next steps. There are also optional steps described below, as well as alternative setup options .","title":"Quickstart"},{"location":"installation/#detailed-installation","text":"The quickstart above should get you started. Each step has some additional detail below.","title":"Detailed installation"},{"location":"installation/#clone-the-repository","text":"Using the console, navigate to the root directory in which your projects live and clone this project's repository: git clone git@github.com:cfpb/consumerfinance.gov.git cd consumerfinance.gov You may also wish to fork the repository on GitHub and clone the resultant personal fork. This is advised if you are going to be doing development on consumerfinance.gov and contributing to the project.","title":"Clone the repository"},{"location":"installation/#set-up-the-environment-optional","text":"The consumerfinance.gov Django site relies on environment variables defined in a .env file. If this is your first time setting up the project, copy .env_SAMPLE to .env : cp -a .env_SAMPLE .env","title":"Set up the environment (optional)"},{"location":"installation/#set-up-a-local-python-environment-optional","text":"For running our Python unit tests, linting, etc outside of the Docker container, we rely on a local Python environment. Note Our local Python environment requires pyenv with pyenv-virtualenv . They can be installed from Homebrew on macOS: brew install pyenv pyenv-virtualenv Python 3.8 must then be installed once pyenv is installed: pyenv install 3.8.12 First we need to create a Python virtualenv for consumerfinance.gov: pyenv virtualenv 3.8.12 consumerfinance.gov Then we'll need to activate it. Activating the virtualenv is necessary before using it in the future as well: pyenv activate consumerfinance.gov Once activated, our Python CI requirements can be installed in the virtualenv: pip install -r requirements/ci.txt","title":"Set up a local Python environment (optional)"},{"location":"installation/#install-our-private-fonts-optional","text":"consumerfinance.gov uses a proprietary licensed font, Avenir. If you want to pull this from a content delivery network (CDN), you can set the @use-font-cdn to true and rebuild the assets with yarn run gulp build . If you want to install self-hosted fonts locally, you can place the font files in static.in/cfgov-fonts/fonts/ . If you are a CFPB employee, you can perform this step with: git clone https://[GHE]/CFGOV/cfgov-fonts/ static.in/cfgov-fonts Where [GHE] is our GitHub Enterprise URL.","title":"Install our private fonts (optional)"},{"location":"installation/#build-the-frontend","text":"Note Our frontend requires Node.js 16 with Yarn . We prefer nvm for Node.js version management. nvm can be installed using: curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.0/install.sh | sh Node 16 must then be installed once nvm is installed: nvm install 16 Node.js 16 can then be used in any sh using: nvm use 16 Yarn must then be installed: curl -o- -L https://yarnpkg.com/install.sh | sh We have a single script that will install our frontend dependencies for both building and unit testing/linting/etc : ./frontend.sh Gulp can be used to rebuild our assets after the initial setup: yarn run gulp build Note If you are having trouble loading JavaScript edits locally, you may need to turn off service workers for localhost:8000. Learn how to manage service workers in Firefox and Chrome .","title":"Build the frontend"},{"location":"installation/#set-up-and-run-the-docker-containers","text":"consumerfinance.gov depends on PostgreSQL database and Elasticsearch. We use docker-compose to run these services along side the consumerfinance.gov Django site. To build and run our Docker containers for the first time, run: docker-compose up This will build and start our PostgreSQL, Elasticsearch, Python, and documentation services. The first time this is fun, it will load initial data and build the frontend for you.","title":"Set up and run the Docker containers"},{"location":"installation/#load-initial-data","text":"Our initial-data.sh script can be used to initialize a new database to make it easy to get started working on consumerfinance.gov. This script ensures that all migrations are applied to the database and then does the following: Creates an admin superuser with password admin . If it doesn't already exist, creates a new Wagtail home page named CFGOV , with a slug of cfgov . Updates the default Wagtail site to use the port defined by the DJANGO_HTTP_PORT environment variable, if defined; otherwise this port is set to 80. If it doesn't already exist, creates a new wagtail-sharing SharingSite with a hostname and port defined by the WAGTAIL_SHARING_HOSTNAME and DJANGO_HTTP_PORT environment variables. This script must be run inside the Docker python container: docker-compose exec python bash ./initial-data.sh","title":"Load initial data"},{"location":"installation/#load-a-database-dump","text":"Alternatively, one of our database dumps can be installed using our refresh-data.sh script. You can get a database dump by defining CFGOV_PROD_DB_LOCATION in your .env file as described in GitHub Enterprise at [GHE]/CFGOV/platform/wiki/Database-downloads#resources-available-via-s3, or inside a Docker python container sh immediately before running refresh-data.sh : docker-compose exec python bash CFGOV_PROD_DB_LOCATION=http://(rest of the URL) ./refresh-data.sh refresh-data.sh can also be given a path to a gziped database dump: ./refresh-data.sh /path/to/dump.sql.gz Finally, build the Elasticsearch index : ./cfgov/manage.py search_index --create","title":"Load a database dump"},{"location":"installation/#alternative-setups","text":"consumerfinance.gov requires a Python environment, PostgreSQL, and Elasticsearch to run. None of this requires Docker, Docker is simply a convenient way to ensure consistent versioning and running of these services along with the consumerfinance.gov Django site. The consumerfinance.gov Django site can be run locally in a virtualenv and can use PostgreSQL and Elasticsearch from either our docker-compose file or from Homebrew.","title":"Alternative setups"},{"location":"installation/#postgresql-and-elasticsearch-from-docker","text":"To build and start only the PostgreSQL ( postgres ) and Elasticsearch ( elasticsearch ) containers from our docker-compose file, explicitly specify them as arguments to docker-compose : docker-compose up postgres elasticsearch This will expose PostgreSQL on port 5432 on localhost and Elasticsearch on port 9200 on localhost .","title":"PostgreSQL and Elasticsearch from Docker"},{"location":"installation/#postgresql-and-elasticsearch-from-homebrew","text":"You can install PostgreSQL and Elasticsearch from Homebrew if you're on a Mac: brew install postgresql brew install elasticsearch Once it's installed, you can configure it to run as a service: brew services start postgresql brew services start elasticsearch Our recommended Postgres configuration is a database named cfgov and a user named cfpb , with data stored in schema cfpb . This can be created with the following commands: dropdb --if-exists cfgov && dropuser --if-exists cfpb psql postgres -c \"CREATE USER cfpb WITH LOGIN PASSWORD 'cfpb' CREATEDB\" psql postgres -c \"CREATE DATABASE cfgov OWNER cfpb\" psql postgres://cfpb:cfpb@localhost/cfgov -c \"CREATE SCHEMA cfpb\" We don't support using an SQLite database because we use database fields that are specific to Postgres. The CREATEDB keyword above allows the cfpb user to create a temporary Django database when running unit tests.","title":"PostgreSQL and Elasticsearch from Homebrew"},{"location":"installation/#set-up-the-consumerfinancegov-virtualenv","text":"After you have chosen a means to run PostgreSQL and Elasticsearch, set up the environment , set up a local Python environment , optionally installed our private fonts , and built the frontend , all the Python dependencies for running locally can be installed: pyenv activate consumerfinance.gov pip install -r requirements/local.txt Once complete, our runserver.sh script will bring up the site at http://localhost:8000 . ./runserver.sh","title":"Set up the consumerfinance.gov virtualenv"},{"location":"installation/#additional-setup","text":"","title":"Additional setup"},{"location":"installation/#sync-local-image-storage-optional","text":"If using a database dump, pages will contain links to images that exist in the database but don't exist on your local disk. This will cause broken or missing images when browsing the site locally. For example, in production images are stored on S3, but when running locally they are stored on disk. This project includes a Django management command that can be used to download any remote images referenced in the database so that they can be served when running locally. cfgov/manage.py sync_image_storage https://files.consumerfinance.gov/f/ ./cfgov/f/ This downloads all remote images (and image renditions) referenced in the database, retrieving them from the specified URL and storing them in the specified local directory.","title":"Sync local image storage (optional)"},{"location":"installation/#install-gnu-gettext-for-django-translation-support-optional","text":"In order to generate Django translations as documented here , you'll need to install the GNU gettext library. On macOS, GNU gettext is available via Homebrew: brew install gettext but it gets installed as \"keg-only\" due to conflicts with the default installation of BSD gettext. This means that GNU gettext won't be loaded in your PATH by default. To fix this, you can run brew link --force gettext to force its installation, or see brew info gettext for an alternate solution. If installed locally, you should be able to run this command successfully: $ gettext --version GNU gettext is also required to run our translation-related unit tests locally.","title":"Install GNU gettext for Django translation support (optional)"},{"location":"javascript-unit-tests/","text":"JavaScript Unit Tests \u00b6 This page provides instructions for writing and running JavaScript (JS) unit tests in consumerfinance.gov. Jest is the framework we use for writing and running JavaScript unit tests. If you\u2019re not familiar with it, it would be a good idea to peruse their docs before diving in here. Table of contents \u00b6 Running unit tests Running a single test file Run a directory of unit tests Run all unit tests Where to find tests Test-driven development Setting up tests New test file from sample Folder structure (where to put your JavaScript and tests) First test run File structure (basic layout of a test file) Providing test data Common test patterns Testing a basic function Testing DOM manipulation Testing browser state Testing user interaction Running unit tests \u00b6 Run a single test file \u00b6 To run a single test file, use the --specs flag to specify a file path: gulp test:unit --specs=js/organisms/Footer-spec.js The above command tests the code at cfgov/unprocessed/js/organisms/Footer.js . Run a directory of unit tests \u00b6 A directory of unit tests can be run with: gulp test:unit --specs=js/molecules/ Run all unit tests \u00b6 To run all of the unit tests: gulp test:unit Where to find tests \u00b6 The following links list out the main directories containing tests (as of January 2019, this page\u2019s initial publication date). All unit tests Tests for individual apps Tests for regular modules Tests for molecules Tests for organisms Test-driven development \u00b6 We recommend using test-driven development (TDD) when coding new JavaScript. The general concept is to start by writing your test first , with the expected behavior and functionality well-described, then you write the code that makes the test pass. A good pithy summary is: Write only enough of a unit test to fail. Write only enough production code to make the failing unit test pass. Then repeat that process until you have written all of the code you need. Read this primer on test-driven development to learn more about how it differs from the typical approach to programming and unit tests. Setting up tests \u00b6 New test file from sample \u00b6 For this guide, we\u2019ll use very basic sample code files to illustrate how to use the test framework in consumerfinance.gov and how to test very common code patterns. Another common approach is to look for existing tests that are testing something similar to what you are writing now. Feel free to do so and copy from an existing module and its tests instead of copying the sample files referenced below. For links to existing tests, refer back to the \u201cWhere to find tests\u201d section . Now, let\u2019s begin! Let\u2019s make a new unit test fail, then we will make it pass, following the principles of TDD. Copy the sample test file to a new location by running this command from the root of consumerfinance.gov: cp docs/samples/sample-spec.js test/unit_tests/js/modules/ Copy the sample module file to a new location by running this command: cp docs/samples/sample.js cfgov/unprocessed/js/modules/ Test file names should always match what they are testing, with the addition of a -spec suffix. Folder structure (where to put your JavaScript and tests) \u00b6 JavaScript unit test files belong in the test/unit_tests/ directory. The folder structure of the test files mirrors the structure of the project JavaScript in cfgov/unprocessed/js/ . When considering exactly where to place JavaScript in these directories, it might be helpful to review the documentation about atomic components in consumerfinance.gov . JavaScript corresponding to atomic elements should go into the appropriate subfolder for the type of element being implemented. In our case, sample.js and sample-spec.js don\u2019t relate to atomic elements, so they can be placed into the uncategorized modules subfolders: cfgov/unprocessed/js/modules and test/unit_tests/modules , respectively. Child apps If you\u2019re working on something in a child app, put it in test/unit_test/appname/js/ . Otherwise, if you\u2019re working on something that belongs to consumerfinance.gov generally, it should go in the corresponding folder under test/unit_test/js/ . First test run \u00b6 Now that you have your sample JS and test files in the right places, let\u2019s try running them and see what happens! I\u2019ll refer to sample-spec.js and sample.js in the instructions below, but you should work in your own new test file and JavaScript file to save and commit your changes. Edit line 6 of your spec file and remove the call to the .skip method. The line should now read: it( 'should return a string with expected value', () => { \u2026 } ); Run your sample test using gulp test:unit --specs=js/modules/sample-spec.js (substituting your own filename). You should see output like this: The test should fail \u2013 this is expected. Remember, when doing TDD, we want to write our test to fail first, then write the corresponding JavaScript that will make the test pass. Make the test pass by changing your script\u2019s line 7 ( see sample.js ) to the following: return 'Shredder'; Run the test again to confirm the test now passes. You should see output like this: Doesn\u2019t it feel good? Refer back to the \u201cRunning unit tests\u201d section for additional commands to run tests. File structure (basic layout of a test file) \u00b6 In order to make the sample-spec.js more meaningful to your own use case, you\u2019ll need to know how to structure a unit test using Jest methods. Let\u2019s take a look at the structure of our very basic sample test file. Loading dependencies \u00b6 Line 1 of any spec file will use an import statement to include the JavaScript file that you are testing. Additional dependencies should be added in the same manner. import sample from '../../../../cfgov/unprocessed/js/modules/sample.js'; Some test files use const declarations to require scripts instead of import , because those files were written before import was available. We prefer to use import because it allows for tree shaking in Webpack , meaning if two modules are importing the same module it should only be included in the bundle once, whereas with require it would be included twice. A consequence is that variables can\u2019t be used in the import path, as they prevent Webpack from figuring out which modules are duplicates. For example, this snippet shows how a require statement should be converted to an import statement, but without including the BASE_JS_PATH variable in the file path: // This works, but could duplicate footer-button.js, if other files also require it. const FooterButton = require( BASE_JS_PATH + 'modules/footer-button.js' ); // This doesn't work and the build will fail. import * as FooterButton from BASE_JS_PATH + 'modules/footer-button.js'; // This is ugly, but it works and supports tree shaking. import * as FooterButton from '../../../../cfgov/unprocessed/js/modules/footer-button.js'; import also provides a benefit in that you can choose specific parts of a module so that you only import the dependencies you need. For testing purposes, we will typically import the whole module to make sure we have full test coverage. Read the import reference guide on MDN on how to implement import for different use cases. The describe function \u00b6 In Jest (whose syntax is based on Jasmine), describe blocks serve as organizational structures that you can use to outline the methods you need in your JS module. The root describe method is where we put the name of the JavaScript module we are testing. For the sample, the name of our module is sample , so we set this up on line 4 of sample-spec.js : describe( 'sample', () => { \u2026 } ); This module name will appear in your test output in the console when the test is running: More complex tests will have additional describe blocks \u2013 children of the root describe block \u2013 that should correspond to a particular method in the module. For example, if we want to add more functionality to our sample JS, we could start by writing these tests in sample-spec.js : describe( 'sample', () => { describe( 'gimmeString()', () => { it( 'should return a string with expected value', () => { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () => { it( 'should return an object with expected value', () => { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); (We\u2019ll talk more about writing the individual tests in the next section.) And then we would create the gimmeString and gimmeObject methods in our sample.js file. Another example is breakpoint-state-spec.js , which tests several methods, including getBreakpointState and viewportIsIn , on the module breakpoint-state . When using TDD, you may prefer to add describe blocks later, during the refactor stage of writing code. Individual tests \u00b6 Within a describe block, individual tests are encapsulated by it methods, which is an alias of Jest\u2019s test method . Each test must include one or more assertions (usually only one) that confirm that the result of executing some code is what you expected. These are called \u201cmatchers\u201d in Jest parlance, and they all follow this format: expect( someValue ).someKindOfComparisonWith( someOtherValue ); For example, let\u2019s take another look at the sample tests we wrote above: describe( 'sample', () => { describe( 'gimmeString()', () => { it( 'should return a string with expected value', () => { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () => { it( 'should return an object with expected value', () => { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); In these tests, we check ( expect ) that the string returned by sample.gimmeString() and the object returned by sample.gimmeObject() match ( toBe ) the sampleString and sampleObject that we defined in the tests. There are many kinds of assertions you can use besides the simple equality comparison of toBe . See the Jest Using Matchers guide for a primer on them and the Jest expect API docs for the full list of its matchers. Providing test data \u00b6 The first principle of test data for unit tests is to keep test data as simple as possible \u2013 use the minimum needed to test the code. Direct definition of test data \u00b6 The simplest way to set up test data is to declare it as variables within each test, e.g., the tests in strings-spec.js . This can include HTML markup for DOM manipulation tests, if each test requires different markup. Setup and teardown methods \u00b6 If you will need to leverage the same test data across different tests, Jest has setup and teardown methods, such as beforeEach and afterEach , or beforeAll and afterAll , which can be used to performing actions that are needed before and after running all tests or each test in a suite. For example, the tests in Analytics-spec.js use both beforeAll and beforeEach inside the root describe block to do a variable definition for all tests at the beginning of the suite and reset the dataLayer before each test, respectively. Check out the Jest documentation on \u201cSetup and teardown\u201d methods. A common structure when the DOM is involved is to create a constant representing an HTML snippet to test, then \u2013 in a beforeEach or beforeAll (depending on whether the tests modify the markup or not) \u2013 set document.body.innerHTML to that snippet. Use beforeAll to attach HTML markup that is unaffected by the tests, e.g., the tests in footer-button-spec.js . Use beforeEach to reset manipulated markup between tests, e.g., the tests in Notification-spec.js . See \u201cTesting DOM manipulation\u201d in the \u201cCommon test patterns\u201d section of this page for a more in-depth discussion of this scenario. Common test patterns \u00b6 Testing a basic function \u00b6 Testing simple functions is pretty straightforward. Each function in a module should have tests set up as a child describe within the module\u2019s own describe . Then, write a number of it statements in prose that describe how the function should respond to various kinds of input. Inside each it , invoke the function with the input described in the it statement and use expect to check that you receive the desired result. Here is a simple example from our array helpers module ( cfgov/unprocessed/js/modules/util/array-helpers.js ): function indexOfObject( array, key, val ) { let match = -1; if ( !array.length > 0 ) { return match; } array.forEach( function( item, index ) { if ( item[key] === val ) { match = index; } } ); return match; } Tests for that function, from test/unit_tests/js/modules/util/array-helpers-spec.js : describe( 'indexOfObject()', () => { it( 'should return -1 if the array is empty', () => { array = []; index = arrayHelpers.indexOfObject( array, 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return -1 if there is no match', () => { array = [ { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return the matched index', () => { array = [ { value: 'foo' }, { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( 0 ); } ); } ); Testing DOM manipulation \u00b6 Jest , the JavaScript testing framework we use, includes jsdom , which simulates a DOM environment as if you were in the browser. This means that we can call any DOM API in our test code and observe it in the same way as we do in the module code itself, which acts on the browser\u2019s DOM. As an example, let\u2019s look at our Notification component. The Notification component uses a common set of markup with different classes and SVG icon code to style it as a particular kind of notification (success, warning, etc.). In the component JS , we have this function that sets the type of a notification before displaying it: function _setType( type ) { // If type hasn't changed, return. if ( _currentType === type ) { return this; } // Remove existing type class const classList = _dom.classList; classList.remove( `${ BASE_CLASS }__${ _currentType }` ); if ( type === SUCCESS || type === WARNING || type === ERROR ) { // Add new type class and update the value of _currentType classList.add( `${ BASE_CLASS }__${ type }` ); _currentType = type; // Replace <svg> element with contents of type_ICON const currentIcon = _dom.querySelector( '.cf-icon-svg' ); const newIconSetup = document.createElement( 'div' ); newIconSetup.innerHTML = ICON[type]; const newIcon = newIconSetup.firstChild; _dom.replaceChild( newIcon, currentIcon ); } else { throw new Error( `${ type } is not a supported notification type!` ); } return this; } This function would be invoked by an instance of the Notification class. _dom is the DOM node for the Notification. As you can see from the code comments above, it has a few different steps that modify the DOM node. Now let\u2019s look at the tests. Here are the first 22 lines of the spec file that tests this component: import Notification from '../../../../cfgov/unprocessed/js/molecules/Notification'; const BASE_CLASS = 'm-notification'; const HTML_SNIPPET = ` <div class=\"m-notification\"> <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1000 1200\" class=\"cf-icon-svg\"></svg> <div class=\"m-notification_content\"> <div class=\"h4 m-notification_message\">Notification content</div> </div> </div> `; describe( 'Notification', () => { let notificationElem; let notification; let thisNotification; beforeEach( () => { document.body.innerHTML = HTML_SNIPPET; notificationElem = document.querySelector( `.${ BASE_CLASS }` ); notification = new Notification( notificationElem, BASE_CLASS, {} ); } ); \u2026 } ); The main things to note here at the beginning of the file are the addition of the HTML_SNIPPET definition, containing the markup we will used for testing as it would be rendered for this component, and the beforeEach function that (1) uses jsdom to add that snippet to the test environment and assigns the component node to the notificationElem variable, and (2) creates a new instance of the Notification class. A word about HTML_SNIPPET s Right now it\u2019s possible to update a component\u2019s Jinja template, forget to update the corresponding JavaScript, and the unit tests would still pass, because they're using their own HTML_SNIPPET . It would be preferable if we had a canonical component markup template that is pulled in by the application, the unit tests, and the docs. We haven\u2019t yet figured out how to do this, since our component templates contain Jinja tags that the tests would have to reconcile into a complete, finished chunk of markup. For now, just be aware of this when editing a Wagtail component that includes JavaScript . Further down, here are some of the tests that cover the _setType function (by way of the setTypeAndContent function that wraps both _setType and _setContent ): describe( 'setTypeAndContent()', () => { it( 'should update the notification type for the success state', () => { notification.init(); notification.setTypeAndContent( notification.SUCCESS, '' ); expect( notificationElem.classList ).toContain( 'm-notification__success' ); } ); it( 'should update the notification type for the warning state', () => { notification.init(); notification.setTypeAndContent( notification.WARNING, '' ); expect( notificationElem.classList ).toContain( 'm-notification__warning' ); } ); \u2026 } ); This part mostly works like testing any other function. The notable distinction here is that the test invokes the function using the DOM nodes and class set up in beforeEach . Testing browser state \u00b6 Another common thing to test is code that interacts with the state of the browser itself, e.g., fragment identifiers, query strings, or other things in the URL; the window object; session storage; page history; etc. One way of doing this is to create a spy (a special kind of mocked function) that watches for browser API calls to be made a certain number of times or with a specific payload. One example is found in the tests for our full-table-row-linking code . In the module code ( o-table-row-links.js ), if an event listener detects a click anywhere on one of these special table rows, it invokes window.location to send the browser to the href of the first link in that row: window.location.assign( target.querySelector( 'a' ).getAttribute( 'href' ) ); To test this, in the aforementioned o-table-row-links-spec.js file, we first set up a standard Jest mock for window.location.assign , and then create our spy to watch it: describe( 'o-table-row-links', () => { beforeEach( () => { window.location.assign = jest.fn(); locationSpy = jest.spyOn( window.location, 'assign' ); \u2026 } ); \u2026 } ); A little further down (after finishing the DOM setup and initializing the module we\u2019re testing), we have three tests that simulate clicks and then assert things that the spy can answer for us: whether it was called with a particular location parameter, and that it was called a specific number of times (zero). it( 'should navigate to new location when link row cell clicked', () => { simulateEvent( 'click', linkRowCellDom ); expect( locationSpy ).toBeCalledWith( 'https://www.example.com' ); } ); it( 'should not set window location when link is clicked', () => { simulateEvent( 'click', linkDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } ); it( 'should not navigate to new location when non link row cell clicked', () => { simulateEvent( 'click', nonLinkRowCellDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } ); Testing user interaction \u00b6 Testing user interaction with simulated pointer events, keystrokes, or form submissions is best handled via browser tests, not unit tests. User interaction in a unit test could falsely pass if the component wasn\u2019t visible on the page, for instance. Read more about how we run browser tests with Cypress.","title":"JavaScript Unit Testing"},{"location":"javascript-unit-tests/#javascript-unit-tests","text":"This page provides instructions for writing and running JavaScript (JS) unit tests in consumerfinance.gov. Jest is the framework we use for writing and running JavaScript unit tests. If you\u2019re not familiar with it, it would be a good idea to peruse their docs before diving in here.","title":"JavaScript Unit Tests"},{"location":"javascript-unit-tests/#table-of-contents","text":"Running unit tests Running a single test file Run a directory of unit tests Run all unit tests Where to find tests Test-driven development Setting up tests New test file from sample Folder structure (where to put your JavaScript and tests) First test run File structure (basic layout of a test file) Providing test data Common test patterns Testing a basic function Testing DOM manipulation Testing browser state Testing user interaction","title":"Table of contents"},{"location":"javascript-unit-tests/#running-unit-tests","text":"","title":"Running unit tests"},{"location":"javascript-unit-tests/#run-a-single-test-file","text":"To run a single test file, use the --specs flag to specify a file path: gulp test:unit --specs=js/organisms/Footer-spec.js The above command tests the code at cfgov/unprocessed/js/organisms/Footer.js .","title":"Run a single test file"},{"location":"javascript-unit-tests/#run-a-directory-of-unit-tests","text":"A directory of unit tests can be run with: gulp test:unit --specs=js/molecules/","title":"Run a directory of unit tests"},{"location":"javascript-unit-tests/#run-all-unit-tests","text":"To run all of the unit tests: gulp test:unit","title":"Run all unit tests"},{"location":"javascript-unit-tests/#where-to-find-tests","text":"The following links list out the main directories containing tests (as of January 2019, this page\u2019s initial publication date). All unit tests Tests for individual apps Tests for regular modules Tests for molecules Tests for organisms","title":"Where to find tests"},{"location":"javascript-unit-tests/#test-driven-development","text":"We recommend using test-driven development (TDD) when coding new JavaScript. The general concept is to start by writing your test first , with the expected behavior and functionality well-described, then you write the code that makes the test pass. A good pithy summary is: Write only enough of a unit test to fail. Write only enough production code to make the failing unit test pass. Then repeat that process until you have written all of the code you need. Read this primer on test-driven development to learn more about how it differs from the typical approach to programming and unit tests.","title":"Test-driven development"},{"location":"javascript-unit-tests/#setting-up-tests","text":"","title":"Setting up tests"},{"location":"javascript-unit-tests/#new-test-file-from-sample","text":"For this guide, we\u2019ll use very basic sample code files to illustrate how to use the test framework in consumerfinance.gov and how to test very common code patterns. Another common approach is to look for existing tests that are testing something similar to what you are writing now. Feel free to do so and copy from an existing module and its tests instead of copying the sample files referenced below. For links to existing tests, refer back to the \u201cWhere to find tests\u201d section . Now, let\u2019s begin! Let\u2019s make a new unit test fail, then we will make it pass, following the principles of TDD. Copy the sample test file to a new location by running this command from the root of consumerfinance.gov: cp docs/samples/sample-spec.js test/unit_tests/js/modules/ Copy the sample module file to a new location by running this command: cp docs/samples/sample.js cfgov/unprocessed/js/modules/ Test file names should always match what they are testing, with the addition of a -spec suffix.","title":"New test file from sample"},{"location":"javascript-unit-tests/#folder-structure-where-to-put-your-javascript-and-tests","text":"JavaScript unit test files belong in the test/unit_tests/ directory. The folder structure of the test files mirrors the structure of the project JavaScript in cfgov/unprocessed/js/ . When considering exactly where to place JavaScript in these directories, it might be helpful to review the documentation about atomic components in consumerfinance.gov . JavaScript corresponding to atomic elements should go into the appropriate subfolder for the type of element being implemented. In our case, sample.js and sample-spec.js don\u2019t relate to atomic elements, so they can be placed into the uncategorized modules subfolders: cfgov/unprocessed/js/modules and test/unit_tests/modules , respectively. Child apps If you\u2019re working on something in a child app, put it in test/unit_test/appname/js/ . Otherwise, if you\u2019re working on something that belongs to consumerfinance.gov generally, it should go in the corresponding folder under test/unit_test/js/ .","title":"Folder structure (where to put your JavaScript and tests)"},{"location":"javascript-unit-tests/#first-test-run","text":"Now that you have your sample JS and test files in the right places, let\u2019s try running them and see what happens! I\u2019ll refer to sample-spec.js and sample.js in the instructions below, but you should work in your own new test file and JavaScript file to save and commit your changes. Edit line 6 of your spec file and remove the call to the .skip method. The line should now read: it( 'should return a string with expected value', () => { \u2026 } ); Run your sample test using gulp test:unit --specs=js/modules/sample-spec.js (substituting your own filename). You should see output like this: The test should fail \u2013 this is expected. Remember, when doing TDD, we want to write our test to fail first, then write the corresponding JavaScript that will make the test pass. Make the test pass by changing your script\u2019s line 7 ( see sample.js ) to the following: return 'Shredder'; Run the test again to confirm the test now passes. You should see output like this: Doesn\u2019t it feel good? Refer back to the \u201cRunning unit tests\u201d section for additional commands to run tests.","title":"First test run"},{"location":"javascript-unit-tests/#file-structure-basic-layout-of-a-test-file","text":"In order to make the sample-spec.js more meaningful to your own use case, you\u2019ll need to know how to structure a unit test using Jest methods. Let\u2019s take a look at the structure of our very basic sample test file.","title":"File structure (basic layout of a test file)"},{"location":"javascript-unit-tests/#loading-dependencies","text":"Line 1 of any spec file will use an import statement to include the JavaScript file that you are testing. Additional dependencies should be added in the same manner. import sample from '../../../../cfgov/unprocessed/js/modules/sample.js'; Some test files use const declarations to require scripts instead of import , because those files were written before import was available. We prefer to use import because it allows for tree shaking in Webpack , meaning if two modules are importing the same module it should only be included in the bundle once, whereas with require it would be included twice. A consequence is that variables can\u2019t be used in the import path, as they prevent Webpack from figuring out which modules are duplicates. For example, this snippet shows how a require statement should be converted to an import statement, but without including the BASE_JS_PATH variable in the file path: // This works, but could duplicate footer-button.js, if other files also require it. const FooterButton = require( BASE_JS_PATH + 'modules/footer-button.js' ); // This doesn't work and the build will fail. import * as FooterButton from BASE_JS_PATH + 'modules/footer-button.js'; // This is ugly, but it works and supports tree shaking. import * as FooterButton from '../../../../cfgov/unprocessed/js/modules/footer-button.js'; import also provides a benefit in that you can choose specific parts of a module so that you only import the dependencies you need. For testing purposes, we will typically import the whole module to make sure we have full test coverage. Read the import reference guide on MDN on how to implement import for different use cases.","title":"Loading dependencies"},{"location":"javascript-unit-tests/#the-describe-function","text":"In Jest (whose syntax is based on Jasmine), describe blocks serve as organizational structures that you can use to outline the methods you need in your JS module. The root describe method is where we put the name of the JavaScript module we are testing. For the sample, the name of our module is sample , so we set this up on line 4 of sample-spec.js : describe( 'sample', () => { \u2026 } ); This module name will appear in your test output in the console when the test is running: More complex tests will have additional describe blocks \u2013 children of the root describe block \u2013 that should correspond to a particular method in the module. For example, if we want to add more functionality to our sample JS, we could start by writing these tests in sample-spec.js : describe( 'sample', () => { describe( 'gimmeString()', () => { it( 'should return a string with expected value', () => { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () => { it( 'should return an object with expected value', () => { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); (We\u2019ll talk more about writing the individual tests in the next section.) And then we would create the gimmeString and gimmeObject methods in our sample.js file. Another example is breakpoint-state-spec.js , which tests several methods, including getBreakpointState and viewportIsIn , on the module breakpoint-state . When using TDD, you may prefer to add describe blocks later, during the refactor stage of writing code.","title":"The describe function"},{"location":"javascript-unit-tests/#individual-tests","text":"Within a describe block, individual tests are encapsulated by it methods, which is an alias of Jest\u2019s test method . Each test must include one or more assertions (usually only one) that confirm that the result of executing some code is what you expected. These are called \u201cmatchers\u201d in Jest parlance, and they all follow this format: expect( someValue ).someKindOfComparisonWith( someOtherValue ); For example, let\u2019s take another look at the sample tests we wrote above: describe( 'sample', () => { describe( 'gimmeString()', () => { it( 'should return a string with expected value', () => { const sampleString = 'Shredder'; expect( sample.gimmeString() ).toBe( sampleString ); } ); } ); describe( 'gimmeObject()', () => { it( 'should return an object with expected value', () => { const sampleObject = { image: 'https://vignette.wikia.nocookie.net/tmnt/images/0/00/Krangnobody.png', caption: 'Krang portrait' }; expect( sample.gimmeObject() ).toBe( sampleObject ); } ); } ); } ); In these tests, we check ( expect ) that the string returned by sample.gimmeString() and the object returned by sample.gimmeObject() match ( toBe ) the sampleString and sampleObject that we defined in the tests. There are many kinds of assertions you can use besides the simple equality comparison of toBe . See the Jest Using Matchers guide for a primer on them and the Jest expect API docs for the full list of its matchers.","title":"Individual tests"},{"location":"javascript-unit-tests/#providing-test-data","text":"The first principle of test data for unit tests is to keep test data as simple as possible \u2013 use the minimum needed to test the code.","title":"Providing test data"},{"location":"javascript-unit-tests/#direct-definition-of-test-data","text":"The simplest way to set up test data is to declare it as variables within each test, e.g., the tests in strings-spec.js . This can include HTML markup for DOM manipulation tests, if each test requires different markup.","title":"Direct definition of test data"},{"location":"javascript-unit-tests/#setup-and-teardown-methods","text":"If you will need to leverage the same test data across different tests, Jest has setup and teardown methods, such as beforeEach and afterEach , or beforeAll and afterAll , which can be used to performing actions that are needed before and after running all tests or each test in a suite. For example, the tests in Analytics-spec.js use both beforeAll and beforeEach inside the root describe block to do a variable definition for all tests at the beginning of the suite and reset the dataLayer before each test, respectively. Check out the Jest documentation on \u201cSetup and teardown\u201d methods. A common structure when the DOM is involved is to create a constant representing an HTML snippet to test, then \u2013 in a beforeEach or beforeAll (depending on whether the tests modify the markup or not) \u2013 set document.body.innerHTML to that snippet. Use beforeAll to attach HTML markup that is unaffected by the tests, e.g., the tests in footer-button-spec.js . Use beforeEach to reset manipulated markup between tests, e.g., the tests in Notification-spec.js . See \u201cTesting DOM manipulation\u201d in the \u201cCommon test patterns\u201d section of this page for a more in-depth discussion of this scenario.","title":"Setup and teardown methods"},{"location":"javascript-unit-tests/#common-test-patterns","text":"","title":"Common test patterns"},{"location":"javascript-unit-tests/#testing-a-basic-function","text":"Testing simple functions is pretty straightforward. Each function in a module should have tests set up as a child describe within the module\u2019s own describe . Then, write a number of it statements in prose that describe how the function should respond to various kinds of input. Inside each it , invoke the function with the input described in the it statement and use expect to check that you receive the desired result. Here is a simple example from our array helpers module ( cfgov/unprocessed/js/modules/util/array-helpers.js ): function indexOfObject( array, key, val ) { let match = -1; if ( !array.length > 0 ) { return match; } array.forEach( function( item, index ) { if ( item[key] === val ) { match = index; } } ); return match; } Tests for that function, from test/unit_tests/js/modules/util/array-helpers-spec.js : describe( 'indexOfObject()', () => { it( 'should return -1 if the array is empty', () => { array = []; index = arrayHelpers.indexOfObject( array, 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return -1 if there is no match', () => { array = [ { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( -1 ); } ); it( 'should return the matched index', () => { array = [ { value: 'foo' }, { value: 'bar' }, { value: 'baz' } ]; index = arrayHelpers.indexOfObject( array, 'value', 'foo' ); expect( index ).toBe( 0 ); } ); } );","title":"Testing a basic function"},{"location":"javascript-unit-tests/#testing-dom-manipulation","text":"Jest , the JavaScript testing framework we use, includes jsdom , which simulates a DOM environment as if you were in the browser. This means that we can call any DOM API in our test code and observe it in the same way as we do in the module code itself, which acts on the browser\u2019s DOM. As an example, let\u2019s look at our Notification component. The Notification component uses a common set of markup with different classes and SVG icon code to style it as a particular kind of notification (success, warning, etc.). In the component JS , we have this function that sets the type of a notification before displaying it: function _setType( type ) { // If type hasn't changed, return. if ( _currentType === type ) { return this; } // Remove existing type class const classList = _dom.classList; classList.remove( `${ BASE_CLASS }__${ _currentType }` ); if ( type === SUCCESS || type === WARNING || type === ERROR ) { // Add new type class and update the value of _currentType classList.add( `${ BASE_CLASS }__${ type }` ); _currentType = type; // Replace <svg> element with contents of type_ICON const currentIcon = _dom.querySelector( '.cf-icon-svg' ); const newIconSetup = document.createElement( 'div' ); newIconSetup.innerHTML = ICON[type]; const newIcon = newIconSetup.firstChild; _dom.replaceChild( newIcon, currentIcon ); } else { throw new Error( `${ type } is not a supported notification type!` ); } return this; } This function would be invoked by an instance of the Notification class. _dom is the DOM node for the Notification. As you can see from the code comments above, it has a few different steps that modify the DOM node. Now let\u2019s look at the tests. Here are the first 22 lines of the spec file that tests this component: import Notification from '../../../../cfgov/unprocessed/js/molecules/Notification'; const BASE_CLASS = 'm-notification'; const HTML_SNIPPET = ` <div class=\"m-notification\"> <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 1000 1200\" class=\"cf-icon-svg\"></svg> <div class=\"m-notification_content\"> <div class=\"h4 m-notification_message\">Notification content</div> </div> </div> `; describe( 'Notification', () => { let notificationElem; let notification; let thisNotification; beforeEach( () => { document.body.innerHTML = HTML_SNIPPET; notificationElem = document.querySelector( `.${ BASE_CLASS }` ); notification = new Notification( notificationElem, BASE_CLASS, {} ); } ); \u2026 } ); The main things to note here at the beginning of the file are the addition of the HTML_SNIPPET definition, containing the markup we will used for testing as it would be rendered for this component, and the beforeEach function that (1) uses jsdom to add that snippet to the test environment and assigns the component node to the notificationElem variable, and (2) creates a new instance of the Notification class. A word about HTML_SNIPPET s Right now it\u2019s possible to update a component\u2019s Jinja template, forget to update the corresponding JavaScript, and the unit tests would still pass, because they're using their own HTML_SNIPPET . It would be preferable if we had a canonical component markup template that is pulled in by the application, the unit tests, and the docs. We haven\u2019t yet figured out how to do this, since our component templates contain Jinja tags that the tests would have to reconcile into a complete, finished chunk of markup. For now, just be aware of this when editing a Wagtail component that includes JavaScript . Further down, here are some of the tests that cover the _setType function (by way of the setTypeAndContent function that wraps both _setType and _setContent ): describe( 'setTypeAndContent()', () => { it( 'should update the notification type for the success state', () => { notification.init(); notification.setTypeAndContent( notification.SUCCESS, '' ); expect( notificationElem.classList ).toContain( 'm-notification__success' ); } ); it( 'should update the notification type for the warning state', () => { notification.init(); notification.setTypeAndContent( notification.WARNING, '' ); expect( notificationElem.classList ).toContain( 'm-notification__warning' ); } ); \u2026 } ); This part mostly works like testing any other function. The notable distinction here is that the test invokes the function using the DOM nodes and class set up in beforeEach .","title":"Testing DOM manipulation"},{"location":"javascript-unit-tests/#testing-browser-state","text":"Another common thing to test is code that interacts with the state of the browser itself, e.g., fragment identifiers, query strings, or other things in the URL; the window object; session storage; page history; etc. One way of doing this is to create a spy (a special kind of mocked function) that watches for browser API calls to be made a certain number of times or with a specific payload. One example is found in the tests for our full-table-row-linking code . In the module code ( o-table-row-links.js ), if an event listener detects a click anywhere on one of these special table rows, it invokes window.location to send the browser to the href of the first link in that row: window.location.assign( target.querySelector( 'a' ).getAttribute( 'href' ) ); To test this, in the aforementioned o-table-row-links-spec.js file, we first set up a standard Jest mock for window.location.assign , and then create our spy to watch it: describe( 'o-table-row-links', () => { beforeEach( () => { window.location.assign = jest.fn(); locationSpy = jest.spyOn( window.location, 'assign' ); \u2026 } ); \u2026 } ); A little further down (after finishing the DOM setup and initializing the module we\u2019re testing), we have three tests that simulate clicks and then assert things that the spy can answer for us: whether it was called with a particular location parameter, and that it was called a specific number of times (zero). it( 'should navigate to new location when link row cell clicked', () => { simulateEvent( 'click', linkRowCellDom ); expect( locationSpy ).toBeCalledWith( 'https://www.example.com' ); } ); it( 'should not set window location when link is clicked', () => { simulateEvent( 'click', linkDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } ); it( 'should not navigate to new location when non link row cell clicked', () => { simulateEvent( 'click', nonLinkRowCellDom ); expect( locationSpy ).toHaveBeenCalledTimes( 0 ); } );","title":"Testing browser state"},{"location":"javascript-unit-tests/#testing-user-interaction","text":"Testing user interaction with simulated pointer events, keystrokes, or form submissions is best handled via browser tests, not unit tests. User interaction in a unit test could falsely pass if the component wasn\u2019t visible on the page, for instance. Read more about how we run browser tests with Cypress.","title":"Testing user interaction"},{"location":"migrations/","text":"Django and Wagtail Migrations \u00b6 Adding or changing fields on Django models, Wagtail page models (which are a particular kind of Django model), or StreamField block classes will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration . Table of contents \u00b6 Reference material Do I need to create a migration? Schema migrations Data migrations Wagtail-specific consideration Utility functions Recreating migrations Reference material \u00b6 The following links may be useful for setting context or diving deeper into the concepts presented throughout this page: Django migrations documentation Django data migrations documentation Wagtail Streamfield migrations documentation Do I need to create a migration? \u00b6 A new Django migration is required for most, but not all, changes that you make to the definitions of Django model classes. Even experienced Django developers may find it unintuitive to determine which changes will require a migration. Example model changes that require a migration: Adding, removing, or renaming a model field Changing a model field definition in a way that impacts the database schema (for example, changing the size of a CharField ) Changing a model field definition in a way that does not impact the database schema (for example, changing the field's help_text ) Example model changes that do not require a migration: Adding, removing, renaming, or modifying a model class method Modifying a model class manager The best way to tell if your changes require a migration is to ask Django to determine that for you. Django's makemigrations management command can be used for this purpose: ./cfgov/manage.py makemigrations --dry-run If you haven't made any changes to your local source code that would necessitate the creation of a new migration, this command will print No changes detected . Otherwise, if you have made changes that require a migration, Django will print information about the migration that would need to be created: Migrations for 'v1': cfgov/v1/migrations/0154_auto_20190412_1008.py - Alter field alt on cfgovimage Running with the --dry-run flag won't actually create any migration files on disk. See below for more information on how to do this, including how to give your migrations a more descriptive name. Schema migrations \u00b6 Any time you add or change a field on a Django model, Wagtail page model (which are a particular kind of Django model), or StreamField block class, a Django schema migration will be required. This includes changes as small as modifying the help_text string. To automatically generate a schema migration, run the following, editing it to give your migration a name that briefly describes the change(s) you're making: ./cfgov/manage.py makemigrations -n <description_of_changes> For examples of good migration names, look through some of our existing migration files . Note Some changes will generate multiple migration files. If you change a block that is used in pages defined in different sub-apps, you will see a migration file for each of those sub-apps. Migration numbering and conflicts \u00b6 When a migration file is generated, it will automatically be given a unique four-digit number at the beginning of its filename. These numbers are assigned in sequence, and they end up being a regular source of conflicts between pull requests that are in flight at the same time. If a PR with a migration gets merged between the time you create your migration and the time that your PR is ready for merging, you will have to update your branch as normal to be current with main and then re-create your migration. Also note that our back-end tests that run in GitHub Actions will fail if a required schema migration is missing or if migrations are in conflict with one another. Data migrations \u00b6 Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it. There is no automatic generation mechanism like there is for schema migrations. You must write the script by hand that automates the transfer of data from old fields to new fields. To generate an empty migration file for your data migration, run: ./cfgov/manage.py makemigrations --empty yourappname You can also copy the code below to get started with forward() and backward() functions to migrate your model's data: from django.db import migrations def forwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make forward changes to the object pass def backwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make backward changes to the object pass class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] The forwards() and backwards() functions are where any changes that need to happen to a model's data are made. Note While backwards migrations are necessary in external libraries that we create, we do not require them in consumerfinance.gov because we prefer not to rollback migrations that have already been applied. Wagtail-specific considerations \u00b6 Django data migrations with Wagtail can be challenging because programmatic editing of Wagtail pages is difficult , and pages have both revisions and StreamFields. This section describes ways we try to address these challenges in consumerfinance.gov. The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the contents of StreamFields. As described in the editing a component guide , it's a three-step process to modify a field without losing data: Create the new field with an automatic schema migration Use a handwritten data migration script to move data from the old field to the new field Delete the old field with an automatic schema migration We've written some utility functions in consumerfinance.gov that make writing data migrations for StreamFields easier. Using these utilities, a Django data migration that modifies a StreamField would use the following format: from django.db import migrations from v1.util.migrations import migrate_page_types_and_fields def forward_mapper(page_or_revision, data): # Manipulate the stream block data forwards return data def backward_mapper(page_or_revision, data): # Manipulate the stream block data backwards return data def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) def backwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, backward_mapper ) class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] field_name is the name of the StreamField on the Page model that contains the blocks to migrate. block_name is the name of the block within a StreamField that contains the data to be migrated. StreamBlocks can themselves also contain child blocks. The block name can be given as a list of block names that form the \"path\" to the block that needs to be migrated. For example : def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', ['parent_block', 'child_block']), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) In this example, a block with the name child_block that is inside a block named parent_block will be passed to the forward_mapper function. The data that gets passed to the forward_mapper or backward_mapper is a JSON-compatible Python dict that corresponds to the block's schema. Utility functions \u00b6 These functions, defined in v1.util.migrations , are used in the above data migration example. They reduce the amount of boilerplate required to work with Wagtail StreamField data in data migrations. migrate_page_types_and_fields(apps, page_types_and_fields, mapper) \u00b6 Migrate the fields of a Wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block_name') or ('app', 'PageType', 'field_name', ['parent_block_name', 'child_block_name']) . field_name is the name of the StreamField on the Page model. block_name is the name of the StreamBlock within the StreamField to migrate. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_stream_field() . migrate_stream_field(page_or_revision, field_name, block_path, mapper) \u00b6 Migrate all occurrences of the block name contained within the block_path list belonging to the page or revision using the mapper function. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_streamfield_data() . migrate_streamfield_data(page_or_revision, block_path, data, mapper) \u00b6 Migrate all occurrences of the block name contained within the block_path list within the data dict using the given mapper function. The mapper function should take page_or_revision and the stream block's value as a dict . get_streamfield_data(page_or_revision, field_name) \u00b6 Get the StreamField data for a given field name on a page or a revision. This function will return a list of dict -like objects containing the blocks within the given StreamField. set_streamfield_data(page_or_revision, field_name, data, commit=True) \u00b6 Set the StreamField data for a given field name on a page or a revision. If commit is True (default), save() is called on the page_or_revision object. data must be a list of dict -like objects containing the blocks within the given StreamField. Recreating migrations \u00b6 As described above , each time a Django model's definition changes it requires the generation of a new Django migration. Over time, the number of migrations in our apps can grow very large, slowing down testing and the migrate command. For this reason it may be desirable to periodically delete and recreate the migration files, so that instead of a series of files detailing every change over time we have a smaller set that just describes the current state of models in the code. Django does provide an automated squashing process for migrations, but this is often not optimal when migrations contain manual RunPython blocks that we don't necessarily care about keeping around. Instead, we delete all existing migration files and then run manage.py makemigrations to create new ones. This will generate the smallest number of migration files needed to describe the state of models in the code; typically one per app although sometimes multiple are needed due to app dependencies. This process does have these critical side effects: Databases that exist at some migration state before the one at the point of the recreation will no longer be able to be migrated to the current state, as the intermediate changes will have been lost. This means that those databases will need to be recreated. This also means that historical database archives will require a bit more work to resurrect; they'll need to first be migrated to the point just before the recreation, and then updated to code at or after that point. For example, say a database dump exists at a point where N migrations occur. At such time as N + 1 migrations occur, we decide to go through the recreation process. Now we have a new migration numbered N + 2 that represents the equivalent of all (1..N+1) migrations that it replaces. If you try to load and migrate the dump at point N, Django no longer has the code necessary to go from N->N+1 only -- it only has the ability to go from 0->N+2. To recover such a dump, you'll need to check out the code at the point before the recreation was done, migrate from N->N+1, and then check out latest and migrate forwards. Any open pull requests at the time of the recreation that reference or depend on some of the existing migrations will need to be modified to instead refer to the new migration files. Migrations can be recreated with this process: Remove all existing migration files: rm -f -v cfgov/*/migrations/0* Create new migration files from the state of model Python code: cfgov/manage.py makemigrations --noinput As it happens this creates new initial migrations ( 0001_initial ) for all apps, plus some subsequent migrations ( 0002_something ) for apps that depend on other apps (for example, ask_cfpb has its own initial migration and then some changes that rely on v1 ). Rename the created migration files so that they follow in sequence the migration files that used to exist. For example, if at the time of recreation there are 101 v1 migrations, the first new migration should be numbered 102. Manually alter all new migration files to indicate that they replace the old migration files. This involves adding lines to these files like: replaces = [('app_name', '0002_foo'), ('app_name', '0003_bar'), ...] This tells Django that these new files replace the old files, so that when migrations are run again, it doesn't need to do anything. Also manually update any new subsequent migration files so that they properly refer to each other. For example, if an app has two new migrations 102 and 103, the 103 file needs to properly depend on 102. To apply these new migration files to an existing database, you can simply run: cfgov/manage.py migrate --noinput You'll see that there are no changes to apply, as the new files should exactly describe the current model state in the same way that the old migrations did. See consumerfinance.gov#3770 for an example of when this was done.","title":"Django and Wagtail Migrations"},{"location":"migrations/#django-and-wagtail-migrations","text":"Adding or changing fields on Django models, Wagtail page models (which are a particular kind of Django model), or StreamField block classes will always require a new Django schema migration ; additionally, changing field names or types on an existing block will require a Django data migration .","title":"Django and Wagtail Migrations"},{"location":"migrations/#table-of-contents","text":"Reference material Do I need to create a migration? Schema migrations Data migrations Wagtail-specific consideration Utility functions Recreating migrations","title":"Table of contents"},{"location":"migrations/#reference-material","text":"The following links may be useful for setting context or diving deeper into the concepts presented throughout this page: Django migrations documentation Django data migrations documentation Wagtail Streamfield migrations documentation","title":"Reference material"},{"location":"migrations/#do-i-need-to-create-a-migration","text":"A new Django migration is required for most, but not all, changes that you make to the definitions of Django model classes. Even experienced Django developers may find it unintuitive to determine which changes will require a migration. Example model changes that require a migration: Adding, removing, or renaming a model field Changing a model field definition in a way that impacts the database schema (for example, changing the size of a CharField ) Changing a model field definition in a way that does not impact the database schema (for example, changing the field's help_text ) Example model changes that do not require a migration: Adding, removing, renaming, or modifying a model class method Modifying a model class manager The best way to tell if your changes require a migration is to ask Django to determine that for you. Django's makemigrations management command can be used for this purpose: ./cfgov/manage.py makemigrations --dry-run If you haven't made any changes to your local source code that would necessitate the creation of a new migration, this command will print No changes detected . Otherwise, if you have made changes that require a migration, Django will print information about the migration that would need to be created: Migrations for 'v1': cfgov/v1/migrations/0154_auto_20190412_1008.py - Alter field alt on cfgovimage Running with the --dry-run flag won't actually create any migration files on disk. See below for more information on how to do this, including how to give your migrations a more descriptive name.","title":"Do I need to create a migration?"},{"location":"migrations/#schema-migrations","text":"Any time you add or change a field on a Django model, Wagtail page model (which are a particular kind of Django model), or StreamField block class, a Django schema migration will be required. This includes changes as small as modifying the help_text string. To automatically generate a schema migration, run the following, editing it to give your migration a name that briefly describes the change(s) you're making: ./cfgov/manage.py makemigrations -n <description_of_changes> For examples of good migration names, look through some of our existing migration files . Note Some changes will generate multiple migration files. If you change a block that is used in pages defined in different sub-apps, you will see a migration file for each of those sub-apps.","title":"Schema migrations"},{"location":"migrations/#migration-numbering-and-conflicts","text":"When a migration file is generated, it will automatically be given a unique four-digit number at the beginning of its filename. These numbers are assigned in sequence, and they end up being a regular source of conflicts between pull requests that are in flight at the same time. If a PR with a migration gets merged between the time you create your migration and the time that your PR is ready for merging, you will have to update your branch as normal to be current with main and then re-create your migration. Also note that our back-end tests that run in GitHub Actions will fail if a required schema migration is missing or if migrations are in conflict with one another.","title":"Migration numbering and conflicts"},{"location":"migrations/#data-migrations","text":"Data migrations are required any time you: rename an existing field change the type of an existing field delete an existing field rename a block within a StreamField delete a block if you do not want to lose any data already stored in that field or block. In other words, if an existing field or block is changing, any data stored in that field or block has to be migrated to a different place, unless you're OK with jettisoning it. There is no automatic generation mechanism like there is for schema migrations. You must write the script by hand that automates the transfer of data from old fields to new fields. To generate an empty migration file for your data migration, run: ./cfgov/manage.py makemigrations --empty yourappname You can also copy the code below to get started with forward() and backward() functions to migrate your model's data: from django.db import migrations def forwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make forward changes to the object pass def backwards(apps, schema_editor): MyModel = apps.get_model('yourappname', 'MyModel') for obj in MyModel.objects.all(): # Make backward changes to the object pass class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] The forwards() and backwards() functions are where any changes that need to happen to a model's data are made. Note While backwards migrations are necessary in external libraries that we create, we do not require them in consumerfinance.gov because we prefer not to rollback migrations that have already been applied.","title":"Data migrations"},{"location":"migrations/#wagtail-specific-considerations","text":"Django data migrations with Wagtail can be challenging because programmatic editing of Wagtail pages is difficult , and pages have both revisions and StreamFields. This section describes ways we try to address these challenges in consumerfinance.gov. The data migration needs to modify both the existing Wagtail pages that correspond to the changed model and all revisions of that page. It also needs to be able to manipulate the contents of StreamFields. As described in the editing a component guide , it's a three-step process to modify a field without losing data: Create the new field with an automatic schema migration Use a handwritten data migration script to move data from the old field to the new field Delete the old field with an automatic schema migration We've written some utility functions in consumerfinance.gov that make writing data migrations for StreamFields easier. Using these utilities, a Django data migration that modifies a StreamField would use the following format: from django.db import migrations from v1.util.migrations import migrate_page_types_and_fields def forward_mapper(page_or_revision, data): # Manipulate the stream block data forwards return data def backward_mapper(page_or_revision, data): # Manipulate the stream block data backwards return data def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) def backwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', 'block_name'), ] migrate_page_types_and_fields( apps, page_types_and_fields, backward_mapper ) class Migration(migrations.Migration): dependencies = [] operations = [ migrations.RunPython(forwards, backwards), ] field_name is the name of the StreamField on the Page model that contains the blocks to migrate. block_name is the name of the block within a StreamField that contains the data to be migrated. StreamBlocks can themselves also contain child blocks. The block name can be given as a list of block names that form the \"path\" to the block that needs to be migrated. For example : def forwards(apps, schema_editor): page_types_and_fields = [ ('myapp', 'MyPage', 'field_name', ['parent_block', 'child_block']), ] migrate_page_types_and_fields( apps, page_types_and_fields, forward_mapper ) In this example, a block with the name child_block that is inside a block named parent_block will be passed to the forward_mapper function. The data that gets passed to the forward_mapper or backward_mapper is a JSON-compatible Python dict that corresponds to the block's schema.","title":"Wagtail-specific considerations"},{"location":"migrations/#utility-functions","text":"These functions, defined in v1.util.migrations , are used in the above data migration example. They reduce the amount of boilerplate required to work with Wagtail StreamField data in data migrations.","title":"Utility functions"},{"location":"migrations/#migrate_page_types_and_fieldsapps-page_types_and_fields-mapper","text":"Migrate the fields of a Wagtail page type using the given mapper function. page_types_and_fields should be a list of 4-tuples providing ('app', 'PageType', 'field_name', 'block_name') or ('app', 'PageType', 'field_name', ['parent_block_name', 'child_block_name']) . field_name is the name of the StreamField on the Page model. block_name is the name of the StreamBlock within the StreamField to migrate. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_stream_field() .","title":"migrate_page_types_and_fields(apps, page_types_and_fields, mapper)"},{"location":"migrations/#migrate_stream_fieldpage_or_revision-field_name-block_path-mapper","text":"Migrate all occurrences of the block name contained within the block_path list belonging to the page or revision using the mapper function. The mapper function should take page_or_revision and the stream block's value as a dict . This function calls migrate_streamfield_data() .","title":"migrate_stream_field(page_or_revision, field_name, block_path, mapper)"},{"location":"migrations/#migrate_streamfield_datapage_or_revision-block_path-data-mapper","text":"Migrate all occurrences of the block name contained within the block_path list within the data dict using the given mapper function. The mapper function should take page_or_revision and the stream block's value as a dict .","title":"migrate_streamfield_data(page_or_revision, block_path, data, mapper)"},{"location":"migrations/#get_streamfield_datapage_or_revision-field_name","text":"Get the StreamField data for a given field name on a page or a revision. This function will return a list of dict -like objects containing the blocks within the given StreamField.","title":"get_streamfield_data(page_or_revision, field_name)"},{"location":"migrations/#set_streamfield_datapage_or_revision-field_name-data-committrue","text":"Set the StreamField data for a given field name on a page or a revision. If commit is True (default), save() is called on the page_or_revision object. data must be a list of dict -like objects containing the blocks within the given StreamField.","title":"set_streamfield_data(page_or_revision, field_name, data, commit=True)"},{"location":"migrations/#recreating-migrations","text":"As described above , each time a Django model's definition changes it requires the generation of a new Django migration. Over time, the number of migrations in our apps can grow very large, slowing down testing and the migrate command. For this reason it may be desirable to periodically delete and recreate the migration files, so that instead of a series of files detailing every change over time we have a smaller set that just describes the current state of models in the code. Django does provide an automated squashing process for migrations, but this is often not optimal when migrations contain manual RunPython blocks that we don't necessarily care about keeping around. Instead, we delete all existing migration files and then run manage.py makemigrations to create new ones. This will generate the smallest number of migration files needed to describe the state of models in the code; typically one per app although sometimes multiple are needed due to app dependencies. This process does have these critical side effects: Databases that exist at some migration state before the one at the point of the recreation will no longer be able to be migrated to the current state, as the intermediate changes will have been lost. This means that those databases will need to be recreated. This also means that historical database archives will require a bit more work to resurrect; they'll need to first be migrated to the point just before the recreation, and then updated to code at or after that point. For example, say a database dump exists at a point where N migrations occur. At such time as N + 1 migrations occur, we decide to go through the recreation process. Now we have a new migration numbered N + 2 that represents the equivalent of all (1..N+1) migrations that it replaces. If you try to load and migrate the dump at point N, Django no longer has the code necessary to go from N->N+1 only -- it only has the ability to go from 0->N+2. To recover such a dump, you'll need to check out the code at the point before the recreation was done, migrate from N->N+1, and then check out latest and migrate forwards. Any open pull requests at the time of the recreation that reference or depend on some of the existing migrations will need to be modified to instead refer to the new migration files. Migrations can be recreated with this process: Remove all existing migration files: rm -f -v cfgov/*/migrations/0* Create new migration files from the state of model Python code: cfgov/manage.py makemigrations --noinput As it happens this creates new initial migrations ( 0001_initial ) for all apps, plus some subsequent migrations ( 0002_something ) for apps that depend on other apps (for example, ask_cfpb has its own initial migration and then some changes that rely on v1 ). Rename the created migration files so that they follow in sequence the migration files that used to exist. For example, if at the time of recreation there are 101 v1 migrations, the first new migration should be numbered 102. Manually alter all new migration files to indicate that they replace the old migration files. This involves adding lines to these files like: replaces = [('app_name', '0002_foo'), ('app_name', '0003_bar'), ...] This tells Django that these new files replace the old files, so that when migrations are run again, it doesn't need to do anything. Also manually update any new subsequent migration files so that they properly refer to each other. For example, if an app has two new migrations 102 and 103, the 103 file needs to properly depend on 102. To apply these new migration files to an existing database, you can simply run: cfgov/manage.py migrate --noinput You'll see that there are no changes to apply, as the new files should exactly describe the current model state in the same way that the old migrations did. See consumerfinance.gov#3770 for an example of when this was done.","title":"Recreating migrations"},{"location":"other-front-end-testing/","text":"Other Front-end Testing \u00b6 Performance testing \u00b6 To audit if the site complies with performance best practices and guidelines, Google's Lighthouse can be run from Google Chrome by opening the developer console and going to the Lighthouse tab to run a performance audit. We also have an automated task that generates a lighthouse report every night at https://cfpb.github.io/cfgov-lighthouse/ ( repository ). Source code linting \u00b6 The default test task includes linting of the JavaScript source, build, and test files. Use the gulp lint command from the command-line to run the ESLint linter, which checks the JavaScript against the rules configured in .eslintrc . See the ESLint docs for detailed rule descriptions. There are a number of options to the command: gulp lint:build : Lint only the gulp build scripts. gulp lint:tests : Lint only the test scripts. gulp lint:scripts : Lint only the project source scripts. --fix : Add this flag (like gulp lint --fix or gulp lint:build --fix ). to auto-fix some errors, where ESLint has support to do so. --path : Add this flag to specify a file to lint, rather than all files. Path is relative to the project root, such as gulp lint --path=cfgov/unprocessed/js/modules/Analytics.js . Cross browser testing \u00b6 Sauce Labs \u00b6 We use https://saucelabs.com to test the site across browsers. After logging in, the production site URLs can be tested via Live > Cross Browser in the sidebar. To test changes from localhost before they make it to production, the Sauce Connect Proxy can be used. See more info on the Sauce Labs documentation site . iOS Simulator \u00b6 While it's possible to check iOS devices in Sauce Labs, the JavaScript developer console will not be available. Therefore, it may be helpful to instead use the iOS Simulator included inside Xcode. To use the simulator with a developer console, perform the following: Find the Xcode application on your computer. Right-click on the application and select Show Package Contents . Find and open the iOS Simulator at Contents > Developer > Applications > Simulator . Open mobile Safari and navigate to the page you want to test out. Now, outside of the iOS Simulator, navigate to and open the desktop Safari application. Ensure the developer menu is shown by checking the box at Safari > Preferences\u2026 > Advanced > Show Develop menu in menu bar . Open the Develop menu in desktop Safari and there should be a Simulator option that when opened will show any JavaScript console output that's coming from the page you're visiting in the iOS Simulator.","title":"Other Front-end Testing"},{"location":"other-front-end-testing/#other-front-end-testing","text":"","title":"Other Front-end Testing"},{"location":"other-front-end-testing/#performance-testing","text":"To audit if the site complies with performance best practices and guidelines, Google's Lighthouse can be run from Google Chrome by opening the developer console and going to the Lighthouse tab to run a performance audit. We also have an automated task that generates a lighthouse report every night at https://cfpb.github.io/cfgov-lighthouse/ ( repository ).","title":"Performance testing"},{"location":"other-front-end-testing/#source-code-linting","text":"The default test task includes linting of the JavaScript source, build, and test files. Use the gulp lint command from the command-line to run the ESLint linter, which checks the JavaScript against the rules configured in .eslintrc . See the ESLint docs for detailed rule descriptions. There are a number of options to the command: gulp lint:build : Lint only the gulp build scripts. gulp lint:tests : Lint only the test scripts. gulp lint:scripts : Lint only the project source scripts. --fix : Add this flag (like gulp lint --fix or gulp lint:build --fix ). to auto-fix some errors, where ESLint has support to do so. --path : Add this flag to specify a file to lint, rather than all files. Path is relative to the project root, such as gulp lint --path=cfgov/unprocessed/js/modules/Analytics.js .","title":"Source code linting"},{"location":"other-front-end-testing/#cross-browser-testing","text":"","title":"Cross browser testing"},{"location":"other-front-end-testing/#sauce-labs","text":"We use https://saucelabs.com to test the site across browsers. After logging in, the production site URLs can be tested via Live > Cross Browser in the sidebar. To test changes from localhost before they make it to production, the Sauce Connect Proxy can be used. See more info on the Sauce Labs documentation site .","title":"Sauce Labs"},{"location":"other-front-end-testing/#ios-simulator","text":"While it's possible to check iOS devices in Sauce Labs, the JavaScript developer console will not be available. Therefore, it may be helpful to instead use the iOS Simulator included inside Xcode. To use the simulator with a developer console, perform the following: Find the Xcode application on your computer. Right-click on the application and select Show Package Contents . Find and open the iOS Simulator at Contents > Developer > Applications > Simulator . Open mobile Safari and navigate to the page you want to test out. Now, outside of the iOS Simulator, navigate to and open the desktop Safari application. Ensure the developer menu is shown by checking the box at Safari > Preferences\u2026 > Advanced > Show Develop menu in menu bar . Open the Develop menu in desktop Safari and there should be a Simulator option that when opened will show any JavaScript console output that's coming from the page you're visiting in the iOS Simulator.","title":"iOS Simulator"},{"location":"page-search/","text":"Page Search \u00b6 For page searches on consumerfinance.gov, we use Elasticsearch and the django-elasticsearch-dsl library, which is a lightweight wrapper around elasticsearch-dsl . Indexing Elasticsearch index configuration Django model information Custom fields Helpers Building the index Searching Autocomplete Suggestions References Indexing \u00b6 For any of our Django apps that need to implement search for their Django models or Wagtail page types , we include a documents.py file that defines the Elasticsearch documents that will map the model or page to the Elasticsearch index . The Document class includes three things: The Elasticsearch index configuration The Django model information Custom fields to index , and any preparation that they require We'll use our Ask CFPB answer search document as an example for each of these: from django_elasticsearch_dsl import Document from django_elasticsearch_dsl.registries import registry @registry.register_document class AnswerPageDocument(Document): pass Elasticsearch index configuration \u00b6 The index configuration is provided by an Index class on the document class that defines the elasticsearch-dsl index options . from search.elasticsearch_helpers import environment_specific_index class AnswerPageDocument(Document): class Index: name = environment_specific_index('ask-cfpb') settings = {'number_of_shards': 1, 'number_of_replicas': 0} For index naming, we have a helper function, environment_specific_index , that will generate the index name specific to the deployment environment. This allows each index to be isolated to a deployment environment within an Elasticsearch cluster. Django model information \u00b6 The Django model information is provided by a Django class on the document class that defines the model and any fields names to be indexed directly from the model. A get_queryset method can be overriden to perform any filtering on the model's queryset before content is indexed. from ask_cfpb.models.answer_page import AnswerPage class AnswerPageDocument(Document): def get_queryset(self): query_set = super().get_queryset() return query_set.filter(live=True, redirect_to_page=None) class Django: model = AnswerPage fields = [ 'search_tags', 'language', ] The fields in fields will be indexed without any preparation/manipulation, directly as are stored on the model. Custom fields \u00b6 Sometimes it might be desirable to index a field as an alternative type \u2014 say, the string that matches an integer for a Django field that species choices . It might also be desirable to construct a field to index from multiple fields on the model, particularly for Wagtail pages with stream fields. We may also want to specify Elasticsearch-specific field properties , like a custom analyzer. To do so, we specify custom fields as attributes on the document class, with an attr argument that specifies the field on the model to reference. from django_elasticsearch_dsl import fields from search.elasticsearch_helpers import synonym_analyzer class AnswerPageDocument(Document): text = fields.TextField(attr=\"text\", analyzer=synonym_analyzer) The attr on the model can be a @property or a Django model field. We can also do any data preparation/manipulation for fields using prepare_ -prefixed methods . from django_elasticsearch_dsl import fields class AnswerPageDocument(Document): portal_topics = fields.KeywordField() def prepare_portal_topics(self, instance): return [topic.heading for topic in instance.portal_topic.all()] Helpers \u00b6 We provide a few common helpers in search.elasticsearch_helpers for use in creating document classes: environment_specific_index(base_name) : Generate the index name for the base_name that is specific to the deployment environment. This allows each index to be isolated to a deployment environment within an Elasticsearch cluster. ngram_tokenizer : A reusable ngram analyzer for creating fields that autocomplete terms . This is used for type-ahead search boxes. synonym_analyzer : A reusable analyzer for creating fields that will match synonyms of a search term. from search.elasticsearch_helpers import ( environment_specific_index, ngram_tokenizer, synonym_analyzer, ) Building the index \u00b6 With the Document class created for your model in a documents.py module within a Django app listed in INSTALLED_APPS , all that is left to do is to use the django-elasticsearch-dsl management commands to rebuild the index: ./cfgov/manage.py search_index --create --models [app] The index for that app's models can also be rebuilt at any time: ./cfgov/manage.py search_index --rebuild -f --models [app] Finally, the indexes for all apps can be rebuilt using: ./cfgov/manage.py search_index --rebuild --parallel -f Searching \u00b6 The document class provides a search() class method that returns a Search object . The Search object is elasticsearch-dsl's representation of Elasticsearch search requests. To query for a specific term, for example: from ask_cfpb.documents import AnswerPageDocument AnswerPageDocument.search().query( \"match\", text={\"query\": search_term, \"operator\": \"AND\"} ) We can also add a filter context before querying, which we do to limit results to a specific language in Ask CFPB: from ask_cfpb.documents import AnswerPageDocument search = AnswerPageDocument.search().filter(\"term\", language=language) search.query( \"match\", text={\"query\": search_term, \"operator\": \"AND\"} ) Autocomplete \u00b6 For search box autocomplete, we use a field with our ngram_tokenizer analyzer and then issue a \"match\" search query for that field. Using the Ask CFPB document search above, with its language filter context, this looks like: from search.elasticsearch_helpers import ngram_tokenizer class AnswerPageDocument(Document): autocomplete = fields.TextField(analyzer=ngram_tokenizer) search = AnswerPageDocument.search().filter(\"term\", language=language) search.query('match', autocomplete=search_term) Suggestions \u00b6 For suggested spelling corrections for search terms, the Search object has a suggest() method that provides spelling suggestions for a given term on a given field. Using the Ask CFPB document search above, with its language filter context, this looks like: from ask_cfpb.documents import AnswerPageDocument search = AnswerPageDocument.search().filter(\"term\", language=language) search.suggest('suggestion', search_term, term={'field': 'text'}) References \u00b6 django-elasticsearch-dsl elasticsearch-dsl","title":"Page Search"},{"location":"page-search/#page-search","text":"For page searches on consumerfinance.gov, we use Elasticsearch and the django-elasticsearch-dsl library, which is a lightweight wrapper around elasticsearch-dsl . Indexing Elasticsearch index configuration Django model information Custom fields Helpers Building the index Searching Autocomplete Suggestions References","title":"Page Search"},{"location":"page-search/#indexing","text":"For any of our Django apps that need to implement search for their Django models or Wagtail page types , we include a documents.py file that defines the Elasticsearch documents that will map the model or page to the Elasticsearch index . The Document class includes three things: The Elasticsearch index configuration The Django model information Custom fields to index , and any preparation that they require We'll use our Ask CFPB answer search document as an example for each of these: from django_elasticsearch_dsl import Document from django_elasticsearch_dsl.registries import registry @registry.register_document class AnswerPageDocument(Document): pass","title":"Indexing"},{"location":"page-search/#elasticsearch-index-configuration","text":"The index configuration is provided by an Index class on the document class that defines the elasticsearch-dsl index options . from search.elasticsearch_helpers import environment_specific_index class AnswerPageDocument(Document): class Index: name = environment_specific_index('ask-cfpb') settings = {'number_of_shards': 1, 'number_of_replicas': 0} For index naming, we have a helper function, environment_specific_index , that will generate the index name specific to the deployment environment. This allows each index to be isolated to a deployment environment within an Elasticsearch cluster.","title":"Elasticsearch index configuration"},{"location":"page-search/#django-model-information","text":"The Django model information is provided by a Django class on the document class that defines the model and any fields names to be indexed directly from the model. A get_queryset method can be overriden to perform any filtering on the model's queryset before content is indexed. from ask_cfpb.models.answer_page import AnswerPage class AnswerPageDocument(Document): def get_queryset(self): query_set = super().get_queryset() return query_set.filter(live=True, redirect_to_page=None) class Django: model = AnswerPage fields = [ 'search_tags', 'language', ] The fields in fields will be indexed without any preparation/manipulation, directly as are stored on the model.","title":"Django model information"},{"location":"page-search/#custom-fields","text":"Sometimes it might be desirable to index a field as an alternative type \u2014 say, the string that matches an integer for a Django field that species choices . It might also be desirable to construct a field to index from multiple fields on the model, particularly for Wagtail pages with stream fields. We may also want to specify Elasticsearch-specific field properties , like a custom analyzer. To do so, we specify custom fields as attributes on the document class, with an attr argument that specifies the field on the model to reference. from django_elasticsearch_dsl import fields from search.elasticsearch_helpers import synonym_analyzer class AnswerPageDocument(Document): text = fields.TextField(attr=\"text\", analyzer=synonym_analyzer) The attr on the model can be a @property or a Django model field. We can also do any data preparation/manipulation for fields using prepare_ -prefixed methods . from django_elasticsearch_dsl import fields class AnswerPageDocument(Document): portal_topics = fields.KeywordField() def prepare_portal_topics(self, instance): return [topic.heading for topic in instance.portal_topic.all()]","title":"Custom fields"},{"location":"page-search/#helpers","text":"We provide a few common helpers in search.elasticsearch_helpers for use in creating document classes: environment_specific_index(base_name) : Generate the index name for the base_name that is specific to the deployment environment. This allows each index to be isolated to a deployment environment within an Elasticsearch cluster. ngram_tokenizer : A reusable ngram analyzer for creating fields that autocomplete terms . This is used for type-ahead search boxes. synonym_analyzer : A reusable analyzer for creating fields that will match synonyms of a search term. from search.elasticsearch_helpers import ( environment_specific_index, ngram_tokenizer, synonym_analyzer, )","title":"Helpers"},{"location":"page-search/#building-the-index","text":"With the Document class created for your model in a documents.py module within a Django app listed in INSTALLED_APPS , all that is left to do is to use the django-elasticsearch-dsl management commands to rebuild the index: ./cfgov/manage.py search_index --create --models [app] The index for that app's models can also be rebuilt at any time: ./cfgov/manage.py search_index --rebuild -f --models [app] Finally, the indexes for all apps can be rebuilt using: ./cfgov/manage.py search_index --rebuild --parallel -f","title":"Building the index"},{"location":"page-search/#searching","text":"The document class provides a search() class method that returns a Search object . The Search object is elasticsearch-dsl's representation of Elasticsearch search requests. To query for a specific term, for example: from ask_cfpb.documents import AnswerPageDocument AnswerPageDocument.search().query( \"match\", text={\"query\": search_term, \"operator\": \"AND\"} ) We can also add a filter context before querying, which we do to limit results to a specific language in Ask CFPB: from ask_cfpb.documents import AnswerPageDocument search = AnswerPageDocument.search().filter(\"term\", language=language) search.query( \"match\", text={\"query\": search_term, \"operator\": \"AND\"} )","title":"Searching"},{"location":"page-search/#autocomplete","text":"For search box autocomplete, we use a field with our ngram_tokenizer analyzer and then issue a \"match\" search query for that field. Using the Ask CFPB document search above, with its language filter context, this looks like: from search.elasticsearch_helpers import ngram_tokenizer class AnswerPageDocument(Document): autocomplete = fields.TextField(analyzer=ngram_tokenizer) search = AnswerPageDocument.search().filter(\"term\", language=language) search.query('match', autocomplete=search_term)","title":"Autocomplete"},{"location":"page-search/#suggestions","text":"For suggested spelling corrections for search terms, the Search object has a suggest() method that provides spelling suggestions for a given term on a given field. Using the Ask CFPB document search above, with its language filter context, this looks like: from ask_cfpb.documents import AnswerPageDocument search = AnswerPageDocument.search().filter(\"term\", language=language) search.suggest('suggestion', search_term, term={'field': 'text'})","title":"Suggestions"},{"location":"page-search/#references","text":"django-elasticsearch-dsl elasticsearch-dsl","title":"References"},{"location":"profiling-django/","text":"Profiling Django \u00b6 One tool we have when optimizing our code in Django is profiling. Profiling helps us understand where our time is spent, and doing what. It also helps us identify duplication of effort, as well as targets for caching and optimization. Because our primary goal is to ensure that page loads are as quick as possible for the public, this document focuses on profiling the Django request/response cycle: view code, Wagtail page rendering, etc. This document covers the tools we use to both generate and understand a profile. Generating a profile from a URL \u00b6 To enable profile generation, we use a middleware that wraps Python's builtin cprofile module , django-cprofile-middleware . This middleware is enabled by default for local development when DEBUG is True . For any URL served by Django, you can add the ?prof querystring (or &prof to an existing query string) in order to see a profile of the request/response cycle for that view. For example, http://localhost:8000/about-us/newsroom/?prof , might return something like: 1579654 function calls (1575009 primitive calls) in 28.273 seconds Ordered by: internal time List reduced from 1108 to 500 due to restriction <500> ncalls tottime percall cumtime percall filename:lineno(function) 74952 2.349 0.000 17.292 0.000 utils.py:487(__getattr__) 174882/174880 2.036 0.000 4.559 0.000 {built-in method builtins.isinstance} 58330 1.985 0.000 5.482 0.000 utils.py:328(__getattr__) 77732 1.900 0.000 5.027 0.000 utils.py:157(__getattr__) 2776 1.821 0.001 21.265 0.008 documents.py:89(init_prepare) \u2026 You can also sort this output using a sort field. For example, to sort by call count instead of the default time sort: http://localhost:8000/about-us/newsroom/?prof&sort=ncalls . The can be any supported by pstats.Stats.sort_stats . Most usefully, howevever, you can download the .prof file that contains the profile data, using a download field. For example http://localhost:8000/about-us/newsroom/?prof&download . This will download view.prof , which you can then open in a tool like SnakeViz (see below). Understanding a generated profile \u00b6 There are a number of tools available to visualize and understand a .prof profile data file. The tool we recommend is SnakeViz , a browser-based graphical viewer of cProfile data. To install SnakeViz locally with pipx : pipx install snakeviz After downloading the profile data for a Django view, the data can be viewed with SnakeViz with: snakeviz view.prof SnakeViz has two visualization styles, Icicle and Sunburst. Both visualizations represent function calls by the time spent inside the function as a portion of total time of its calling function. The root function will represent the total time for the Django request/response cycle. For example: This visualization shows the request/response cycle took 27.2s in the root view. We can identify the Wagtail serve() view from models.py . When hovering over a function in the visualization, you see the function's file, directory, and line number in additional context to the left. If that function is called multiple times, other calls will also highlight in the visualization. This is a good way to identify duplication that one may wish to refactor. Basic workflow \u00b6 Install SnakeViz, if you haven't already: pipx install snakeviz Visit the local URL you wish to profile and download the profile data: http://localhost:8000/about-us/newsroom/?prof&download Open the profile data in SnakeViz: snakeviz view.prof Use the SnakeViz visualization to identify: The call stack of the request/response cycle for a particular Django view Bottlenecks in the view's request/response cycle Long calls that can be optimized Duplicate calls that can be eliminated or cached","title":"Profiling Django"},{"location":"profiling-django/#profiling-django","text":"One tool we have when optimizing our code in Django is profiling. Profiling helps us understand where our time is spent, and doing what. It also helps us identify duplication of effort, as well as targets for caching and optimization. Because our primary goal is to ensure that page loads are as quick as possible for the public, this document focuses on profiling the Django request/response cycle: view code, Wagtail page rendering, etc. This document covers the tools we use to both generate and understand a profile.","title":"Profiling Django"},{"location":"profiling-django/#generating-a-profile-from-a-url","text":"To enable profile generation, we use a middleware that wraps Python's builtin cprofile module , django-cprofile-middleware . This middleware is enabled by default for local development when DEBUG is True . For any URL served by Django, you can add the ?prof querystring (or &prof to an existing query string) in order to see a profile of the request/response cycle for that view. For example, http://localhost:8000/about-us/newsroom/?prof , might return something like: 1579654 function calls (1575009 primitive calls) in 28.273 seconds Ordered by: internal time List reduced from 1108 to 500 due to restriction <500> ncalls tottime percall cumtime percall filename:lineno(function) 74952 2.349 0.000 17.292 0.000 utils.py:487(__getattr__) 174882/174880 2.036 0.000 4.559 0.000 {built-in method builtins.isinstance} 58330 1.985 0.000 5.482 0.000 utils.py:328(__getattr__) 77732 1.900 0.000 5.027 0.000 utils.py:157(__getattr__) 2776 1.821 0.001 21.265 0.008 documents.py:89(init_prepare) \u2026 You can also sort this output using a sort field. For example, to sort by call count instead of the default time sort: http://localhost:8000/about-us/newsroom/?prof&sort=ncalls . The can be any supported by pstats.Stats.sort_stats . Most usefully, howevever, you can download the .prof file that contains the profile data, using a download field. For example http://localhost:8000/about-us/newsroom/?prof&download . This will download view.prof , which you can then open in a tool like SnakeViz (see below).","title":"Generating a profile from a URL"},{"location":"profiling-django/#understanding-a-generated-profile","text":"There are a number of tools available to visualize and understand a .prof profile data file. The tool we recommend is SnakeViz , a browser-based graphical viewer of cProfile data. To install SnakeViz locally with pipx : pipx install snakeviz After downloading the profile data for a Django view, the data can be viewed with SnakeViz with: snakeviz view.prof SnakeViz has two visualization styles, Icicle and Sunburst. Both visualizations represent function calls by the time spent inside the function as a portion of total time of its calling function. The root function will represent the total time for the Django request/response cycle. For example: This visualization shows the request/response cycle took 27.2s in the root view. We can identify the Wagtail serve() view from models.py . When hovering over a function in the visualization, you see the function's file, directory, and line number in additional context to the left. If that function is called multiple times, other calls will also highlight in the visualization. This is a good way to identify duplication that one may wish to refactor.","title":"Understanding a generated profile"},{"location":"profiling-django/#basic-workflow","text":"Install SnakeViz, if you haven't already: pipx install snakeviz Visit the local URL you wish to profile and download the profile data: http://localhost:8000/about-us/newsroom/?prof&download Open the profile data in SnakeViz: snakeviz view.prof Use the SnakeViz visualization to identify: The call stack of the request/response cycle for a particular Django view Bottlenecks in the view's request/response cycle Long calls that can be optimized Duplicate calls that can be eliminated or cached","title":"Basic workflow"},{"location":"python-unit-tests/","text":"Python testing \u00b6 Writing tests \u00b6 We have multiple resources for writing new unit tests for Django, Wagtail, and Python code: CFPB Django and Wagtail unit testing documentation The Django testing documentation The Wagtail testing documentation Real Python's \"Testing in Django\" Testing Elasticsearch \u00b6 When writing tests that rely on a running Elasticsearch service, consider using the search.elasticsearch_helpers.ElasticsearchTestsMixin mixin: from django.test import TestCase from search.elasticsearch_helpers import ElasticsearchTestsMixin class MyTests(ElasticsearchTestsMixin, TestCase): def test_something(self): self.rebuild_elasticsearch_index() # test something that relies on the Elasticsearch index Refer to the mixin's source code for additional details on its functionality. Prerequisites \u00b6 If you have set up a standalone installation of consumerfinance.gov , you'll need to activate your virtual environment before running the tests: workon consumerfinance.gov If you have not set up the standalone installation of consumerfinance.gov, you can still run the tests if you install Tox in your local installation of Python : pip install tox If you have set up a Docker-based installation of consumerfinance.gov , you can run the tests there by accessing the Python container's shell : docker-compose exec python bash Running tests \u00b6 Our test suite can either be run in a local virtualenv or in Docker. Please note, the tests run quite slow in Docker. To run the the full suite of Python tests using Tox, make sure you are in the consumerfinance.gov root and then run: tox Tox runs different isolated Python environments with different versions of dependencies. We use it to lint our Python files, check out import sorting, and run unit tests in both Python 3.6 and Python 3.8. You can select specific environments using -e . Running tox by itself is the same as running: tox -e lint -e unittest These default environments are: lint , which runs our linters . We require this environment to pass in CI. validate-migrations , which checks for any missing Django migrations. We require this environment to pass in CI. unittest , which runs unit tests against the current production versions of Python, Django, and Wagtail. We require this environment to pass in CI. Tests will run against the default Django database. If you would like to run only a specific test, or the tests for a specific app, you can provide a dotted path to the test as the final argument to any of the above calls to tox : tox -e unittest regulations3k.tests.test_regdown If you would like to skip running Django migrations when testing, set the SKIP_DJANGO_MIGRATIONS environment variable to any value before running tox . Linting \u00b6 We use the flake8 and isort tools to ensure compliance with PEP8 style guide , Django coding style guidelines , and the CFPB Python style guide . Both flake8 and isort can be run using the Tox lint environment: tox -e lint This will run isort in check-only mode and it will print diffs for imports that need to be fixed. To automatically fix import sort issues, run: isort --recursive cfgov/ From the root of consumerfinance.gov . Coverage \u00b6 To see Python code coverage information immediately following a test run, you can add the coverage env to the list of envs for tox to run: tox -e lint -e unittest -e coverage You can also run coverage directly to see coverage information from a previous test run: coverage report -m To see coverage for a limited number of files, use the --include argument to coverage and provide a path to the files you wish to see: coverage report -m --include=./cfgov/regulations3k/* Test output \u00b6 Python tests should avoid writing to stdout as part of their normal execution. To enforce this convention, the tests can be run using a custom Django test runner that fails if anything is written to stdout. This test runner is at cfgov.test.StdoutCapturingTestRunner and can be enabled with the TEST_RUNNER environment variable: TEST_RUNNER=core.testutils.runners.StdoutCapturingTestRunner tox -e unittest This test runner is enabled when tests are run automatically on GitHub Actions , but is not used by default when running tests locally. GovDelivery \u00b6 If you write Python code that interacts with the GovDelivery subscription API, you can use the functionality provided in core.govdelivery.MockGovDelivery as a mock interface to avoid the use of patch in unit tests. This object behaves similarly to the real govdelivery.api.GovDelivery class in that it handles all requests and returns a valid (200) requests.Response instance. Conveniently for unit testing, all calls are stored in a class-level list that can be retrieved at MockGovDelivery.calls . This allows for testing of code that interacts with GovDelivery by checking the contents of this list to ensure that the right methods were called. This pattern is modeled after Django's django.core.mail.outbox which provides similar functionality for testing sending of emails. The related classes ExceptionMockGovDelivery and ServerErrorMockGovDelivery can similarly be used in unit tests to test for cases where a call to the GovDelivery API raises an exception and returns an HTTP status code of 500, respectively.","title":"Python Unit Testing"},{"location":"python-unit-tests/#python-testing","text":"","title":"Python testing"},{"location":"python-unit-tests/#writing-tests","text":"We have multiple resources for writing new unit tests for Django, Wagtail, and Python code: CFPB Django and Wagtail unit testing documentation The Django testing documentation The Wagtail testing documentation Real Python's \"Testing in Django\"","title":"Writing tests"},{"location":"python-unit-tests/#testing-elasticsearch","text":"When writing tests that rely on a running Elasticsearch service, consider using the search.elasticsearch_helpers.ElasticsearchTestsMixin mixin: from django.test import TestCase from search.elasticsearch_helpers import ElasticsearchTestsMixin class MyTests(ElasticsearchTestsMixin, TestCase): def test_something(self): self.rebuild_elasticsearch_index() # test something that relies on the Elasticsearch index Refer to the mixin's source code for additional details on its functionality.","title":"Testing Elasticsearch"},{"location":"python-unit-tests/#prerequisites","text":"If you have set up a standalone installation of consumerfinance.gov , you'll need to activate your virtual environment before running the tests: workon consumerfinance.gov If you have not set up the standalone installation of consumerfinance.gov, you can still run the tests if you install Tox in your local installation of Python : pip install tox If you have set up a Docker-based installation of consumerfinance.gov , you can run the tests there by accessing the Python container's shell : docker-compose exec python bash","title":"Prerequisites"},{"location":"python-unit-tests/#running-tests","text":"Our test suite can either be run in a local virtualenv or in Docker. Please note, the tests run quite slow in Docker. To run the the full suite of Python tests using Tox, make sure you are in the consumerfinance.gov root and then run: tox Tox runs different isolated Python environments with different versions of dependencies. We use it to lint our Python files, check out import sorting, and run unit tests in both Python 3.6 and Python 3.8. You can select specific environments using -e . Running tox by itself is the same as running: tox -e lint -e unittest These default environments are: lint , which runs our linters . We require this environment to pass in CI. validate-migrations , which checks for any missing Django migrations. We require this environment to pass in CI. unittest , which runs unit tests against the current production versions of Python, Django, and Wagtail. We require this environment to pass in CI. Tests will run against the default Django database. If you would like to run only a specific test, or the tests for a specific app, you can provide a dotted path to the test as the final argument to any of the above calls to tox : tox -e unittest regulations3k.tests.test_regdown If you would like to skip running Django migrations when testing, set the SKIP_DJANGO_MIGRATIONS environment variable to any value before running tox .","title":"Running tests"},{"location":"python-unit-tests/#linting","text":"We use the flake8 and isort tools to ensure compliance with PEP8 style guide , Django coding style guidelines , and the CFPB Python style guide . Both flake8 and isort can be run using the Tox lint environment: tox -e lint This will run isort in check-only mode and it will print diffs for imports that need to be fixed. To automatically fix import sort issues, run: isort --recursive cfgov/ From the root of consumerfinance.gov .","title":"Linting"},{"location":"python-unit-tests/#coverage","text":"To see Python code coverage information immediately following a test run, you can add the coverage env to the list of envs for tox to run: tox -e lint -e unittest -e coverage You can also run coverage directly to see coverage information from a previous test run: coverage report -m To see coverage for a limited number of files, use the --include argument to coverage and provide a path to the files you wish to see: coverage report -m --include=./cfgov/regulations3k/*","title":"Coverage"},{"location":"python-unit-tests/#test-output","text":"Python tests should avoid writing to stdout as part of their normal execution. To enforce this convention, the tests can be run using a custom Django test runner that fails if anything is written to stdout. This test runner is at cfgov.test.StdoutCapturingTestRunner and can be enabled with the TEST_RUNNER environment variable: TEST_RUNNER=core.testutils.runners.StdoutCapturingTestRunner tox -e unittest This test runner is enabled when tests are run automatically on GitHub Actions , but is not used by default when running tests locally.","title":"Test output"},{"location":"python-unit-tests/#govdelivery","text":"If you write Python code that interacts with the GovDelivery subscription API, you can use the functionality provided in core.govdelivery.MockGovDelivery as a mock interface to avoid the use of patch in unit tests. This object behaves similarly to the real govdelivery.api.GovDelivery class in that it handles all requests and returns a valid (200) requests.Response instance. Conveniently for unit testing, all calls are stored in a class-level list that can be retrieved at MockGovDelivery.calls . This allows for testing of code that interacts with GovDelivery by checking the contents of this list to ensure that the right methods were called. This pattern is modeled after Django's django.core.mail.outbox which provides similar functionality for testing sending of emails. The related classes ExceptionMockGovDelivery and ServerErrorMockGovDelivery can similarly be used in unit tests to test for cases where a call to the GovDelivery API raises an exception and returns an HTTP status code of 500, respectively.","title":"GovDelivery"},{"location":"related-projects/","text":"Related Projects \u00b6 Satellite apps \u00b6 We use the term \"satellite apps\" to refer to other CFPB-maintained Python source code repositories and packages that provide functionality or content for https://www.consumerfinance.gov but are not included in consumerfinance.gov. Most but not all of these repositories can be run or tested without needing to set up the full website. We have six satellite apps that are maintained outside of the consumerfinance.gov codebase: ccdb5-api ccdb5-ui owning-a-home-api teachers-digital-platform These satellite apps are imported into consumerfinance.gov as part of the project requirements files . Thinking about making a new satellite app? Satellite apps were originally built to be imported into the consumerfinance.gov website before we started using Wagtail to manage site content. We now prefer to build projects as apps inside the consumerfinance.gov repo. For more info, refer to the \"Setting up new project code for consumerfinance.gov\" page on the CFGOV/platform wiki on GHE. Other Python packages \u00b6 CFPB also develops and maintains several other Python source code repositories and packages that are used on https://www.consumerfinance.gov but are more general-purpose. These include: django-flags wagtail-flags wagtail-inventory wagtail-sharing wagtail-treemodeladmin The cfpb/development repository contains the CFPB development guidelines used on these projects. Developing Python packages with consumerfinance.gov \u00b6 Developers working on changes to satellite apps or other Python packages often want or need to test their work as part of the larger consumerfinance.gov project. The standard installation process for consumerfinance.gov includes whatever versions of these packages are specified in project requirements files . Developers may want to temporarily or permanently replace those with a local copy of package source code. Using a local virtual environment \u00b6 When running in a virtual environment local to your developer machine, standard usage of pip install can be used to swap out package versions. Running pip install -e path/to/local/repo will install local source code in \"editable\" mode, replacing the satellite app version installed from requirements files with your local copy: Note and is shown for illustration purpooses only. # In a local virtual environment, after installing consumerfinance.gov. # Python packages have been installed from requirements files. # This includes, for example, the retirement satellite app: (consumerfinance.gov) $ pip freeze | grep retirement retirement==0.10.0 # Replace this version with an editable install pointing to your local copy. (consumerfinance.gov) $ pip install -e ../retirement/ ... Installing collected packages: retirement Found existing installation: retirement 0.10.0 Uninstalling retirement-0.10.0: Successfully uninstalled retirement-0.10.0 Running setup.py develop for retirement Successfully installed retirement # The Python environment is now using the local copy. (consumerfinance.gov) $ pip freeze | grep retirement -e git+git@github.com:myfork/retirement.git@7d2b8eca86ed33d90b5cd7782e1f90b7ac89f6f9#egg=retirement Installing a local copy of source code in editable mode ensures that any changes made to files there will take effect immediately. To switch back to the default version of a Python package, removing the use of a local copy, you can manually install the version specified in the requirements files: (consumerfinance.gov) $ pip install retirement==0.10.0 Re-running the full virtual environment setup script will do the same thing. Using Docker \u00b6 Working on Python packages requires a different approach when running consumerfinance.gov locally with its Docker setup . This is because while your local consumerfinance.gov directory is exposed to the container, sibling directories or other locations where you might clone other repositories are not. For this reason, the Docker setup provides the ability to use the local consumerfinance.gov/develop-apps subdirectory as place to put local copies of Python packages. Any packages put there (e.g. via a git clone of a satellite apps' repo) will be automatically added to the PYTHONPATH in the Python containers. Apps located within will be importable from Python running in the Docker containers, and will take precedence over any versions installed from requirements files as part of the Docker images. To switch back to the default version of a Python package, removing the use of a local copy, simply delete the copy from the develop-apps directory. For any satellite apps that provide front-end assets that need to be built, you will need to run that step seperately: # Check out the retirement satellite app into develop-apps: cd develop-apps git clone git@github.com:cfpb/retirement.git # Build the front-end: cd retirement ./frontendbuild.sh If the satellite app needs any Python requirements that are not specified in the consumerfinance.gov requirements , they will need to be installed seperately by accessing the Python container shell and using pip : docker-compose exec python bash pip install [PACKAGE NAME]","title":"Related Projects"},{"location":"related-projects/#related-projects","text":"","title":"Related Projects"},{"location":"related-projects/#satellite-apps","text":"We use the term \"satellite apps\" to refer to other CFPB-maintained Python source code repositories and packages that provide functionality or content for https://www.consumerfinance.gov but are not included in consumerfinance.gov. Most but not all of these repositories can be run or tested without needing to set up the full website. We have six satellite apps that are maintained outside of the consumerfinance.gov codebase: ccdb5-api ccdb5-ui owning-a-home-api teachers-digital-platform These satellite apps are imported into consumerfinance.gov as part of the project requirements files . Thinking about making a new satellite app? Satellite apps were originally built to be imported into the consumerfinance.gov website before we started using Wagtail to manage site content. We now prefer to build projects as apps inside the consumerfinance.gov repo. For more info, refer to the \"Setting up new project code for consumerfinance.gov\" page on the CFGOV/platform wiki on GHE.","title":"Satellite apps"},{"location":"related-projects/#other-python-packages","text":"CFPB also develops and maintains several other Python source code repositories and packages that are used on https://www.consumerfinance.gov but are more general-purpose. These include: django-flags wagtail-flags wagtail-inventory wagtail-sharing wagtail-treemodeladmin The cfpb/development repository contains the CFPB development guidelines used on these projects.","title":"Other Python packages"},{"location":"related-projects/#developing-python-packages-with-consumerfinancegov","text":"Developers working on changes to satellite apps or other Python packages often want or need to test their work as part of the larger consumerfinance.gov project. The standard installation process for consumerfinance.gov includes whatever versions of these packages are specified in project requirements files . Developers may want to temporarily or permanently replace those with a local copy of package source code.","title":"Developing Python packages with consumerfinance.gov"},{"location":"related-projects/#using-a-local-virtual-environment","text":"When running in a virtual environment local to your developer machine, standard usage of pip install can be used to swap out package versions. Running pip install -e path/to/local/repo will install local source code in \"editable\" mode, replacing the satellite app version installed from requirements files with your local copy: Note and is shown for illustration purpooses only. # In a local virtual environment, after installing consumerfinance.gov. # Python packages have been installed from requirements files. # This includes, for example, the retirement satellite app: (consumerfinance.gov) $ pip freeze | grep retirement retirement==0.10.0 # Replace this version with an editable install pointing to your local copy. (consumerfinance.gov) $ pip install -e ../retirement/ ... Installing collected packages: retirement Found existing installation: retirement 0.10.0 Uninstalling retirement-0.10.0: Successfully uninstalled retirement-0.10.0 Running setup.py develop for retirement Successfully installed retirement # The Python environment is now using the local copy. (consumerfinance.gov) $ pip freeze | grep retirement -e git+git@github.com:myfork/retirement.git@7d2b8eca86ed33d90b5cd7782e1f90b7ac89f6f9#egg=retirement Installing a local copy of source code in editable mode ensures that any changes made to files there will take effect immediately. To switch back to the default version of a Python package, removing the use of a local copy, you can manually install the version specified in the requirements files: (consumerfinance.gov) $ pip install retirement==0.10.0 Re-running the full virtual environment setup script will do the same thing.","title":"Using a local virtual environment"},{"location":"related-projects/#using-docker","text":"Working on Python packages requires a different approach when running consumerfinance.gov locally with its Docker setup . This is because while your local consumerfinance.gov directory is exposed to the container, sibling directories or other locations where you might clone other repositories are not. For this reason, the Docker setup provides the ability to use the local consumerfinance.gov/develop-apps subdirectory as place to put local copies of Python packages. Any packages put there (e.g. via a git clone of a satellite apps' repo) will be automatically added to the PYTHONPATH in the Python containers. Apps located within will be importable from Python running in the Docker containers, and will take precedence over any versions installed from requirements files as part of the Docker images. To switch back to the default version of a Python package, removing the use of a local copy, simply delete the copy from the develop-apps directory. For any satellite apps that provide front-end assets that need to be built, you will need to run that step seperately: # Check out the retirement satellite app into develop-apps: cd develop-apps git clone git@github.com:cfpb/retirement.git # Build the front-end: cd retirement ./frontendbuild.sh If the satellite app needs any Python requirements that are not specified in the consumerfinance.gov requirements , they will need to be installed seperately by accessing the Python container shell and using pip : docker-compose exec python bash pip install [PACKAGE NAME]","title":"Using Docker"},{"location":"running-docker/","text":"Running in Docker \u00b6 First, follow the Docker installation instructions to setup your Docker environment and create the project Docker containers. We use docker-compose to run an Elasticsearch container, a PostgreSQL container, and Django in a Python container. There is also a container serving the documentation. All of these containers are configured in our docker-compose.yml file . See the Docker documentation for more about the format and use of this file. The following URLs are mapped to your host from the containers: Access consumerfinance.gov running in the Python container: http://localhost:8000/ Access Elasticsearch: http://localhost:9200/ View this documentation: http://localhost:8888/ To build and run the containers for the first time, run: docker network create cfgov docker-compose up Environment variables \u00b6 Environment variables from your .env file are sourced when the Python container starts and when you access the running Python container . Your local shell environment variables, however, are not visible to applications running in Docker. To add new environment variables, simply add them to the .env file, stop docker-compose with Ctrl+C, and start it again with docker-compose up . Commands that must be run from within the Python container \u00b6 Django manage.py commands can only be run after you've opened up a shell in the Python container . From there, commands like cfgov/manage.py migrate should run as expected. The same goes for scripts like ./refresh-data.sh and ./initial-data.sh \u2014 they will work as expected once you\u2019re inside the Python container. Access a container\u2019s shell \u00b6 Python: docker-compose exec python bash Elasticsearch: docker-compose exec elasticsearch bash PostgreSQL: docker-compose exec postgres bash Update Python dependencies \u00b6 If the Python package requirements files have changed, you will need to stop docker-compose (if it is running) and rebuild the Python container using: docker-compose up --build python Work on satellite apps \u00b6 See \u201cUsing Docker\u201d on the Related Projects page . Attach for debugging \u00b6 If you have inserted a PDB breakpoint in your code and need to interact with the running Django process when the breakpoint is reached you can run docker attach : docker attach consumerfinancegov_python_1 When you're done, you can detach with Ctrl+P Ctrl+Q . Note docker attach takes the specific container name or ID. Yours may or may not be consumerfinancegov_python_1 . To verify, use docker container ls to get the Python container's full name or ID. Useful Docker commands \u00b6 For docker-compose commands, [SERVICE] is the service name that is defined in docker-compose.yml . For docker commands, [CONTAINER] is the container name displayed with docker ps . docker ps will list all containers. docker logs [CONTAINER] will print the logs of a container. docker top [CONTAINER] will display the running processes in a container. docker-compose build [SERVICE] will build any of our configured containers. Production-like Docker Image \u00b6 This repository includes a \"production-like\" Docker image, created for experimenting with how cf.gov could be built and run as a Docker container in production. This includes: all relevant consumerfinance.gov source code all OS, Python, and JS dependencies for building and running the cf.gov webapp procedures for executing Django collectstatic and yarn -based frontend build process an Apache HTTPD webserver with mod_wsgi , run with configs in consumerfinance.gov How do I use it? \u00b6 Just Docker \u00b6 If you just want to build the image: docker build . --build-arg scl_python_version=rh-python38 -t your-desired-image-name Note: The scl_python_version build arg specifies which Python Software Collection version you'd like to use. We've tested this against rh-python38 . Docker Compose \u00b6 You can also launch the full cf.gov stack locally via docker-compose . This setup is a nice way to test out new Apache config changes. It includes volumes that mount your local checkout cfgov/apache config directories into the container, allowing you to change configs locally without having to rebuild the image each time. Launch the stack. docker-compose -f docker-compose.yml -f docker-compose.prod.yml up --build This creates a container running cf.gov on Python, as well as Postgres and Elasticsearch containers, much like the development environment. Load the cfgov database (optional). If you do not already have a running cfgov database, you will need to download and load it from within the container. docker-compose exec python bash # Once in the container... export CFGOV_PROD_DB_LOCATION=<database-dump-url> ./refresh-data.sh Browse to your new local cf.gov site. http://localhost:8000 Adjust an Apache cfgov/apache config and reload Apache (optional). docker-compose exec python bash # Once in the container... httpd -d ./cfgov/apache -k restart Switch back to the development Compose setup. docker-compose rm -sf python docker-compose up --build python Jenkins CI + Docker Swarm \u00b6 This repo also includes a Docker Swarm-compatible Compose file ( docker-stack.yml ). This file is intended for use with the project's Jenkinsfile multibranch build pipeline . It follows a standard Docker build/scan/push workflow, optionally deploying to our Docker Swarm cluster. How does it work? \u00b6 The production image extends the development image. If you look at the Dockerfile , this is spelled out by the line: FROM cfgov-dev as cfgov-prod Both 'cfgov-dev' and 'cfgov-prod' are called \" build stages \". That line means, \"create a new stage, starting from cfgov-dev, called cfgov-prod\". From there, we: Install SCL-based Apache HTTPD, and the mod_wsgi version appropriate for our chosen scl_python_version . Run frontend.sh, Django's collectstatic command, and then uninstall node and yarn. Set the default command on container startup to httpd -d ./cfgov/apache -D FOREGROUND , which runs Apache using the configuration in consumerfinance.gov , in the foreground (typical when running Apache in a container).","title":"Running in Docker"},{"location":"running-docker/#running-in-docker","text":"First, follow the Docker installation instructions to setup your Docker environment and create the project Docker containers. We use docker-compose to run an Elasticsearch container, a PostgreSQL container, and Django in a Python container. There is also a container serving the documentation. All of these containers are configured in our docker-compose.yml file . See the Docker documentation for more about the format and use of this file. The following URLs are mapped to your host from the containers: Access consumerfinance.gov running in the Python container: http://localhost:8000/ Access Elasticsearch: http://localhost:9200/ View this documentation: http://localhost:8888/ To build and run the containers for the first time, run: docker network create cfgov docker-compose up","title":"Running in Docker"},{"location":"running-docker/#environment-variables","text":"Environment variables from your .env file are sourced when the Python container starts and when you access the running Python container . Your local shell environment variables, however, are not visible to applications running in Docker. To add new environment variables, simply add them to the .env file, stop docker-compose with Ctrl+C, and start it again with docker-compose up .","title":"Environment variables"},{"location":"running-docker/#commands-that-must-be-run-from-within-the-python-container","text":"Django manage.py commands can only be run after you've opened up a shell in the Python container . From there, commands like cfgov/manage.py migrate should run as expected. The same goes for scripts like ./refresh-data.sh and ./initial-data.sh \u2014 they will work as expected once you\u2019re inside the Python container.","title":"Commands that must be run from within the Python container"},{"location":"running-docker/#access-a-containers-shell","text":"Python: docker-compose exec python bash Elasticsearch: docker-compose exec elasticsearch bash PostgreSQL: docker-compose exec postgres bash","title":"Access a container\u2019s shell"},{"location":"running-docker/#update-python-dependencies","text":"If the Python package requirements files have changed, you will need to stop docker-compose (if it is running) and rebuild the Python container using: docker-compose up --build python","title":"Update Python dependencies"},{"location":"running-docker/#work-on-satellite-apps","text":"See \u201cUsing Docker\u201d on the Related Projects page .","title":"Work on satellite apps"},{"location":"running-docker/#attach-for-debugging","text":"If you have inserted a PDB breakpoint in your code and need to interact with the running Django process when the breakpoint is reached you can run docker attach : docker attach consumerfinancegov_python_1 When you're done, you can detach with Ctrl+P Ctrl+Q . Note docker attach takes the specific container name or ID. Yours may or may not be consumerfinancegov_python_1 . To verify, use docker container ls to get the Python container's full name or ID.","title":"Attach for debugging"},{"location":"running-docker/#useful-docker-commands","text":"For docker-compose commands, [SERVICE] is the service name that is defined in docker-compose.yml . For docker commands, [CONTAINER] is the container name displayed with docker ps . docker ps will list all containers. docker logs [CONTAINER] will print the logs of a container. docker top [CONTAINER] will display the running processes in a container. docker-compose build [SERVICE] will build any of our configured containers.","title":"Useful Docker commands"},{"location":"running-docker/#production-like-docker-image","text":"This repository includes a \"production-like\" Docker image, created for experimenting with how cf.gov could be built and run as a Docker container in production. This includes: all relevant consumerfinance.gov source code all OS, Python, and JS dependencies for building and running the cf.gov webapp procedures for executing Django collectstatic and yarn -based frontend build process an Apache HTTPD webserver with mod_wsgi , run with configs in consumerfinance.gov","title":"Production-like Docker Image"},{"location":"running-docker/#how-do-i-use-it","text":"","title":"How do I use it?"},{"location":"running-docker/#just-docker","text":"If you just want to build the image: docker build . --build-arg scl_python_version=rh-python38 -t your-desired-image-name Note: The scl_python_version build arg specifies which Python Software Collection version you'd like to use. We've tested this against rh-python38 .","title":"Just Docker"},{"location":"running-docker/#docker-compose","text":"You can also launch the full cf.gov stack locally via docker-compose . This setup is a nice way to test out new Apache config changes. It includes volumes that mount your local checkout cfgov/apache config directories into the container, allowing you to change configs locally without having to rebuild the image each time. Launch the stack. docker-compose -f docker-compose.yml -f docker-compose.prod.yml up --build This creates a container running cf.gov on Python, as well as Postgres and Elasticsearch containers, much like the development environment. Load the cfgov database (optional). If you do not already have a running cfgov database, you will need to download and load it from within the container. docker-compose exec python bash # Once in the container... export CFGOV_PROD_DB_LOCATION=<database-dump-url> ./refresh-data.sh Browse to your new local cf.gov site. http://localhost:8000 Adjust an Apache cfgov/apache config and reload Apache (optional). docker-compose exec python bash # Once in the container... httpd -d ./cfgov/apache -k restart Switch back to the development Compose setup. docker-compose rm -sf python docker-compose up --build python","title":"Docker Compose"},{"location":"running-docker/#jenkins-ci-docker-swarm","text":"This repo also includes a Docker Swarm-compatible Compose file ( docker-stack.yml ). This file is intended for use with the project's Jenkinsfile multibranch build pipeline . It follows a standard Docker build/scan/push workflow, optionally deploying to our Docker Swarm cluster.","title":"Jenkins CI + Docker Swarm"},{"location":"running-docker/#how-does-it-work","text":"The production image extends the development image. If you look at the Dockerfile , this is spelled out by the line: FROM cfgov-dev as cfgov-prod Both 'cfgov-dev' and 'cfgov-prod' are called \" build stages \". That line means, \"create a new stage, starting from cfgov-dev, called cfgov-prod\". From there, we: Install SCL-based Apache HTTPD, and the mod_wsgi version appropriate for our chosen scl_python_version . Run frontend.sh, Django's collectstatic command, and then uninstall node and yarn. Set the default command on container startup to httpd -d ./cfgov/apache -D FOREGROUND , which runs Apache using the configuration in consumerfinance.gov , in the foreground (typical when running Apache in a container).","title":"How does it work?"},{"location":"running-virtualenv/","text":"Running in a Virtual Environment \u00b6 First, follow the standalone installation instructions to create your virtual environment, install required dependencies, and run the setup scripts. You will generally have three tabs (or windows) open in your terminal, which will be used for: Git operations . Perform Git operations and general development in the repository, such as git checkout main . Elasticsearch . Run an Elasticsearch (ES) instance. See instructions below . Django server . Start and stop the web server. Server is started with ./runserver.sh , but see more details below . What follows are the specific steps for each of these tabs. 1. Git operations \u00b6 From this tab you can do Git operations, such as checking out our main branch: git checkout main Updating all dependencies \u00b6 Each time you fetch from the upstream repository (this repo), run ./setup.sh . This setup script will remove and reinstall the project dependencies and rebuild the site's JavaScript and CSS assets. Note You may also run ./backend.sh or ./frontend.sh if you only want to re-build the backend or front-end, respectively. Setting environments \u00b6 The NODE_ENV environment variable can be set in your .env file to either development or production , which will affect how the build is made and what gulp tasks are available. To install dependencies of one environment or the other run ./frontend.sh (dependencies and devDependencies) or ./frontend.sh production (dependencies but not devDependencies). 2. Run Elasticsearch (optional) \u00b6 Elasticsearch is needed for certain pieces of this project but is not a requirement for basic functionality. If Elasticsearch is installed via Homebrew , you can see instructions for running manually or as a background service using: brew info elasticsearch Typically to run Elasticsearch as a background service you can run: brew services start elasticsearch 3. Launch Site \u00b6 First, move into the consumerfinance.gov project directory and ready your environment: # Use the consumerfinance.gov virtualenv. workon consumerfinance.gov # cd into this directory (if you aren't already there) cd consumerfinance.gov From the project root, start the Django server: ./runserver.sh Note If prompted to migrate database changes, stop the server with ctrl + c and run these commands: python cfgov/manage.py migrate ./initial-data.sh ./runserver.sh To view the site browse to: http://localhost:8000 Using a different port If you want to run the server at a port other than 8000 use python cfgov/manage.py runserver <port number> Specify an alternate port number, e.g. 8001 . To view the Wagtail admin login, browse to http://localhost:8000/admin and login with username admin and password admin (created in initial-data.sh above; note that this password will expire after 60 days). Using HTTPS locally To access a local server using HTTPS use ./runserver.sh ssl You'll need to ignore any browser certificate errors. Available Gulp Tasks \u00b6 There are a number of important gulp tasks, particularly build and test , which will build the project and test it, respectively. Tasks are invoked via an yarn run command so that the local gulp-cli can be used. Using the yarn run gulp -- --tasks command you can view all available tasks. The important ones are listed below: yarn run gulp build # Concatenate, optimize, and copy source files to the production /dist/ directory. yarn run gulp clean # Remove the contents of the production /dist/ directory. yarn run gulp lint # Lint the scripts and build files. yarn run gulp docs # Generate JSDocs from the scripts. yarn run gulp test # Run linting, unit, and functional tests (see below). yarn run gulp test:unit # Run only unit tests on source code. Reinstalling the virtual environment \u00b6 To remove an existing virtual environment for a reinstall of consumerfinance.gov , first deactivate the virtual environment if it is active: deactivate Then remove the existing virtual environment: rmvirtualenv consumerfinance.gov After this, you may follow the installation instructions again.","title":"Running in a Virtual Environment"},{"location":"running-virtualenv/#running-in-a-virtual-environment","text":"First, follow the standalone installation instructions to create your virtual environment, install required dependencies, and run the setup scripts. You will generally have three tabs (or windows) open in your terminal, which will be used for: Git operations . Perform Git operations and general development in the repository, such as git checkout main . Elasticsearch . Run an Elasticsearch (ES) instance. See instructions below . Django server . Start and stop the web server. Server is started with ./runserver.sh , but see more details below . What follows are the specific steps for each of these tabs.","title":"Running in a Virtual Environment"},{"location":"running-virtualenv/#1-git-operations","text":"From this tab you can do Git operations, such as checking out our main branch: git checkout main","title":"1. Git operations"},{"location":"running-virtualenv/#updating-all-dependencies","text":"Each time you fetch from the upstream repository (this repo), run ./setup.sh . This setup script will remove and reinstall the project dependencies and rebuild the site's JavaScript and CSS assets. Note You may also run ./backend.sh or ./frontend.sh if you only want to re-build the backend or front-end, respectively.","title":"Updating all dependencies"},{"location":"running-virtualenv/#setting-environments","text":"The NODE_ENV environment variable can be set in your .env file to either development or production , which will affect how the build is made and what gulp tasks are available. To install dependencies of one environment or the other run ./frontend.sh (dependencies and devDependencies) or ./frontend.sh production (dependencies but not devDependencies).","title":"Setting environments"},{"location":"running-virtualenv/#2-run-elasticsearch-optional","text":"Elasticsearch is needed for certain pieces of this project but is not a requirement for basic functionality. If Elasticsearch is installed via Homebrew , you can see instructions for running manually or as a background service using: brew info elasticsearch Typically to run Elasticsearch as a background service you can run: brew services start elasticsearch","title":"2. Run Elasticsearch (optional)"},{"location":"running-virtualenv/#3-launch-site","text":"First, move into the consumerfinance.gov project directory and ready your environment: # Use the consumerfinance.gov virtualenv. workon consumerfinance.gov # cd into this directory (if you aren't already there) cd consumerfinance.gov From the project root, start the Django server: ./runserver.sh Note If prompted to migrate database changes, stop the server with ctrl + c and run these commands: python cfgov/manage.py migrate ./initial-data.sh ./runserver.sh To view the site browse to: http://localhost:8000 Using a different port If you want to run the server at a port other than 8000 use python cfgov/manage.py runserver <port number> Specify an alternate port number, e.g. 8001 . To view the Wagtail admin login, browse to http://localhost:8000/admin and login with username admin and password admin (created in initial-data.sh above; note that this password will expire after 60 days). Using HTTPS locally To access a local server using HTTPS use ./runserver.sh ssl You'll need to ignore any browser certificate errors.","title":"3. Launch Site"},{"location":"running-virtualenv/#available-gulp-tasks","text":"There are a number of important gulp tasks, particularly build and test , which will build the project and test it, respectively. Tasks are invoked via an yarn run command so that the local gulp-cli can be used. Using the yarn run gulp -- --tasks command you can view all available tasks. The important ones are listed below: yarn run gulp build # Concatenate, optimize, and copy source files to the production /dist/ directory. yarn run gulp clean # Remove the contents of the production /dist/ directory. yarn run gulp lint # Lint the scripts and build files. yarn run gulp docs # Generate JSDocs from the scripts. yarn run gulp test # Run linting, unit, and functional tests (see below). yarn run gulp test:unit # Run only unit tests on source code.","title":"Available Gulp Tasks"},{"location":"running-virtualenv/#reinstalling-the-virtual-environment","text":"To remove an existing virtual environment for a reinstall of consumerfinance.gov , first deactivate the virtual environment if it is active: deactivate Then remove the existing virtual environment: rmvirtualenv consumerfinance.gov After this, you may follow the installation instructions again.","title":"Reinstalling the virtual environment"},{"location":"site-search/","text":"Site Search \u00b6 Global site search of consumerfinance.gov is implemented using Search.gov . The search box in the site header is linked to this global site search. Searches using this method return results at the search.consumerfinance.gov subdomain. This subdomain is hosted by Search.gov. We maintain several independent search indexes on Search.gov for various purposes: cfpb : English-language search for www.consumerfinance.gov . cfpb_es : Spanish-language search for pages under www.consumerfinance.gov/es/ . cfpb_beta : An alternate English-language search index for beta.consumerfinance.gov . cfpb_beta_es : An alternate Spanish-language search index for beta.consumerfinance.gov/es/ . These search indexes are hardcoded depending on the page being viewed. Configuration of site search on Search.gov requires an approved login to its admin console. The Search.gov Help Manual documents the various configuration options used on our indexes, for example text-based \"best bets\" and our custom visual design. The website base page template includes the necessary Search.gov code snippets required for searching and indexing of site content. Search.gov also makes use of the website robots.txt and sitemap.xml files.","title":"Site Search"},{"location":"site-search/#site-search","text":"Global site search of consumerfinance.gov is implemented using Search.gov . The search box in the site header is linked to this global site search. Searches using this method return results at the search.consumerfinance.gov subdomain. This subdomain is hosted by Search.gov. We maintain several independent search indexes on Search.gov for various purposes: cfpb : English-language search for www.consumerfinance.gov . cfpb_es : Spanish-language search for pages under www.consumerfinance.gov/es/ . cfpb_beta : An alternate English-language search index for beta.consumerfinance.gov . cfpb_beta_es : An alternate Spanish-language search index for beta.consumerfinance.gov/es/ . These search indexes are hardcoded depending on the page being viewed. Configuration of site search on Search.gov requires an approved login to its admin console. The Search.gov Help Manual documents the various configuration options used on our indexes, for example text-based \"best bets\" and our custom visual design. The website base page template includes the necessary Search.gov code snippets required for searching and indexing of site content. Search.gov also makes use of the website robots.txt and sitemap.xml files.","title":"Site Search"},{"location":"translation/","text":"Translation \u00b6 As consumerfinance.gov is a Django project, the Django translation documentation is a good place to start . What follows is a brief introduction to translations with the particular tools consumerfinance.gov uses (like Jinja2 templates) and the conventions we use. Overview \u00b6 Django translations use GNU gettext (see the installation instructions ). By convention, translations are usually performed in code by wrapping a string to be translated in a function that is either named or aliased with an underscore. For example: _(\"This is a translatable string.\") These strings are collected into portable object ( .po ) files for each supported language. These files map the original string ( msgid ) to a matching translated string ( msgstr ). For example: msgid \"This is a translatable string.\" msgstr \"Esta es una cadena traducible.\" These portable object files are compiled into machine object files ( .mo ) that the translation system uses when looking up the original string. By convention the .po and .mo files live inside a locale/[LANGUAGE]/LC_MESSAGES/ folder structure, for example, cfgov/locale/es/LC_MESSAGES/django.po for the Spanish language portable object file for all of our consumerfinance.gov messages. How to translate text in consumerfinance.gov \u00b6 This brief howto will guide you through adding translatable text to consumerfinance.gov. 1. Add the translation function around the string \u00b6 In Jinja2 templates: {{ _(\"Hello World!\") }} In Django templates: {% load i18n %} {% trans \"Hello World!\" %} In Python code: from django.utils.translation import ugettext as _ mystring = _(\"Hello World!\") The string in the call to the translation function will be the msgid in the portable object file below. 2. Run the makemessages management command to add the string to the portable object file \u00b6 The makemessages management command will look through all Python, Django, and Jinja2 template files to find strings that are wrapped in a translation function call and add them to the portable object file for a particular language. The language is specified with -l . The command also must be called from the root of the Django app tree, not the project root. To generate or update the portable object file for Spanish: cd cfgov django-admin.py makemessages -l es 3. Edit the portable object file to add a translation for the string \u00b6 The portable object files are stored in cfgov/locale/[LANGUAGE]/LC_MESSAGES/ . For the Spanish portable object file, edit cfgov/locale/es/LC_MESSAGES/django.po and add the Spanish translation as the msgstr for your new msgid msgid \"Hello World!\" msgstr \"Hola Mundo!\" 4. Run the compilemessages management command to compile the machine object file \u00b6 cd cfgov django-admin.py compilemessages Wagtail Considerations \u00b6 All of our Wagtail pages include a language-selection dropdown under its Configuration tab: The selected language will force translation of all translatable strings in templates and code for that page. Troubleshooting \u00b6 To ensure that strings in templates are picked up in message extraction ( django-admin.py makemessages ), it also helps to know that the way makemessages works. makemessages converts all Django {% translate %} , {% blocktranslate %} , and Jinja2 {% trans %} tags into _(\u2026) gettext calls and then to have xgettext process the files as if they were Python. This process does not work the same as general template parsing, and it means that it's best to make the translatable strings as discoverable as possible. There are a few things to avoid to make sure the strings are picked up by makemessages : Do not include the _() call in a larger Jinja2 template data structure: \u00b6 + {% set link_text = _(\"Visit our website\") %} {% set link_data = { \"url\": \"https://consumerfinance.gov\", - \"text\": _(\"Visit our website\"), + \"text\": link_text, } %} Do not include spaces between the parentheses and the string in Jinja2 templates: \u00b6 - {{ _( \"Hello World!\" ) }} + {{ _(\"Hello World!\") }} Do not use f-strings in Python calls to gettext: \u00b6 - _(f\"Hello {world_name}\") + _(\"Hello %(world_name)s\" % {'world_name': world_name}) Django's documentation has some additional information on the limitations of translatable strings and gettext . Do mark variable strings for translation with gettext_noop \u00b6 If you have a variable that will be translated in a template later using the variable name, but you need to mark it for translation so that makemessages will pick it up, use Django's gettext_noop : from django.utils.translation import gettext_noop mystring = gettext_noop(\"Hello World!\") {{ _(mystring) }}","title":"Translation"},{"location":"translation/#translation","text":"As consumerfinance.gov is a Django project, the Django translation documentation is a good place to start . What follows is a brief introduction to translations with the particular tools consumerfinance.gov uses (like Jinja2 templates) and the conventions we use.","title":"Translation"},{"location":"translation/#overview","text":"Django translations use GNU gettext (see the installation instructions ). By convention, translations are usually performed in code by wrapping a string to be translated in a function that is either named or aliased with an underscore. For example: _(\"This is a translatable string.\") These strings are collected into portable object ( .po ) files for each supported language. These files map the original string ( msgid ) to a matching translated string ( msgstr ). For example: msgid \"This is a translatable string.\" msgstr \"Esta es una cadena traducible.\" These portable object files are compiled into machine object files ( .mo ) that the translation system uses when looking up the original string. By convention the .po and .mo files live inside a locale/[LANGUAGE]/LC_MESSAGES/ folder structure, for example, cfgov/locale/es/LC_MESSAGES/django.po for the Spanish language portable object file for all of our consumerfinance.gov messages.","title":"Overview"},{"location":"translation/#how-to-translate-text-in-consumerfinancegov","text":"This brief howto will guide you through adding translatable text to consumerfinance.gov.","title":"How to translate text in consumerfinance.gov"},{"location":"translation/#1-add-the-translation-function-around-the-string","text":"In Jinja2 templates: {{ _(\"Hello World!\") }} In Django templates: {% load i18n %} {% trans \"Hello World!\" %} In Python code: from django.utils.translation import ugettext as _ mystring = _(\"Hello World!\") The string in the call to the translation function will be the msgid in the portable object file below.","title":"1. Add the translation function around the string"},{"location":"translation/#2-run-the-makemessages-management-command-to-add-the-string-to-the-portable-object-file","text":"The makemessages management command will look through all Python, Django, and Jinja2 template files to find strings that are wrapped in a translation function call and add them to the portable object file for a particular language. The language is specified with -l . The command also must be called from the root of the Django app tree, not the project root. To generate or update the portable object file for Spanish: cd cfgov django-admin.py makemessages -l es","title":"2. Run the makemessages management command to add the string to the portable object file"},{"location":"translation/#3-edit-the-portable-object-file-to-add-a-translation-for-the-string","text":"The portable object files are stored in cfgov/locale/[LANGUAGE]/LC_MESSAGES/ . For the Spanish portable object file, edit cfgov/locale/es/LC_MESSAGES/django.po and add the Spanish translation as the msgstr for your new msgid msgid \"Hello World!\" msgstr \"Hola Mundo!\"","title":"3. Edit the portable object file to add a translation for the string"},{"location":"translation/#4-run-the-compilemessages-management-command-to-compile-the-machine-object-file","text":"cd cfgov django-admin.py compilemessages","title":"4. Run the compilemessages management command to compile the machine object file"},{"location":"translation/#wagtail-considerations","text":"All of our Wagtail pages include a language-selection dropdown under its Configuration tab: The selected language will force translation of all translatable strings in templates and code for that page.","title":"Wagtail Considerations"},{"location":"translation/#troubleshooting","text":"To ensure that strings in templates are picked up in message extraction ( django-admin.py makemessages ), it also helps to know that the way makemessages works. makemessages converts all Django {% translate %} , {% blocktranslate %} , and Jinja2 {% trans %} tags into _(\u2026) gettext calls and then to have xgettext process the files as if they were Python. This process does not work the same as general template parsing, and it means that it's best to make the translatable strings as discoverable as possible. There are a few things to avoid to make sure the strings are picked up by makemessages :","title":"Troubleshooting"},{"location":"translation/#do-not-include-the-_-call-in-a-larger-jinja2-template-data-structure","text":"+ {% set link_text = _(\"Visit our website\") %} {% set link_data = { \"url\": \"https://consumerfinance.gov\", - \"text\": _(\"Visit our website\"), + \"text\": link_text, } %}","title":"Do not include the _() call in a larger Jinja2 template data structure:"},{"location":"translation/#do-not-include-spaces-between-the-parentheses-and-the-string-in-jinja2-templates","text":"- {{ _( \"Hello World!\" ) }} + {{ _(\"Hello World!\") }}","title":"Do not include spaces between the parentheses and the string in Jinja2 templates:"},{"location":"translation/#do-not-use-f-strings-in-python-calls-to-gettext","text":"- _(f\"Hello {world_name}\") + _(\"Hello %(world_name)s\" % {'world_name': world_name}) Django's documentation has some additional information on the limitations of translatable strings and gettext .","title":"Do not use f-strings in Python calls to gettext:"},{"location":"translation/#do-mark-variable-strings-for-translation-with-gettext_noop","text":"If you have a variable that will be translated in a template later using the variable name, but you need to mark it for translation so that makemessages will pick it up, use Django's gettext_noop : from django.utils.translation import gettext_noop mystring = gettext_noop(\"Hello World!\") {{ _(mystring) }}","title":"Do mark variable strings for translation with gettext_noop"},{"location":"wagtail-pages/","text":"Wagtail pages \u00b6 Wagtail pages are Django models that are constructed of fields , StreamFields , and panels that are rendered in a standard way. All CFPB Wagtail pages should inherit from the v1.models.base.CFGOVPage class . Note Before creating a new Wagtail page type please consider whether one of our existing page types can meet your needs. Talk to the consumerfinance.gov product owners if your content is significantly different from anything else on the site or a specific maintenance efficiency will be gained from a new page type. There are types of information defined on a new Wagtail page model: basic database fields (like any Django model), specialized database fields called StreamFields that allow for freeform page content, and editor panels that present these fields to content editors. Fields \u00b6 Database fields in Wagtail pages work exactly the same as in Django models , and Wagtail pages can use any Django model field . For example, our BrowsePage includes a standard Django BooleanField that allows content editors to toggle secondary navigation sibling pages: from django.db import models from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False) StreamFields \u00b6 StreamFields are special Django model fields provided by Wagtail for freeform page content . They allow a content editor to pick any number number of optional components and place them in any order within their StreamField. In practice, this provides the flexibility of a large rich text field, with the structure of individual components . For example, our LandingPage page model includes a header StreamField that can have a hero and/or a text introduction: from wagtail.core.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) The specifics of StreamField block components can be found in Creating and Editing Wagtail Components . Panels \u00b6 Editor panels define how the page's fields and StreamFields will be organized for content editors; they correspond to the tabs that appear across the top of the edit view for a page in the Wagtail admin. The base Wagtail Page class and the CFGOVPage subclass of it define specific sets of panels to which all fields should be added: content_panels : For page body content. These fields appear on the \"General Content\" tab when editing a page. sidefoot_panels : For page sidebar or footer content. These fields appear on the \"Sidebar\" tab when editing a page. settings_panels : Page configuration such as the categories, tags, scheduled publishing, etc. Appears on the \"Configuration\" tab when editing a page. Most fields will simply require a FieldPanel to be added to one of the sets of panels above. StreamFields will require a StreamFieldPanel . See the Wagtail documentation for additional, more complex panel options . For example, in our BrowsePage (used in the database fields example above ), the secondary_nav_exclude_sibling_pages BooleanField is added to the sidefoot_panels as a FieldPanel : from django.db import models from wagtail.admin.edit_handlers import FieldPanel from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False) # \u2026 sidefoot_panels = CFGOVPage.sidefoot_panels + [ FieldPanel('secondary_nav_exclude_sibling_pages'), ] Because secondary_nav_exclude_sibling_pages is a boolean field, this creates a checkbox on the \"Sidebar/Footer\" tab when editing a page. Checking or unchecking that checkbox will set the value of secondary_nav_exclude_sibling_pages when the page is saved. In our LandingPage (used in the StreamFields example above ), the header StreamField is added to the content_panels as a StreamFieldPanel : from wagtail.admin.edit_handlers import StreamFieldPanel from wagtail.core.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) # \u2026 content_panels = CFGOVPage.content_panels + [ StreamFieldPanel('header'), # \u2026 ] Parent / child page relationships \u00b6 Wagtail provides two attributes to page models that enable restricting the types of subpages or parent pages a particular page model can have. On any page model: parent_page_types limits which page types this type can be created under. subpage_types limits which page types can be created under this type. For example, in our interactive regulations page models we have a RegulationLandingPage that can be created anywhere in the page tree. RegulationLandingPage , however, can only have two types of pages created within it: RegulationPage and RegulationSearchPage . This parent/child relationship is expressed by setting subpage_types on RegulationLandingPage and parent_page_types on RegulationPage and RegulationSearchPage to a model name in the form 'app_label.ModelName': from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): subpage_types = ['regulations3k.RegulationPage', 'regulations3k.RegulationsSearchPage'] class RegulationsSearchPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] class RegulationPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] Note We prevent child pages from being added to RegulationPage and RegulationSearchPage by setting subpage_types to an empty list. Template rendering \u00b6 New Wagtail page types will usually need to make customizations to their base template when rendering the page . This is done by overriding the template attribute on the page model. For example, the interactive regulations landing page includes a customized list of recently issued notices that gets loaded dynamically from the Federal Register. To do this it provides its own template that inherits from our base templates and overrides the content_sidebar block to include a separate recent_notices template: from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): template = 'regulations3k/landing-page.html' And in regulations3k/landing-page.html : {% extends 'layout-2-1-bleedbar.html' %} {% import 'recent-notices.html' as recent_notices with context %} {% block content_sidebar scoped -%} {{ recent_notices }} {%- endblock %}","title":"Wagtail Pages"},{"location":"wagtail-pages/#wagtail-pages","text":"Wagtail pages are Django models that are constructed of fields , StreamFields , and panels that are rendered in a standard way. All CFPB Wagtail pages should inherit from the v1.models.base.CFGOVPage class . Note Before creating a new Wagtail page type please consider whether one of our existing page types can meet your needs. Talk to the consumerfinance.gov product owners if your content is significantly different from anything else on the site or a specific maintenance efficiency will be gained from a new page type. There are types of information defined on a new Wagtail page model: basic database fields (like any Django model), specialized database fields called StreamFields that allow for freeform page content, and editor panels that present these fields to content editors.","title":"Wagtail pages"},{"location":"wagtail-pages/#fields","text":"Database fields in Wagtail pages work exactly the same as in Django models , and Wagtail pages can use any Django model field . For example, our BrowsePage includes a standard Django BooleanField that allows content editors to toggle secondary navigation sibling pages: from django.db import models from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False)","title":"Fields"},{"location":"wagtail-pages/#streamfields","text":"StreamFields are special Django model fields provided by Wagtail for freeform page content . They allow a content editor to pick any number number of optional components and place them in any order within their StreamField. In practice, this provides the flexibility of a large rich text field, with the structure of individual components . For example, our LandingPage page model includes a header StreamField that can have a hero and/or a text introduction: from wagtail.core.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) The specifics of StreamField block components can be found in Creating and Editing Wagtail Components .","title":"StreamFields"},{"location":"wagtail-pages/#panels","text":"Editor panels define how the page's fields and StreamFields will be organized for content editors; they correspond to the tabs that appear across the top of the edit view for a page in the Wagtail admin. The base Wagtail Page class and the CFGOVPage subclass of it define specific sets of panels to which all fields should be added: content_panels : For page body content. These fields appear on the \"General Content\" tab when editing a page. sidefoot_panels : For page sidebar or footer content. These fields appear on the \"Sidebar\" tab when editing a page. settings_panels : Page configuration such as the categories, tags, scheduled publishing, etc. Appears on the \"Configuration\" tab when editing a page. Most fields will simply require a FieldPanel to be added to one of the sets of panels above. StreamFields will require a StreamFieldPanel . See the Wagtail documentation for additional, more complex panel options . For example, in our BrowsePage (used in the database fields example above ), the secondary_nav_exclude_sibling_pages BooleanField is added to the sidefoot_panels as a FieldPanel : from django.db import models from wagtail.admin.edit_handlers import FieldPanel from v1.models.base import CFGOVPage class BrowsePage(CFGOVPage): secondary_nav_exclude_sibling_pages = models.BooleanField(default=False) # \u2026 sidefoot_panels = CFGOVPage.sidefoot_panels + [ FieldPanel('secondary_nav_exclude_sibling_pages'), ] Because secondary_nav_exclude_sibling_pages is a boolean field, this creates a checkbox on the \"Sidebar/Footer\" tab when editing a page. Checking or unchecking that checkbox will set the value of secondary_nav_exclude_sibling_pages when the page is saved. In our LandingPage (used in the StreamFields example above ), the header StreamField is added to the content_panels as a StreamFieldPanel : from wagtail.admin.edit_handlers import StreamFieldPanel from wagtail.core.fields import StreamField from v1.atomic_elements import molecules from v1.models import CFGOVPage class LandingPage(CFGOVPage): header = StreamField([ ('hero', molecules.Hero()), ('text_introduction', molecules.TextIntroduction()), ], blank=True) # \u2026 content_panels = CFGOVPage.content_panels + [ StreamFieldPanel('header'), # \u2026 ]","title":"Panels"},{"location":"wagtail-pages/#parent-child-page-relationships","text":"Wagtail provides two attributes to page models that enable restricting the types of subpages or parent pages a particular page model can have. On any page model: parent_page_types limits which page types this type can be created under. subpage_types limits which page types can be created under this type. For example, in our interactive regulations page models we have a RegulationLandingPage that can be created anywhere in the page tree. RegulationLandingPage , however, can only have two types of pages created within it: RegulationPage and RegulationSearchPage . This parent/child relationship is expressed by setting subpage_types on RegulationLandingPage and parent_page_types on RegulationPage and RegulationSearchPage to a model name in the form 'app_label.ModelName': from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): subpage_types = ['regulations3k.RegulationPage', 'regulations3k.RegulationsSearchPage'] class RegulationsSearchPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] class RegulationPage(CFGOVPage): parent_page_types = ['regulations3k.RegulationLandingPage'] subpage_types = [] Note We prevent child pages from being added to RegulationPage and RegulationSearchPage by setting subpage_types to an empty list.","title":"Parent / child page relationships"},{"location":"wagtail-pages/#template-rendering","text":"New Wagtail page types will usually need to make customizations to their base template when rendering the page . This is done by overriding the template attribute on the page model. For example, the interactive regulations landing page includes a customized list of recently issued notices that gets loaded dynamically from the Federal Register. To do this it provides its own template that inherits from our base templates and overrides the content_sidebar block to include a separate recent_notices template: from v1.models import CFGOVPage class RegulationLandingPage(CFGOVPage): template = 'regulations3k/landing-page.html' And in regulations3k/landing-page.html : {% extends 'layout-2-1-bleedbar.html' %} {% import 'recent-notices.html' as recent_notices with context %} {% block content_sidebar scoped -%} {{ recent_notices }} {%- endblock %}","title":"Template rendering"},{"location":"website-feedback/","text":"Website feedback \u00b6 Website pages can be configured to display forms that solicit feedback from visitors, for example asking whether or not page information was useful and accurate. User-submitted feedback is stored in the Django database. There are several ways to access and export this data. Exporting feedback using Wagtail \u00b6 Users with appropriate permissions have access to an \"Export feedback\" tool in Wagtail. This tool generates a ZIP file containing submitted feedback between two user-specified dates. Feedback is automatically grouped into three segments, each stored as a CSV: Feedback submitted on Ask CFPB pages (both English and Spanish) Feedback submitted on Buying a House pages Feedback submitted on all other website pages In order to access this part of the Wagtail admin, users must be granted the \"Can export feedback from the Wagtail admin\" permission. This can be enabled or disabled for different Wagtail groups in the Groups area of the admin. Exporting feedback from the command line \u00b6 For more control over what feedback to export, the export_feedback Django management command can be used. This command generates a CSV file containing feedback for a specified set of pages and/or dates. To export all website feedback for all time, use this command: cfgov/manage.py export_feedback --filename output.csv (Alternatively, exclude the --filename parameter to send the output to the console.) To export feedback for a specific Wagtail page and its descendants, provide its slug. Specified pages must have slugs that are unique across the website. cfgov/manage.py export_feedback ask-cfpb --filename ask_cfpb.csv Specify multiple slugs to export feedback for multiple pages: cfgov/manage.py export_feedback ask-cfpb owning-a-home --filename ask_and_bah.csv To invert the selection, and export feedback for all pages except those specified, use the --exclude parameter: cfgov/manage.py export_feedback --exclude ask-cfpb --filename everything_except_ask.csv To limit the exported feedback within a given date range, provide the --from-date and/or the --to-date parameters. These parameters are inclusive, meaning that feedback submitted on or between the dates is included in the export. cfgov/manage.py export_feedback --from-date 2019-01-01 --to-date 2019-12-31 --filename 2019.csv Querying feedback programmatically \u00b6 For even greater control, feedback can be queried from a Python shell using the Feedback Django model. This model includes a custom manager that exposes a for_pages method that can be used to efficiently query feedback for one or more pages and all of their descendants: feedback = Feedback.objects.for_pages([page1, page2]) Like the management command, it can also be used to invert the selection: feedback = Feedback.objects.for_pages([page], exclude=True) This method can also be combined in the usual way with other standard Django queryset methods: feedback = Feedback.objects.for_pages([page]).filter(language='es')","title":"Website Feedback"},{"location":"website-feedback/#website-feedback","text":"Website pages can be configured to display forms that solicit feedback from visitors, for example asking whether or not page information was useful and accurate. User-submitted feedback is stored in the Django database. There are several ways to access and export this data.","title":"Website feedback"},{"location":"website-feedback/#exporting-feedback-using-wagtail","text":"Users with appropriate permissions have access to an \"Export feedback\" tool in Wagtail. This tool generates a ZIP file containing submitted feedback between two user-specified dates. Feedback is automatically grouped into three segments, each stored as a CSV: Feedback submitted on Ask CFPB pages (both English and Spanish) Feedback submitted on Buying a House pages Feedback submitted on all other website pages In order to access this part of the Wagtail admin, users must be granted the \"Can export feedback from the Wagtail admin\" permission. This can be enabled or disabled for different Wagtail groups in the Groups area of the admin.","title":"Exporting feedback using Wagtail"},{"location":"website-feedback/#exporting-feedback-from-the-command-line","text":"For more control over what feedback to export, the export_feedback Django management command can be used. This command generates a CSV file containing feedback for a specified set of pages and/or dates. To export all website feedback for all time, use this command: cfgov/manage.py export_feedback --filename output.csv (Alternatively, exclude the --filename parameter to send the output to the console.) To export feedback for a specific Wagtail page and its descendants, provide its slug. Specified pages must have slugs that are unique across the website. cfgov/manage.py export_feedback ask-cfpb --filename ask_cfpb.csv Specify multiple slugs to export feedback for multiple pages: cfgov/manage.py export_feedback ask-cfpb owning-a-home --filename ask_and_bah.csv To invert the selection, and export feedback for all pages except those specified, use the --exclude parameter: cfgov/manage.py export_feedback --exclude ask-cfpb --filename everything_except_ask.csv To limit the exported feedback within a given date range, provide the --from-date and/or the --to-date parameters. These parameters are inclusive, meaning that feedback submitted on or between the dates is included in the export. cfgov/manage.py export_feedback --from-date 2019-01-01 --to-date 2019-12-31 --filename 2019.csv","title":"Exporting feedback from the command line"},{"location":"website-feedback/#querying-feedback-programmatically","text":"For even greater control, feedback can be queried from a Python shell using the Feedback Django model. This model includes a custom manager that exposes a for_pages method that can be used to efficiently query feedback for one or more pages and all of their descendants: feedback = Feedback.objects.for_pages([page1, page2]) Like the management command, it can also be used to invert the selection: feedback = Feedback.objects.for_pages([page], exclude=True) This method can also be combined in the usual way with other standard Django queryset methods: feedback = Feedback.objects.for_pages([page]).filter(language='es')","title":"Querying feedback programmatically"}]}